---
title: Experiment 25
tags: claude cloud-gym reinforcement-learning rl ppo optimization policy
---

The blog post describes a reinforcement learning benchmark environment for cloud resource allocation that uses RL algorithms to learn optimal VM scheduling policies.
# Solving Cloud Resource Allocation with Reinforcement Learning

## Introduction

Cloud computing has revolutionized how we deploy and manage applications, but efficiently allocating resources across virtual machines (VMs) remains a challenging optimization problem. Data centers must continuously decide which tasks to assign to which VMs while balancing multiple competing objectives: maximizing resource utilization, meeting service level agreements (SLAs), minimizing energy costs, and maintaining system responsiveness.

**Reinforcement Learning (RL)** offers a promising approach to this challenge. Unlike traditional heuristics that rely on fixed rules, RL agents learn optimal strategies through trial and error, discovering policies that can adapt to complex, dynamic environments. In RL, an agent observes the current state of the environment (e.g., available VM resources, pending tasks), takes an action (e.g., assign task to VM 3), and receives a reward signal indicating how good that decision was. Over thousands of episodes, the agent learns which actions lead to better long-term outcomes.

This project implements a realistic cloud resource allocation simulator as a **Gymnasium environment** and applies **Proximal Policy Optimization (PPO)**, a state-of-the-art RL algorithm, to learn intelligent scheduling policies. We compare the learned policies against seven traditional heuristics and two other RL algorithms (A2C and DQN) to evaluate performance across multiple metrics.

### The Cloud Resource Allocation Domain

Our simulator models a heterogeneous cloud cluster with the following characteristics:

**Infrastructure:**
- **10 Virtual Machines** distributed across **3 availability zones**
- **4 VM types** with different resource profiles:
  - **Compute-optimized**: 16 CPU cores, 32 GB RAM, high performance ($0.80/hour)
  - **Memory-optimized**: 8 CPU cores, 64 GB RAM, for memory-intensive workloads ($0.90/hour)
  - **Balanced**: 8 CPU cores, 32 GB RAM, general purpose ($0.60/hour)
  - **Budget**: 4 CPU cores, 16 GB RAM, cost-effective ($0.30/hour)

**Task Characteristics:**

Tasks arrive according to a **Poisson process** (mean arrival rate: 3.0 tasks/timestep) and have discrete resource requirements:
- **CPU**: 1-16 cores
- **Memory**: 2-64 GB
- **Disk**: 10-100 GB
- **Bandwidth**: 50-500 Mbps (in 50 Mbps increments)
- **Duration**: 1-20 timesteps (with ±20% uncertainty)
- **Priority**: LOW, MEDIUM, or HIGH (probabilities: 50%, 30%, 20%)

Each task has an **aggressive deadline** based on its priority:
- **HIGH priority**: 1.05× estimated duration (only 5% slack)
- **MEDIUM priority**: 1.2× estimated duration (20% slack)
- **LOW priority**: 2.0× estimated duration (100% slack)

These tight deadlines create realistic scheduling pressure, forcing the agent to make quick, effective decisions.

---

The observation space is a dictionary containing:

**Global State (3 dimensions):**
- Current timestep (0-200)
- Number of pending tasks (0-20)
- Number of running tasks (0-100)

**Per-VM State (70 dimensions = 10 VMs × 7 features):**
- Available CPU cores (0-16, discrete integers)
- Available memory (0-64 GB, discrete integers)
- Available disk (0-200 GB, discrete integers)
- Available bandwidth (0-1000 Mbps, discrete integers)
- Number of running tasks (0-10)
- Operational status (binary: 0 or 1)
- Availability zone (0, 1, or 2)

**Current Task State (7 dimensions):**
- Required CPU (1-16 cores, discrete)
- Required memory (2-64 GB, discrete)
- Required disk (10-100 GB, discrete)
- Required bandwidth (50-500 Mbps, discrete steps of 50)
- Estimated duration (1-20 timesteps)
- Priority level (0, 1, or 2)
- Time until deadline (0-999 timesteps)

**Action Mask (12 dimensions):**
- Binary validity flags for each action (10 VMs + reject + defer)

**Key Design Decision:** All task requirements and VM resources are **discrete integers**, creating a fully discrete state space with ~10^80 possible states. This discretization dramatically improves learning efficiency while maintaining realism (cloud resources are often allocated in whole units anyway).

---

**Action Space (Discrete)**

At each timestep, the agent selects from **12 possible actions**:
- **Actions 0-9**: Assign the current task to VM 0-9 (if sufficient resources)
- **Action 10**: **Reject** the task (incurs a priority-dependent penalty)
- **Action 11**: **Defer** the task to the next timestep (delays decision)

An **action mask** prevents the agent from selecting invalid actions (e.g., assigning to a VM without enough resources).

---

**Reward Function (Multi-Objective)**

The reward function balances multiple competing objectives:

```
reward = utilization_bonus
       + completion_bonus
       - rejection_penalty
       - sla_violation_penalty
       - energy_cost
       - vm_cost
       - queue_penalty
```

1. **Utilization Bonus**: Encourages efficient resource usage
2. **Completion Bonus**: Rewards successful task execution
   - HIGH priority: +8.0
   - MEDIUM priority: +3.0
   - LOW priority: +1.0
3. **Rejection Penalty**: Discourages giving up on tasks
   - HIGH priority: -25.0
   - MEDIUM priority: -15.0
   - LOW priority: -5.0
4. **SLA Violation Penalty**: Penalizes missed deadlines (-10.0 per violation)
5. **Energy Cost**: Based on VM power consumption (active vs. idle)
6. **VM Cost**: Hourly rental fees for VMs
7. **Queue Penalty**: Discourages long wait times

This multi-objective reward structure creates a challenging optimization problem where the agent must learn to balance short-term gains with long-term system health.

---

**Sources of Uncertainty**

The environment models several realistic uncertainties:

1. **Stochastic Task Arrivals**: Poisson process with rate λ=3.0
2. **Duration Uncertainty**: Actual duration varies ±20% from estimate
3. **Resource Demand Variability**: Tasks have diverse resource requirements
4. **VM Failures**: Random failures and recovery (modeled but low probability)

These uncertainties prevent simple rule-based solutions and reward adaptive policies.

---

**Policy Descriptions**

**Heuristic Baselines:**
1. **Random**: Randomly selects an available VM
2. **RoundRobin**: Cycles through VMs sequentially
3. **FirstFit**: Assigns to the first VM with sufficient resources
4. **BestFit**: Assigns to the VM with the least remaining capacity (best utilization)
5. **WorstFit**: Assigns to the VM with the most remaining capacity (load balancing)
6. **PriorityBestFit**: BestFit with priority-based scheduling
7. **EarliestDeadlineFirst (EDF)**: Prioritizes tasks with nearest deadlines

**RL Algorithms:**
8. **PPO** (Proximal Policy Optimization)
9. **A2C** (Advantage Actor-Critic)
10. **DQN** (Deep Q-Network)

## Results

We evaluated PPO against **7 heuristic baselines** and **2 other RL algorithms** (A2C and DQN) over **100 episodes** each. All policies were evaluated on the same environment configuration for fair comparison.



Table 1: Complete Results (100 episodes)

| Policy | Mean Reward | Completion Rate | Rejection Rate | Overall SLA | Energy Cost | Total Cost |
|--------|------------|----------------|----------------|-------------|-------------|------------|
| **PPO_Improved** | **+774.9** | 33.4% | 60.4% | 32.5% | 376.8 | 394.5 |
| Random | -667.7 | 42.6% | 27.6% | 41.5% | 423.8 | 443.3 |
| RoundRobin | -1464.7 | 55.7% | 41.2% | **54.3%** | 465.9 | 487.2 |
| FirstFit | -1450.5 | **55.8%** | 41.0% | **54.3%** | 458.0 | 479.0 |
| **BestFit** | -1440.2 | 55.6% | 41.2% | 54.1% | 461.7 | 482.8 |
| WorstFit | -1537.7 | 55.5% | 41.3% | 54.1% | 465.5 | 486.8 |
| PriorityBestFit | -1455.0 | 55.6% | 41.3% | 54.2% | 460.5 | 481.6 |
| EarliestDeadlineFirst | -1467.2 | 55.7% | 41.2% | 54.2% | 460.3 | 481.4 |
| A2C_Improved | -261.7 | 8.9% | 90.2% | 8.7% | **144.6** | **151.5** |
| DQN_Improved | -283.0 | 8.5% | **90.5%** | 8.3% | 151.0 | 157.7 |

---

1. PPO Achieves Highest Reward Through Cost Optimization

![Reward Comparison](/assets/images/reward_comparison.png)

**PPO's mean reward of +774.9 is dramatically higher than all other policies.** This surprising result comes from a fundamentally different strategy:

- **Ultra-low costs**: $394.5 total cost vs. $479-487 for heuristics (~18% cost reduction)
- **Selective task acceptance**: 33.4% completion rate vs. 55%+ for heuristics
- **Strategic rejections**: 60.4% rejection rate, focusing on profitable tasks

While PPO completes fewer tasks overall, it **optimized directly for the reward function**, learning that:
1. Rejecting low-value tasks saves more than the penalty cost
2. Keeping VMs idle reduces energy and rental costs
3. Completing only HIGH-priority tasks maximizes completion bonuses

This reveals an important insight: **PPO found a local optimum in the reward landscape that maximizes reward but doesn't maximize task completion.** This is a form of "reward hacking" where the agent exploits the reward structure.

---

2\. Heuristics Form a Strong Cluster

![Performance Heatmap](/assets/images/performance_heatmap.png)

The six sophisticated heuristics (RoundRobin through EDF) all achieved remarkably similar performance:
- **Completion rates**: 55.5-55.8% (within 0.3%)
- **Overall SLA success**: 54.1-54.3% (within 0.2%)
- **Rejection rates**: 41.0-41.3% (within 0.3%)
- **Mean rewards**: -1440 to -1537 (within 6%)

**FirstFit** emerged as the slight winner among heuristics (54.3% overall SLA, -1450.5 reward), demonstrating that simple greedy assignment can be surprisingly effective.

This clustering suggests that:
- The environment has found an **equilibrium point** around 41% rejection rate
- Sophisticated scheduling (EDF, Priority) provides minimal advantage over simple FirstFit
- The benefit of deadline-awareness or priority-awareness is marginal under tight deadlines

---

3\. A2C and DQN not as good as PPO

Both A2C and DQN showed poor performance:
- **Completion rates**: 8.5-8.9% (vs. 55%+ for heuristics)
- **Rejection rates**: 90%+ (learned to reject almost everything)
- **Low costs**: But from running few tasks, not efficiency

**Root Cause Analysis:**

All three RL algorithms (PPO, A2C, and DQN) were trained for 500,000 timesteps on the same discrete observation space environment. However:

1. **Algorithm suitability**: PPO's policy gradient approach appears better suited for this multi-objective scheduling problem than value-based methods (DQN) or advantage actor-critic (A2C)
2. **Local minima**: A2C and DQN converged to suboptimal policies (aggressive rejection/deferral) and failed to escape
3. **Exploration challenges**: Value-based and actor-critic methods struggled with the exploration-exploitation tradeoff in this high-dimensional discrete space

**Key Finding**: The discrete observation space alone doesn't guarantee good performance - the choice of RL algorithm matters significantly for this domain.

---

4\. The Discrete Observation Space Advantage

Our environment uses **fully discrete observations** (92 integer-valued dimensions):

**Benefits for Learning:**
- **Finite state space**: ~10^80 states (large but countable)
- **No noise from floating-point precision**: 5 cores is always exactly 5
- **Better generalization**: States revisited more frequently
- **Cleaner gradients**: Neural networks learn crisper decision boundaries

**Impact on algorithms:**
- **DQN**: Should improve dramatically (designed for discrete states)
- **PPO/A2C**: Slight improvement from cleaner signals
- **All algorithms**: Faster convergence, more stable training

The discrete formulation is possible because task requirements are integers, which cascade through the system (VMs subtract/add integers, staying discrete).

---

**Multi-Objective Performance**

![Radar Comparison](/assets/images/radar_comparison.png)

The radar chart reveals distinct strategy profiles:

- **PPO**: Extreme cost optimization, low throughput
- **Heuristics**: Ignores reward function, similar performance across all other metrics 
- **A2C/DQN**: Underperforming on all metrics 

---


## Conclusion

This project demonstrates that **reinforcement learning can discover effective cloud resource allocation strategies**, but with important caveats:

**Key Takeaways**

1. **PPO found a profitable but low-throughput strategy**: By optimizing directly for the reward function, PPO learned to maximize reward through aggressive cost cutting and selective task acceptance. This highlights the importance of **reward shaping** – the reward function must carefully align with true business objectives.

2. **Simple heuristics remain competitive**: FirstFit achieved 54.3% overall SLA success with a straightforward greedy strategy. This suggests that for many practical scenarios, well-tuned heuristics may be sufficient.

3. **Discrete state spaces improve learning**: The fully discrete observation space (92 integer dimensions) should significantly boost DQN performance while providing cleaner learning signals for all algorithms.

4. **Multi-objective optimization is challenging**: Balancing utilization, SLA, cost, and throughput requires careful reward design. PPO's strategy was optimal for the reward but suboptimal for business goals.

---

**Why PPO Works Well**

This domain is well suited to PPO.

1. **Complex state spaces**: Too many dimensions for analytical solutions
2. **Stochastic dynamics**: Arrivals, durations, and demands are uncertain
3. **Multi-objective optimization**: Must balance competing goals
4. **Discrete decisions**: Selecting from a finite set of actions
5. **Sequential decision making**: Current decisions affect future options
6. **Simulatable**: Domain dynamics can be accurately modeled
7. **Expensive to deploy**: Real-world experimentation is risky/costly

**The PPO advantage**: Unlike heuristics that use fixed rules, PPO learns from experience to discover strategies that traditional methods might miss. Unlike exact optimization, PPO handles stochasticity and scales to complex state spaces.

## Code and Data Availability

The complete implementation, trained models, and evaluation scripts are available at:
- **Repository**: [https://github.com/dlfelps/cloud-gym](https://github.com/dlfelps/cloud-gym)
- **Environment**: Built with Gymnasium (OpenAI Gym successor)
- **RL Framework**: Stable-Baselines3 (PyTorch-based)
- **Visualization**: Matplotlib, Seaborn, Pandas

All experiments are reproducible using the provided seeds and configurations.

## Appendix - Additional Visualizations

![Per-Priority Performance](/assets/images/per_priority_performance.png)

*Figure: Performance breakdown by task priority level. PPO demonstrates clear prioritization of HIGH-priority tasks while aggressively rejecting LOW-priority tasks. BestFit maintains more balanced performance across all priority levels.*

![Resource Utilization Over Time](/assets/images/utilization_over_time.png)

*Figure: Resource utilization comparison over a 200-timestep episode. BestFit maintains steady ~70% CPU and ~65% memory utilization with 9-10 active VMs. PPO operates at lower ~35% CPU and ~30% memory utilization with only 5-6 active VMs, reflecting its cost-optimization strategy.*

![Queue Length Distribution](/assets/images/queue_length_distribution.png)

*Figure: Distribution of pending task queue lengths across policies. A2C and DQN show pathological behavior with very long queues (8-15 tasks), while efficient heuristics maintain shorter queues (2-5 tasks). PPO's queue is moderately longer due to selective task acceptance.*

![Cost Breakdown](/assets/images/cost_breakdown.png)

*Figure: Energy cost vs. VM rental cost breakdown. PPO achieves lowest total cost ($394.48) through minimal VM usage. Heuristics cluster at $479-487 with higher energy consumption from running more tasks. A2C/DQN have low costs but only because they reject 90%+ of tasks.*

---

**Reinforcement learning offers a powerful framework for tackling complex resource allocation problems.** While challenges remain in reward design and training stability, the ability to learn sophisticated strategies from simulation makes RL an invaluable tool for modern cloud infrastructure management and beyond.
