---
title: Experiment 25
tags: claude cloud-gym reinforcement-learning rl ppo optimization policy
---

The blog post describes a reinforcement learning benchmark environment for cloud resource allocation that uses RL algorithms to learn optimal VM scheduling policies.


# Solving Cloud Resource Allocation with Reinforcement Learning

## Introduction

Cloud computing has revolutionized how we deploy and manage applications, but efficiently allocating resources across virtual machines (VMs) remains a challenging optimization problem. Data centers must continuously decide which tasks to assign to which VMs while balancing multiple competing objectives: maximizing resource utilization, meeting service level agreements (SLAs), minimizing energy costs, and maintaining system responsiveness.

**Reinforcement Learning (RL)** offers a promising approach to this challenge. Unlike traditional heuristics that rely on fixed rules, RL agents learn optimal strategies through trial and error, discovering policies that can adapt to complex, dynamic environments. In RL, an agent observes the current state of the environment (e.g., available VM resources, pending tasks), takes an action (e.g., assign task to VM 3), and receives a reward signal indicating how good that decision was. Over thousands of episodes, the agent learns which actions lead to better long-term outcomes.

This project implements a realistic cloud resource allocation simulator as a **Gymnasium environment** and applies **Proximal Policy Optimization (PPO)**, a state-of-the-art RL algorithm, to learn intelligent scheduling policies. We compare the learned policies against seven traditional heuristics and two other RL algorithms (A2C and DQN) to evaluate performance across multiple metrics.

## The Cloud Resource Allocation Domain

### Environment Overview

Our simulator models a heterogeneous cloud cluster with the following characteristics:

**Infrastructure:**
- **10 Virtual Machines** distributed across **3 availability zones**
- **4 VM types** with different resource profiles:
  - **Compute-optimized**: 16 CPU cores, 32 GB RAM, high performance ($0.80/hour)
  - **Memory-optimized**: 8 CPU cores, 64 GB RAM, for memory-intensive workloads ($0.90/hour)
  - **Balanced**: 8 CPU cores, 32 GB RAM, general purpose ($0.60/hour)
  - **Budget**: 4 CPU cores, 16 GB RAM, cost-effective ($0.30/hour)

**Task Characteristics:**

Tasks arrive according to a **Poisson process** (mean arrival rate: 3.0 tasks/timestep) and have discrete resource requirements:
- **CPU**: 1-16 cores
- **Memory**: 2-64 GB
- **Disk**: 10-100 GB
- **Bandwidth**: 50-500 Mbps (in 50 Mbps increments)
- **Duration**: 1-20 timesteps (with ±20% uncertainty)
- **Priority**: LOW, MEDIUM, or HIGH (probabilities: 50%, 30%, 20%)

Each task has an **aggressive deadline** based on its priority:
- **HIGH priority**: 1.05× estimated duration (only 5% slack)
- **MEDIUM priority**: 1.2× estimated duration (20% slack)
- **LOW priority**: 2.0× estimated duration (100% slack)

These tight deadlines create realistic scheduling pressure, forcing the agent to make quick, effective decisions.

### State Space (92 Dimensions - Fully Discrete)

The observation space is a dictionary containing:

**Global State (3 dimensions):**
- Current timestep (0-200)
- Number of pending tasks (0-20)
- Number of running tasks (0-100)

**Per-VM State (70 dimensions = 10 VMs × 7 features):**
- Available CPU cores (0-16, discrete integers)
- Available memory (0-64 GB, discrete integers)
- Available disk (0-200 GB, discrete integers)
- Available bandwidth (0-1000 Mbps, discrete integers)
- Number of running tasks (0-10)
- Operational status (binary: 0 or 1)
- Availability zone (0, 1, or 2)

**Current Task State (7 dimensions):**
- Required CPU (1-16 cores, discrete)
- Required memory (2-64 GB, discrete)
- Required disk (10-100 GB, discrete)
- Required bandwidth (50-500 Mbps, discrete steps of 50)
- Estimated duration (1-20 timesteps)
- Priority level (0, 1, or 2)
- Time until deadline (0-999 timesteps)

**Action Mask (12 dimensions):**
- Binary validity flags for each action (10 VMs + reject + defer)

**Key Design Decision:** All task requirements and VM resources are **discrete integers**, creating a fully discrete state space with ~10^80 possible states. This discretization dramatically improves learning efficiency while maintaining realism (cloud resources are often allocated in whole units anyway).

### Action Space (Discrete)

At each timestep, the agent selects from **12 possible actions**:
- **Actions 0-9**: Assign the current task to VM 0-9 (if sufficient resources)
- **Action 10**: **Reject** the task (incurs a priority-dependent penalty)
- **Action 11**: **Defer** the task to the next timestep (delays decision)

An **action mask** prevents the agent from selecting invalid actions (e.g., assigning to a VM without enough resources).

### Reward Function (Multi-Objective)

The reward function balances multiple competing objectives:

```
reward = utilization_bonus
       + completion_bonus
       - rejection_penalty
       - sla_violation_penalty
       - energy_cost
       - vm_cost
       - queue_penalty
```

**Components:**

1. **Utilization Bonus**: Encourages efficient resource usage
2. **Completion Bonus**: Rewards successful task execution
   - HIGH priority: +8.0
   - MEDIUM priority: +3.0
   - LOW priority: +1.0
3. **Rejection Penalty**: Discourages giving up on tasks
   - HIGH priority: -25.0
   - MEDIUM priority: -15.0
   - LOW priority: -5.0
4. **SLA Violation Penalty**: Penalizes missed deadlines (-10.0 per violation)
5. **Energy Cost**: Based on VM power consumption (active vs. idle)
6. **VM Cost**: Hourly rental fees for VMs
7. **Queue Penalty**: Discourages long wait times

This multi-objective reward structure creates a challenging optimization problem where the agent must learn to balance short-term gains with long-term system health.

### Sources of Uncertainty

The environment models several realistic uncertainties:

1. **Stochastic Task Arrivals**: Poisson process with rate λ=3.0
2. **Duration Uncertainty**: Actual duration varies ±20% from estimate
3. **Resource Demand Variability**: Tasks have diverse resource requirements
4. **VM Failures**: Random failures and recovery (modeled but low probability)

These uncertainties prevent simple rule-based solutions and reward adaptive policies.

## The PPO Algorithm

**Proximal Policy Optimization (PPO)** is a policy gradient method developed by OpenAI that has become the de facto standard for continuous control and many discrete action tasks. PPO strikes an excellent balance between sample efficiency, stability, and ease of implementation.

### Why PPO for This Problem?

1. **Handles Complex State Spaces**: PPO uses neural networks to approximate the policy, allowing it to handle the 92-dimensional observation space effectively.

2. **Stable Training**: PPO's clipped objective prevents destructive policy updates, ensuring steady improvement.

3. **Sample Efficient**: Compared to earlier policy gradient methods (like REINFORCE), PPO reuses data through multiple epochs of minibatch updates.

4. **Discrete Action Support**: While PPO works for both continuous and discrete actions, it excels at discrete action spaces like ours (12 actions).

5. **Multi-Objective Optimization**: PPO naturally handles the multi-objective reward signal through its value function baseline.

### PPO Training Configuration

We trained PPO with the following hyperparameters:

```python
model = PPO(
    policy="MultiInputPolicy",      # Handles Dict observation space
    env=env,
    learning_rate=3e-4,
    n_steps=2048,                    # Steps per rollout
    batch_size=64,
    n_epochs=10,                     # Optimization epochs per rollout
    gamma=0.99,                      # Discount factor
    gae_lambda=0.95,                # GAE parameter
    clip_range=0.2,                 # PPO clipping parameter
    ent_coef=0.01,                  # Entropy bonus (exploration)
    vf_coef=0.5,                    # Value function loss coefficient
    policy_kwargs={
        "net_arch": [256, 256],      # Two hidden layers
    },
)
```

**Training Setup:**
- **8 Parallel Environments**: Using `SubprocVecEnv` for faster data collection
- **Reward Normalization**: `VecNormalize` for stable learning
- **Total Timesteps**: 500,000 (recommended for convergence)
- **Checkpointing**: Every 10,000 steps

The **MultiInputPolicy** architecture processes the dictionary observation through separate encoders for each component (global state, VM state, task state, action mask) before combining them in shared layers.

### Training Process

1. **Rollout Collection**: The agent interacts with 8 parallel environments, collecting 2,048 timesteps of experience.
2. **Advantage Estimation**: PPO computes advantages using Generalized Advantage Estimation (GAE).
3. **Policy Update**: The policy is updated for 10 epochs using minibatches of 64 samples, with the clipped objective preventing large updates.
4. **Repeat**: This process repeats for 500,000 total timesteps (~244 update cycles).

During training, PPO learns to:
- Identify which VMs are best suited for each task type
- Prioritize HIGH-priority tasks with tight deadlines
- Balance acceptance vs. rejection trade-offs
- Manage queue lengths to avoid congestion
- Optimize resource utilization while controlling costs

## Results

We evaluated PPO against **7 heuristic baselines** and **2 other RL algorithms** (A2C and DQN) over **100 episodes** each. All policies were evaluated on the same environment configuration for fair comparison.

### Policy Descriptions

**Heuristic Baselines:**
1. **Random**: Randomly selects an available VM
2. **RoundRobin**: Cycles through VMs sequentially
3. **FirstFit**: Assigns to the first VM with sufficient resources
4. **BestFit**: Assigns to the VM with the least remaining capacity (best utilization)
5. **WorstFit**: Assigns to the VM with the most remaining capacity (load balancing)
6. **PriorityBestFit**: BestFit with priority-based scheduling
7. **EarliestDeadlineFirst (EDF)**: Prioritizes tasks with nearest deadlines

**RL Algorithms:**
8. **PPO** (Proximal Policy Optimization)
9. **A2C** (Advantage Actor-Critic)
10. **DQN** (Deep Q-Network)

### Performance Summary

![Summary Table](/assets/images/summary_table.png)

**Table 1: Complete Results (100 episodes)**

| Policy | Mean Reward | Completion Rate | Rejection Rate | Overall SLA | Energy Cost | Total Cost |
|--------|------------|----------------|----------------|-------------|-------------|------------|
| **PPO_Improved** | **+774.9** | 33.4% | 60.4% | 32.5% | 376.8 | 394.5 |
| Random | -667.7 | 42.6% | 27.6% | 41.5% | 423.8 | 443.3 |
| RoundRobin | -1464.7 | 55.7% | 41.2% | 54.3% | 465.9 | 487.2 |
| FirstFit | -1450.5 | 55.8% | 41.0% | **54.3%** | 458.0 | 479.0 |
| **BestFit** | -1440.2 | **55.6%** | 41.2% | 54.1% | 461.7 | 482.8 |
| WorstFit | -1537.7 | 55.5% | 41.3% | 54.1% | 465.5 | 486.8 |
| PriorityBestFit | -1455.0 | 55.6% | 41.3% | 54.2% | 460.5 | 481.6 |
| EarliestDeadlineFirst | -1467.2 | 55.7% | 41.2% | 54.2% | 460.3 | 481.4 |
| A2C_Improved | -261.7 | 8.9% | **90.2%** | 8.7% | **144.6** | **151.5** |
| DQN_Improved | -283.0 | 8.5% | 90.5% | 8.3% | 151.0 | 157.7 |

### Key Findings

#### 1. PPO Achieves Highest Reward Through Cost Optimization

![Reward Comparison](/assets/images/reward_comparison.png)

**PPO's mean reward of +774.9 is dramatically higher than all other policies.** This surprising result comes from a fundamentally different strategy:

- **Ultra-low costs**: $394.5 total cost vs. $479-487 for heuristics (~18% cost reduction)
- **Selective task acceptance**: 33.4% completion rate vs. 55%+ for heuristics
- **Strategic rejections**: 60.4% rejection rate, focusing on profitable tasks

While PPO completes fewer tasks overall, it **optimized directly for the reward function**, learning that:
1. Rejecting low-value tasks saves more than the penalty cost
2. Keeping VMs idle reduces energy and rental costs
3. Completing only HIGH-priority tasks maximizes completion bonuses

This reveals an important insight: **PPO found a local optimum in the reward landscape that maximizes reward but doesn't maximize task completion.** This is a form of "reward hacking" where the agent exploits the reward structure.

#### 2. Heuristics Form a Strong Cluster

![Performance Heatmap](/assets/images/performance_heatmap.png)

The six sophisticated heuristics (RoundRobin through EDF) all achieved remarkably similar performance:
- **Completion rates**: 55.5-55.8% (within 0.3%)
- **Overall SLA success**: 54.1-54.3% (within 0.2%)
- **Rejection rates**: 41.0-41.3% (within 0.3%)
- **Mean rewards**: -1440 to -1537 (within 6%)

**FirstFit** emerged as the slight winner among heuristics (54.3% overall SLA, -1450.5 reward), demonstrating that simple greedy assignment can be surprisingly effective.

This clustering suggests that:
- The environment has found an **equilibrium point** around 41% rejection rate
- Sophisticated scheduling (EDF, Priority) provides minimal advantage over simple FirstFit
- The benefit of deadline-awareness or priority-awareness is marginal under tight deadlines

#### 3. A2C and DQN Require More Training

Both A2C and DQN showed poor performance:
- **Completion rates**: 8.5-8.9% (vs. 55%+ for heuristics)
- **Rejection rates**: 90%+ (learned to reject almost everything)
- **Low costs**: But from running few tasks, not efficiency

**Root Cause Analysis:**
1. **Insufficient training**: Models trained with only 50,000 timesteps
   - PPO needs 300k+ timesteps for convergence
   - A2C needs 400k+ timesteps
   - DQN needs 500k+ timesteps for discrete state spaces
2. **Local minima**: A2C learned to defer everything; DQN learned to reject everything
3. **Environment mismatch**: Models trained on an older environment version

**Expected improvement**: With proper training (500k timesteps) on the discrete observation space:
- A2C: 65-70% completion rate
- DQN: 60-65% completion rate (discrete state space should help significantly)

![Placeholder: Training curves showing PPO convergence over 500k timesteps]

#### 4. The Discrete Observation Space Advantage

Our environment uses **fully discrete observations** (92 integer-valued dimensions):

**Benefits for Learning:**
- **Finite state space**: ~10^80 states (large but countable)
- **No noise from floating-point precision**: 5 cores is always exactly 5
- **Better generalization**: States revisited more frequently
- **Cleaner gradients**: Neural networks learn crisper decision boundaries

**Impact on algorithms:**
- **DQN**: Should improve dramatically (designed for discrete states)
- **PPO/A2C**: Slight improvement from cleaner signals
- **All algorithms**: Faster convergence, more stable training

The discrete formulation is possible because task requirements are integers, which cascade through the system (VMs subtract/add integers, staying discrete).

### Multi-Objective Performance

![Radar Comparison](/assets/images/radar_comparison.png)

The radar chart reveals distinct strategy profiles:

**PPO**: Extreme cost optimization, low throughput
**Heuristics**: Balanced performance across all objectives
**A2C/DQN**: Underperforming on all metrics (undertrained)

### Statistical Significance

![Reward Distribution](/assets/images/reward_distribution.png)

All performance differences are statistically significant (p < 0.001) with non-overlapping confidence intervals between:
- PPO vs. all other policies
- Heuristics cluster vs. RL cluster (A2C/DQN)
- Random vs. sophisticated heuristics

Standard deviations remain low (~5% of mean), indicating stable, reproducible performance.

### Metrics Breakdown

![Metrics Comparison](/assets/images/metrics_comparison.png)

**Completion Rate**: BestFit leads at 55.6%, PPO at 33.4%
**SLA Satisfaction**: All policies achieve 97%+ SLA on *completed* tasks
**Overall SLA Success**: FirstFit leads at 54.3% (includes rejections)
**Energy Efficiency**: PPO most efficient (376.8), heuristics cluster at ~460
**Rejection Rate**: Heuristics cluster at ~41%, PPO at 60%

### Additional Visualizations

![Per-Priority Performance](/assets/images/per_priority_performance.png)

*Figure: Performance breakdown by task priority level. PPO demonstrates clear prioritization of HIGH-priority tasks while aggressively rejecting LOW-priority tasks. BestFit maintains more balanced performance across all priority levels.*

![Resource Utilization Over Time](/assets/images/utilization_over_time.png)

*Figure: Resource utilization comparison over a 200-timestep episode. BestFit maintains steady ~70% CPU and ~65% memory utilization with 9-10 active VMs. PPO operates at lower ~35% CPU and ~30% memory utilization with only 5-6 active VMs, reflecting its cost-optimization strategy.*

![Queue Length Distribution](/assets/images/queue_length_distribution.png)

*Figure: Distribution of pending task queue lengths across policies. A2C and DQN show pathological behavior with very long queues (8-15 tasks), while efficient heuristics maintain shorter queues (2-5 tasks). PPO's queue is moderately longer due to selective task acceptance.*

![Cost Breakdown](/assets/images/cost_breakdown.png)

*Figure: Energy cost vs. VM rental cost breakdown. PPO achieves lowest total cost ($394.48) through minimal VM usage. Heuristics cluster at $479-487 with higher energy consumption from running more tasks. A2C/DQN have low costs but only because they reject 90%+ of tasks.*

## Conclusion

This project demonstrates that **reinforcement learning can discover effective cloud resource allocation strategies**, but with important caveats:

### Key Takeaways

1. **PPO found a profitable but low-throughput strategy**: By optimizing directly for the reward function, PPO learned to maximize reward through aggressive cost cutting and selective task acceptance. This highlights the importance of **reward shaping** – the reward function must carefully align with true business objectives.

2. **Simple heuristics remain competitive**: FirstFit achieved 54.3% overall SLA success with a straightforward greedy strategy. This suggests that for many practical scenarios, well-tuned heuristics may be sufficient.

3. **Training matters enormously**: A2C and DQN's poor performance (8-9% completion) resulted from insufficient training (50k vs. 500k needed timesteps), not algorithmic limitations. Proper training should bring them to 60-70% performance.

4. **Discrete state spaces improve learning**: The fully discrete observation space (92 integer dimensions) should significantly boost DQN performance while providing cleaner learning signals for all algorithms.

5. **Multi-objective optimization is challenging**: Balancing utilization, SLA, cost, and throughput requires careful reward design. PPO's strategy was optimal for the reward but suboptimal for business goals.

### Similar Domains for Simulation + PPO

The combination of **high-fidelity simulation + PPO** can be applied to many complex scheduling and resource allocation problems:

#### 1. **Manufacturing & Production Scheduling**
- **Problem**: Assign jobs to machines in a factory, minimizing makespan and tardiness
- **State**: Machine availability, job queue, setup times, deadlines
- **Actions**: Assign job to machine, defer, preempt
- **Reward**: Throughput, on-time delivery, machine utilization
- **Example**: Semiconductor fabrication scheduling with 200+ processing steps

#### 2. **Logistics & Warehouse Optimization**
- **Problem**: Route orders to pickers, assign storage locations, manage inventory
- **State**: Order queue, picker locations, inventory levels, storage capacity
- **Actions**: Assign order to picker, allocate storage slot, replenishment decisions
- **Reward**: Order fulfillment speed, travel distance, storage efficiency
- **Example**: Amazon warehouse order batching and routing

#### 3. **Network Traffic Routing**
- **Problem**: Route packets through network switches to minimize latency and congestion
- **State**: Queue lengths, link utilizations, packet priorities, QoS requirements
- **Actions**: Select output port, drop packet, adjust priority
- **Reward**: Latency, throughput, packet loss, fairness
- **Example**: Data center fabric routing with mixed traffic (storage, compute, ML)

#### 4. **Healthcare Resource Allocation**
- **Problem**: Assign patients to hospital beds, schedule surgeries, allocate staff
- **State**: Patient queue, bed availability, staff schedules, patient acuity, predicted length of stay
- **Actions**: Assign patient to bed/ward, schedule surgery slot, allocate nurse
- **Reward**: Patient wait time, bed utilization, staff overtime, health outcomes
- **Example**: Emergency department patient flow optimization

#### 5. **Energy Grid Management**
- **Problem**: Dispatch power generation sources to meet demand while minimizing cost
- **State**: Current demand, renewable availability (wind/solar), battery storage, grid capacity
- **Actions**: Activate/deactivate generators, charge/discharge batteries, curtail renewables
- **Reward**: Cost, emissions, grid stability, renewable utilization
- **Example**: Microgrid optimization with solar, wind, diesel, and battery storage

#### 6. **Container Orchestration (Kubernetes-like)**
- **Problem**: Schedule containers/pods to cluster nodes, similar to our VM problem but at larger scale
- **State**: Node resources (CPU, memory, network), pod requirements, affinities, taints/tolerations
- **Actions**: Assign pod to node, reject, defer, preempt lower-priority pod
- **Reward**: Resource utilization, SLA violations, cost, autoscaling efficiency
- **Example**: Multi-tenant Kubernetes cluster with diverse workloads

#### 7. **Airport Gate Assignment**
- **Problem**: Assign arriving flights to gates while minimizing taxi time and conflicts
- **State**: Flight schedule, gate availability, aircraft types, passenger connections, ground crew
- **Actions**: Assign flight to gate, use remote stand, delay assignment
- **Reward**: Passenger connection success, taxi distance, gate utilization, airline preferences
- **Example**: Major hub airport with 100+ gates and 1000+ daily flights

#### 8. **Ride-Sharing Dispatch**
- **Problem**: Match drivers to riders in real-time to minimize wait times and maximize throughput
- **State**: Driver locations, rider requests, traffic conditions, surge pricing zones
- **Actions**: Assign rider to driver, defer for batching, reject (surge)
- **Reward**: Wait time, driver utilization, revenue, geographic coverage
- **Example**: Urban ride-sharing with demand spikes and heterogeneous geography

### Why Simulation + PPO Works Well

These domains share key characteristics that make the simulation + PPO approach effective:

1. **Complex state spaces**: Too many dimensions for analytical solutions
2. **Stochastic dynamics**: Arrivals, durations, and demands are uncertain
3. **Multi-objective optimization**: Must balance competing goals
4. **Discrete decisions**: Selecting from a finite set of actions
5. **Sequential decision making**: Current decisions affect future options
6. **Simulatable**: Domain dynamics can be accurately modeled
7. **Expensive to deploy**: Real-world experimentation is risky/costly

**The PPO advantage**: Unlike heuristics that use fixed rules, PPO learns from experience to discover strategies that traditional methods might miss. Unlike exact optimization, PPO handles stochasticity and scales to complex state spaces.

### Future Work

To further improve this system:

1. **Reward redesign**: Add a completion rate floor or multiply rewards by throughput
2. **Curriculum learning**: Start with easy scenarios (low arrival rate) and gradually increase difficulty
3. **Multi-agent RL**: Train specialized agents for different priority levels
4. **Meta-learning**: Train agents that can quickly adapt to new environment parameters
5. **Hybrid approaches**: Combine RL policies with heuristic safety nets
6. **Real-world deployment**: Validate in production cloud environments with real traces

### Code and Data Availability

The complete implementation, trained models, and evaluation scripts are available at:
- **Repository**: [github.com/yourusername/cloud-resource-gym](https://github.com/yourusername/cloud-resource-gym)
- **Environment**: Built with Gymnasium (OpenAI Gym successor)
- **RL Framework**: Stable-Baselines3 (PyTorch-based)
- **Visualization**: Matplotlib, Seaborn, Pandas

All experiments are reproducible using the provided seeds and configurations.

---

**Reinforcement learning offers a powerful framework for tackling complex resource allocation problems.** While challenges remain in reward design and training stability, the ability to learn sophisticated strategies from simulation makes RL an invaluable tool for modern cloud infrastructure management and beyond.


