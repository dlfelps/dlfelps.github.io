---
title: Experiment 16
tags: few-shot computer-vision ml-portfolio meta-learning
---

This post explores unique concepts in few-shot learning.

## Introduction

Few-shot learning describes the situation where a classifier must generalize to new classes using only a small number of examples of each new class. It represents scenarios where data collection (or annotation) is costly for the classes you care about, but you have access to similar data. There are some unique terms associated with few-shot learning, which we will introduce next.

In standard supervised learning, the training and testing sets contain the SAME classes. The classifier is tested for its ability to discriminate between KNOWN classes.

![supervised learning](/assets/images/supervised_learning.PNG "supervised learning")

[image reference](https://ieeexplore.ieee.org/document/10287966)

In few-shot learning, the training and testing sets contain DIFFERENT classes. A few-shot classifier is tested for its ability to discriminate UNKNOWN classes given only a few examples. These samples are further divided into the support set and query set. The support set contains labeled examples (for tuning the few-shot classifier) and the query set contains unlabeled samples for evaluation. The number of classes in the support set is denoted by N and the number of examples per class is denoted by K. A 5-way 1-shot method describes a method that has (N=5) classes with (K=1) examples per class.

![few_shot_learning](/assets/images/few_shot_learning.PNG "few_shot_learning")

[image reference](https://ieeexplore.ieee.org/document/10287966)





## Episodic training (meta-learning)
(comparison with transfer learning approach)
https://arxiv.org/pdf/2204.11181
 To simulate generalization challenges at test times, such strategies build sequences of artificially balanced few-shot tasks (or episodes) during base training, each containing both query and support samples.
 
 
https://arxiv.org/pdf/1703.05175
Vinyals et al. [29] proposed matching networks, which uses an attention mechanism over a learned embedding of the
labeled set of examples (the support set) to predict classes for the unlabeled points (the query set).
Matching networks can be interpreted as a weighted nearest-neighbor classifier applied within an
embedding space. Notably, this model utilizes sampled mini-batches called episodes during training,
where each episode is designed to mimic the few-shot task by subsampling classes as well as data
points. The use of episodes makes the training problem more faithful to the test environment and
thereby improves generalization.


## Transductive vs inductive learning-based approaches
https://arxiv.org/pdf/2204.11181
 In few-shot learning, transductive inference has access to exactly the same training and testing data as its inductive counterpart3 . The difference is that it classifies all the unlabeled query samples of each single few-shot task jointly, rather than one sample at a time.

https://arxiv.org/pdf/1805.10002
One way to achieve larger improvements with limited amount of training
data is to consider relationships between instances in the test set and thus predicting them as a whole,
which is referred to as transduction, or transductive inference. In previous work (Joachims, 1999;
Zhou et al., 2004; Vapnik, 1999), transductive inference has shown to outperform inductive methods
which predict test examples one by one, especially in small training sets

https://en.wikipedia.org/wiki/Transduction_(machine_learning)
 transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases

# Experiment

## CUB dataset

## Prototype model

### relationship to image retrieval
https://arxiv.org/pdf/1904.04232v2
baseline++ prototype


### Conclusion

The code for this experiment can be found at my [ML portfolio website](https://github.com/dlfelps/ml_portfolio).


