<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://dlfelps.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dlfelps.github.io/" rel="alternate" type="text/html" /><updated>2025-08-22T19:49:55+00:00</updated><id>https://dlfelps.github.io/feed.xml</id><title type="html">.NET Experiments</title><subtitle>Bill Nye meets Bill Gates ;)</subtitle><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><entry><title type="html">Experiment 21</title><link href="https://dlfelps.github.io/2025/08/21/mobility.html" rel="alternate" type="text/html" title="Experiment 21" /><published>2025-08-21T00:00:00+00:00</published><updated>2025-08-21T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/08/21/mobility</id><content type="html" xml:base="https://dlfelps.github.io/2025/08/21/mobility.html"><![CDATA[<p>This post converts GPS trajectory data into learned embedded features to reveal human mobility patterns, enabling applications in urban planning, personalized services, and behavioral analysis.</p>

<h1 id="introduction">Introduction</h1>

<p>Human mobility patterns tell fascinating stories. Where we go, when we travel, and how we move through space reveals insights about urban planning, social behavior, economic activity, and even public health. But extracting meaningful patterns from raw GPS trajectories is challenging‚Äîuntil now.</p>

<p>In this post, I‚Äôll walk you through an exciting deep learning project that transforms complex mobility data into intelligent embeddings using a hybrid transformer-LSTM architecture. We‚Äôll explore how this approach can revolutionize everything from urban planning to personalized location services.</p>

<h2 id="the-challenge-making-sense-of-mobility-data">The Challenge: Making Sense of Mobility Data</h2>

<p>GPS trajectories are deceptively complex. A single user‚Äôs movement data contains:</p>
<ul>
  <li><strong>Spatial information</strong>: Latitude and longitude coordinates</li>
  <li><strong>Temporal patterns</strong>: When movements occur, day-of-week effects</li>
  <li><strong>Sequential dependencies</strong>: How current location influences future destinations</li>
  <li><strong>Individual behaviors</strong>: Personal routines and preferences</li>
</ul>

<p>Traditional analysis methods struggle to capture these multi-dimensional relationships. That‚Äôs where deep learning comes in.</p>

<h2 id="introducing-lymob-4cities-a-rich-mobility-dataset">Introducing LYMob-4Cities: A Rich Mobility Dataset</h2>

<p>Our project leverages the <strong>LYMob-4Cities dataset</strong>, a comprehensive collection of anonymized mobility traces from four major cities. This dataset provides:</p>

<ul>
  <li><strong>Multi-city coverage</strong>: Diverse urban environments with different characteristics</li>
  <li><strong>Temporal depth</strong>: Extended observation periods capturing seasonal and weekly patterns</li>
  <li><strong>User diversity</strong>: Thousands of anonymized users with varying mobility behaviors</li>
  <li><strong>High resolution</strong>: Detailed GPS coordinates with timestamp information</li>
</ul>

<p>The dataset structure includes:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">uid</code>: Anonymized user identifier</li>
  <li><code class="language-plaintext highlighter-rouge">d</code>: Day number in the observation period</li>
  <li>Geographic coordinates (latitude/longitude)</li>
  <li>Temporal metadata including timestamps and day-of-week indicators</li>
</ul>

<p>This rich dataset enables us to build models that generalize across different urban contexts while respecting user privacy through anonymization.</p>

<h2 id="the-architecture-hybrid-transformer-lstm-for-mobility">The Architecture: Hybrid Transformer-LSTM for Mobility</h2>

<p>Our approach combines the best of both worlds: transformers for attention-based pattern recognition and LSTMs for sequential modeling.</p>

<h3 id="core-components">Core Components</h3>

<p><strong>1. Multi-Modal Embeddings</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Spatial embedding: lat/lon ‚Üí dense vector
</span><span class="n">spatial_emb</span> <span class="o">=</span> <span class="nc">SpatialEmbedding</span><span class="p">(</span><span class="n">coordinates</span><span class="p">)</span>

<span class="c1"># Temporal embedding: time features ‚Üí dense vector  
</span><span class="n">temporal_emb</span> <span class="o">=</span> <span class="nc">TemporalEmbedding</span><span class="p">(</span><span class="n">timestamps</span><span class="p">,</span> <span class="n">weekend_flags</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>2. Cross-Attention Mechanism</strong>
The model learns how spatial and temporal features interact through dedicated attention layers, capturing patterns like ‚Äúthis user typically visits restaurants on Friday evenings.‚Äù</p>

<p><strong>3. Transformer Blocks</strong>
Multi-head self-attention processes the entire trajectory sequence simultaneously, identifying important locations and movement patterns.</p>

<p><strong>4. Bidirectional LSTM</strong>
Sequential processing captures temporal dependencies and generates the final 512-dimensional embedding that summarizes the entire trajectory.</p>

<h3 id="why-this-architecture-works">Why This Architecture Works</h3>

<ul>
  <li><strong>Transformers</strong> excel at identifying important locations through attention</li>
  <li><strong>LSTMs</strong> capture sequential dependencies in movement</li>
  <li><strong>Cross-attention</strong> learns spatio-temporal interactions</li>
  <li><strong>Bidirectional processing</strong> considers both past and future context</li>
</ul>

<h2 id="the-complete-pipeline">The Complete Pipeline</h2>

<h3 id="stage-1-model-training">Stage 1: Model Training</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train_s2s.py
</code></pre></div></div>

<p>The training process:</p>
<ul>
  <li>Loads mobility data from multiple cities</li>
  <li>Applies data augmentation (Gaussian noise) for robustness</li>
  <li>Trains the hybrid model with early stopping</li>
  <li>Saves the best checkpoint for inference</li>
</ul>

<p><strong>Key Innovation</strong>: The model uses only users with ID &lt; 4000 for training, reserving higher IDs for testing‚Äîensuring true out-of-sample evaluation.</p>

<h3 id="stage-2-embedding-generation">Stage 2: Embedding Generation</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python inference.py
</code></pre></div></div>

<p>This stage:</p>
<ul>
  <li>Processes test users to generate trajectory embeddings</li>
  <li>Applies PCA to reduce dimensionality for visualization</li>
  <li>Creates analysis outputs including scatter plots and statistical summaries</li>
</ul>

<p><strong>Technical Detail</strong>: The 512-dimensional embeddings come from a bidirectional LSTM with 256 hidden units, effectively doubling the representation capacity.</p>

<h3 id="stage-3-interactive-exploration">Stage 3: Interactive Exploration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python interactive_plot.py
</code></pre></div></div>

<p>The crown jewel: a Dash web application that enables:</p>
<ul>
  <li>Interactive exploration of embedding space</li>
  <li>Point selection and trajectory heatmap generation</li>
  <li>Real-time analysis of mobility patterns</li>
</ul>

<p><img src="/assets/images/Screenshot_20-8-2025_15056_127.0.0.1.jpeg" alt="Interactive Dashboard" />
<em>Interactive dashboard showing PCA projections (left) and trajectory heatmaps (right)</em></p>

<h2 id="remarkable-results-what-the-embeddings-reveal">Remarkable Results: What the Embeddings Reveal</h2>

<p>The PCA analysis reveals fascinating insights:</p>

<p><strong>Clustering by Behavior</strong>: Users with similar mobility patterns cluster together in embedding space, even across different cities.</p>

<p><strong>Spatial Preservation</strong>: Geographic relationships are maintained‚Äîusers who frequent similar areas have similar embeddings.</p>

<p><strong>Temporal Patterns</strong>: Weekend vs. weekday behaviors create distinct embedding regions.</p>

<p><strong>Generalization</strong>: The model successfully captures mobility patterns for completely unseen users.</p>

<h2 id="real-world-applications">Real-World Applications</h2>

<p>This technology opens doors to numerous applications:</p>

<h3 id="Ô∏è-urban-planning">üèôÔ∏è <strong>Urban Planning</strong></h3>
<ul>
  <li><strong>Traffic optimization</strong>: Predict congestion patterns before infrastructure changes</li>
  <li><strong>Public transit</strong>: Design routes based on actual movement patterns</li>
  <li><strong>Zoning decisions</strong>: Understand how people actually use urban spaces</li>
</ul>

<h3 id="-personalized-services">üì± <strong>Personalized Services</strong></h3>
<ul>
  <li><strong>Location recommendations</strong>: ‚ÄúUsers with similar patterns also enjoyed‚Ä¶‚Äù</li>
  <li><strong>Predictive routing</strong>: Anticipate destinations based on historical behavior</li>
  <li><strong>Context-aware apps</strong>: Adapt interfaces based on movement patterns</li>
</ul>

<h3 id="-anomaly-detection">üö® <strong>Anomaly Detection</strong></h3>
<ul>
  <li><strong>Security applications</strong>: Identify unusual movement patterns</li>
  <li><strong>Public health</strong>: Track population mobility during emergencies</li>
  <li><strong>Fraud detection</strong>: Verify location-based claims</li>
</ul>

<h3 id="-business-intelligence">üìä <strong>Business Intelligence</strong></h3>
<ul>
  <li><strong>Retail analytics</strong>: Understand customer foot traffic patterns</li>
  <li><strong>Real estate</strong>: Evaluate locations based on mobility data</li>
  <li><strong>Marketing</strong>: Target audiences based on movement behaviors</li>
</ul>

<h3 id="-research-applications">üî¨ <strong>Research Applications</strong></h3>
<ul>
  <li><strong>Social science</strong>: Study human behavior at scale</li>
  <li><strong>Transportation research</strong>: Validate mobility models</li>
  <li><strong>Economic analysis</strong>: Understand commuting and economic patterns</li>
</ul>

<h2 id="privacy-and-ethics-a-responsible-approach">Privacy and Ethics: A Responsible Approach</h2>

<p>Our implementation prioritizes privacy:</p>
<ul>
  <li><strong>Anonymized data</strong>: No personally identifiable information</li>
  <li><strong>Aggregated insights</strong>: Focus on population-level patterns</li>
  <li><strong>Secure processing</strong>: Local analysis without data sharing</li>
  <li><strong>Transparent methods</strong>: Open-source implementation for scrutiny</li>
</ul>

<h2 id="technical-innovation-why-this-matters">Technical Innovation: Why This Matters</h2>

<p>This project demonstrates several key advances:</p>

<p><strong>1. Multi-Modal Learning</strong>: Successfully combining spatial and temporal information in a unified architecture.</p>

<p><strong>2. Scalable Embeddings</strong>: 512-dimensional representations that capture complex mobility patterns while remaining computationally tractable.</p>

<p><strong>3. Interactive Analysis</strong>: Real-time exploration tools that make complex AI accessible to domain experts.</p>

<p><strong>4. Generalization</strong>: Models that work across different cities and user populations.</p>

<h2 id="getting-started-try-it-yourself">Getting Started: Try It Yourself</h2>

<p>The complete implementation is available with detailed documentation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install dependencies</span>
pip <span class="nb">install </span>torch pandas scikit-learn plotly dash

<span class="c"># Train the model</span>
python train_s2s.py

<span class="c"># Generate embeddings</span>
python inference.py

<span class="c"># Launch interactive dashboard</span>
python interactive_plot.py
</code></pre></div></div>

<p>The modular design makes it easy to:</p>
<ul>
  <li>Adapt to your own mobility datasets</li>
  <li>Experiment with different architectures</li>
  <li>Add new visualization components</li>
  <li>Integrate with existing systems</li>
</ul>

<h2 id="the-future-of-mobility-intelligence">The Future of Mobility Intelligence</h2>

<p>This project represents just the beginning. Future directions include:</p>

<ul>
  <li><strong>Federated learning</strong>: Training on distributed datasets while preserving privacy</li>
  <li><strong>Multi-modal integration</strong>: Incorporating weather, events, and social data</li>
  <li><strong>Real-time processing</strong>: Streaming analysis for live mobility insights</li>
  <li><strong>Causal modeling</strong>: Understanding why people move, not just where</li>
</ul>

<h2 id="conclusion-from-data-to-intelligence">Conclusion: From Data to Intelligence</h2>

<p>Human mobility data contains incredible insights waiting to be unlocked. By combining modern deep learning architectures with thoughtful data science, we can transform raw GPS coordinates into intelligent representations that reveal the hidden patterns of human behavior.</p>

<p>The implications extend far beyond technology‚Äîthis work can inform better cities, more personalized services, and deeper understanding of human society. As we continue to generate more location data through our digital lives, tools like these become essential for making sense of our increasingly connected world.</p>

<p>Whether you‚Äôre an urban planner designing the cities of tomorrow, a data scientist exploring new applications, or simply curious about the intersection of AI and human behavior, this project demonstrates the power of applying modern machine learning to one of humanity‚Äôs most fundamental activities: movement through space and time.</p>

<hr />

<p><em>Want to explore the code? Check out the <a href="https://github.com/dlfelps/lstm-trajectory">GitHub repository</a> for complete implementation details, documentation, and examples.</em></p>

<h2 id="about-the-dataset">About the Dataset</h2>

<p>The LYMob-4Cities dataset used in this project represents a significant contribution to mobility research, providing researchers with access to large-scale, anonymized mobility data from diverse urban environments. This type of dataset is crucial for developing and validating mobility models that can generalize across different cities and cultures.</p>

<p>For researchers interested in working with similar data, consider exploring:</p>
<ul>
  <li>Academic partnerships with transportation authorities</li>
  <li>Open mobility datasets from cities with data sharing initiatives</li>
  <li>Synthetic mobility data generation techniques</li>
  <li>Privacy-preserving data collection methods</li>
</ul>

<p>The responsible use of such datasets, with proper anonymization and ethical oversight, enables breakthrough research while protecting individual privacy‚Äîa balance that‚Äôs essential as we advance mobility intelligence.</p>

<h2 id="claude-gotchas">Claude Gotchas</h2>

<p>I was impressed by Claude Code‚Äôs ability to assist and expedite with the exploration of this dataset and creation of the LSTM model. I wanted to document a few gotchas that I came across during the development process that would have been difficult for a non-developer / non-ML person to identify since they didn‚Äôt cause any errors:</p>

<ul>
  <li>ISSUE #1 - The model was actually predicting the final location from the full trajectory (i.e. including the final location). <em>RESOLUTION #1 - I identified the problem by when I noticed the model converged quickly and had excellent performance on test datasets. I found the problem, told Claude, and Claude fixed it.</em></li>
  <li>ISSUE #2 - The LSTM used bad values for padding points. <em>RESOLUTION #2 - Trajectories are not the same length and the LSTM model requires that you pad short sequences to make them equal. One is supposed to choose padding values that do not appear in the real data. However, Claude chose ‚Äú0‚Äù for user_id, x, y, and time. These are all valid values. I manually changed them to acceptable values.</em></li>
  <li>ISSUE #3 - The model initially treated the user_id variable as a continuous variable instead of a categorical variable. <em>RESOLUTION #3 - When I pointed out the mistake Claude had a good suggestion to use an embedding layer and updated the code accordingly.</em></li>
  <li>ISSUE #4 - Claude chose the wrong normalization algorithm. <em>RESOLUTION #4 - I noticed that the first two PCA dimensions captured almost 97% of the variance. This was surprising given the complexity of the data. Upon further inspection, I noticed that before performing PCA Claude applied a matrix L2 norm to the trajectory embeddings. The correct normalization method is a zero-mean, unit-variance normalization method. I was able to tell Claude to replace the method.</em></li>
</ul>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="gps" /><category term="mobility" /><summary type="html"><![CDATA[This post converts GPS trajectory data into learned embedded features to reveal human mobility patterns, enabling applications in urban planning, personalized services, and behavioral analysis.]]></summary></entry><entry><title type="html">Experiment 20</title><link href="https://dlfelps.github.io/2024/12/01/scraper.html" rel="alternate" type="text/html" title="Experiment 20" /><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/12/01/scraper</id><content type="html" xml:base="https://dlfelps.github.io/2024/12/01/scraper.html"><![CDATA[<p>This post describes a command line tool I created that scrapes a website and generates reports.</p>

<h1 id="introduction">Introduction</h1>

<p>It is greatly rewarding to build a tool that solves a real problem. The scale of the problem dictates the number of potential users your tool can help. This project solves a small problem - improving the registration process for my son‚Äôs summer camps. The problem is mostly created by a poor website design that doesn‚Äôt allow parents to easily find the correct camps.</p>

<p>I built this tool to scrape the camp data on <a href="https://fairfax.usedirect.com/FairfaxFCPAWeb/Default.aspx">Fairfax County Park Authority</a> (FCPA) website and generate useful camp options. I also added new functionality like calculating the estimated commute times to each camp.</p>

<h2 id="goals">Goals</h2>
<p>The goals of this project are to:</p>
<ul>
  <li>Improve user experience registering for camps hosted by FCPA</li>
  <li>Demonstrate the use of developing a command line tool</li>
  <li>Demonstrate the 3rd party API (Google MAP API)</li>
  <li>Demonstrate report creation</li>
</ul>

<h2 id="fcpa-site-limitations">FCPA site limitations</h2>
<p>Why is this tool needed? Because FCPA offers hundreds of potential camps to register for and their filter options are rather limited. Here is a screenshot of the FCPA camp search site:</p>

<p><img src="/assets/images/fcpa_search.jpeg" alt="" /></p>

<p>At first glance the fields look reasonable. We should be able to narrow down the list of available camps and register for one. Let‚Äôs see what happens if I try to find a camp for my 6 year old son for the two days after New Years Day (Jan 2-3).</p>

<p><img src="/assets/images/fcpa_results.jpeg" alt="" /></p>

<p>Due to the broad age range filter (6-12 yrs old), only one of the first six camps listed in the results are appropriate for a 6 year old. Unfortunately that camp is for the wrong dates (January 20). This happens because the date filter only allows you to specify the date ‚Äústarting on or after‚Äù. So the best I can do is limit results from a starting date to the end of that month. This is especially frustrating when trying to book camps over the summer break. Typically I want to book one camp per week for the entire summer. To do this on the FCPA website I can change the ‚Äústarting on or after‚Äù date, but for the first week in a month I get all camps in that month (which could be more than 100 sessions!).</p>

<h2 id="desired-capabilities">Desired capabilities</h2>

<p>I would like to build a tool that can output a tailored report to facilitate selecting and registring for camps. Ideally I would be able to quickly update the data based on the current openings as classes can fill up quickly. So I need a way to:</p>
<ol>
  <li>Automatically scrape the camp data from the FCPA website</li>
  <li>Select camps by various criteria</li>
  <li>Generate a custom report</li>
</ol>

<h1 id="installation">Installation</h1>
<ol>
  <li>Clone the repo (git clone https://github.com/dlfelps/camp-little-scrape.git)</li>
  <li>Install dependencies (pip install -r requirements.txt)</li>
  <li>Install Chrome (https://www.google.com/chrome/)</li>
  <li>Sign up for a Google Maps API key (https://developers.google.com/maps)</li>
</ol>

<h1 id="usage">Usage</h1>

<h2 id="process-overview">Process overview</h2>

<p>The overall process is described in the flowchart below. I will demonstrate each step below in detail, but at a high level, the tool completes a series of steps to:</p>
<ol>
  <li>Scrape camp links</li>
  <li>Scrape camp details</li>
  <li>(optional) Calculate commute times to each camp</li>
  <li>Filter camps and generate report</li>
</ol>

<p><img src="/assets/images/camp%20scraper-2024-11-21-162211.svg" alt="" /></p>

<h2 id="step-1---scrape-camp-links">Step 1 - Scrape camp links</h2>
<p>The first step in the process uses <a href="https://selenium-python.readthedocs.io/">Selenium</a> to semi-automatically scrape the camp session links from the FCPA website.</p>
<ol>
  <li>launch jupyter lab</li>
  <li>open step1.ipynb</li>
  <li>Follow instructions in step1.ipynb to open remote Chrome browser</li>
  <li>Go to FCPA website and search for ALL camps</li>
  <li>Follow instructions in step1.ipynb to scrape each page</li>
</ol>

<p>This step produces a <code class="language-plaintext highlighter-rouge">links.txt</code> file containing all the camp session links.</p>

<h2 id="step-2---scrape-camp-details">Step 2 - Scrape camp details</h2>
<p>Steps 2-4 all use the command line gui provided by the camp.py file. It is built using <a href="https://typer.tiangolo.com/">Typer</a>. View the list of possible commands by calling <code class="language-plaintext highlighter-rouge">python camp.py --help</code></p>

<p><img src="/assets/images/gui_help.PNG" alt="" /></p>

<p>Step 2 is initiated by calling <code class="language-plaintext highlighter-rouge">python camp.py details</code>. This step scrapes the camp details (e.g. location, dates, times) for each camp found in the <code class="language-plaintext highlighter-rouge">links.txt</code>.</p>

<h2 id="step-3---optional-calculate-commutes">Step 3 - (optional) Calculate commutes</h2>

<p>The third step uses Google Maps API to calculate the typical daily commute to each camp. This calculation is the sum of the following distances between home (user specified) and camp location:</p>
<ul>
  <li>morning dropoff (arriving by 9 AM)</li>
  <li>morning return (departing at 9 AM)</li>
  <li>afternoon pickup (arriving by 4 PM)</li>
  <li>afternooon return (departing at 4 PM)</li>
</ul>

<p>Commute time is one of the most important factors when selecting camps. Since full day camps are only 7 hours, it would not be time efficient to spend 2 hours in the car, which is shockingly easy to do even staying within Fairfax county (where all FCPA camps are located). This step does require a Google Map API key, which can be created at https://developers.google.com/maps. The code retrieves your individial key from the environment variable <code class="language-plaintext highlighter-rouge">MAP_API_KEY</code>. Here is a <a href="https://lazyprogrammer.me/how-to-set-environment-variables-permanently-in-windows-linux-and-mac/">guide</a> if you need help.</p>

<h2 id="step-4---filter-camps-and-generate-report">Step 4 - Filter camps and generate report</h2>

<p>The final step filters the available camps to generate a report. We can see the full list of available options by looking at the report help command <code class="language-plaintext highlighter-rouge">python camp.py report --help</code>.</p>

<p><img src="/assets/images/gui_report_help.PNG" alt="" /></p>

<p>The filtering options for this step are as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Required?</th>
      <th style="text-align: center">Default value</th>
      <th style="text-align: center">Description</th>
      <th style="text-align: center">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">first_day</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">The camp must start on this date</td>
      <td style="text-align: center">‚Äú01/02/2025‚Äù</td>
    </tr>
    <tr>
      <td style="text-align: center">last_day</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">The camp must end on this date</td>
      <td style="text-align: center">‚Äú01/03/2025‚Äù</td>
    </tr>
    <tr>
      <td style="text-align: center">min_age</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">5 (years)</td>
      <td style="text-align: center">The camp must accept kids of this age</td>
      <td style="text-align: center">‚Äìmin-age=5</td>
    </tr>
    <tr>
      <td style="text-align: center">max_commute</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">120 (minutes)</td>
      <td style="text-align: center">The camp must not have an estimated daily commute greater than this many minutes</td>
      <td style="text-align: center">‚Äìmax-commute=120</td>
    </tr>
    <tr>
      <td style="text-align: center">remove_half_day</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">True</td>
      <td style="text-align: center">Half day camps (from 9-noon) are not shown</td>
      <td style="text-align: center">‚Äìremove-half-day</td>
    </tr>
    <tr>
      <td style="text-align: center">remove_schools</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">True</td>
      <td style="text-align: center">Camps located at schools are not shown</td>
      <td style="text-align: center">‚Äìremove-schools</td>
    </tr>
    <tr>
      <td style="text-align: center">show_full</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">False</td>
      <td style="text-align: center">Camps that are already fully registered are not shown</td>
      <td style="text-align: center">‚Äìno-show-full</td>
    </tr>
  </tbody>
</table>

<p>If your desired parameters are different from default, then change them when you call the report command, e.g. <code class="language-plaintext highlighter-rouge">python camp.py report "01/02/2025" "01/03/2025" --min-age=7</code>.</p>

<p>This will produce an HTML file in the reports directory with all matching camps:</p>

<p><img src="/assets/images/report.PNG" alt="" /></p>

<p>The report is generated using the <a href="https://www.yattag.org/">yattag</a> library to procedurally generate the HTML report from python code.
A few highlights of the report:</p>
<ul>
  <li>The camps are sorted by shortest commute first</li>
  <li>The report highlights which camps include a swimming break</li>
  <li>Each camp includes a session link for easy registration</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>Overall I found this side project to be quite rewarding. I learned how to:</p>
<ul>
  <li>scrape data from websites (selenium and httpx)</li>
  <li>parse structured data from websites (beautifulsoup)</li>
  <li>keep API keys secret using environment variables</li>
  <li>use 3rd party APIs</li>
  <li>generate HTML from Python</li>
</ul>

<p>Finally I found the tool to actually make finding and registering my son for camp much easier!</p>

<h2 id="possible-improvements">Possible improvements</h2>

<p>I was not able to find a way for <a href="https://typer.tiangolo.com/">Typer</a> to hide commands that were not yet applicable. Ideally, only the <code class="language-plaintext highlighter-rouge">details</code> command would be initially available (taking in the links.txt that was produced during the semi-manual scraping step). Then after creating ‚Äúcamps.pkl‚Äù and ‚Äúplaces.txt‚Äù the <code class="language-plaintext highlighter-rouge">commute</code> command would be available. Finally the <code class="language-plaintext highlighter-rouge">report</code> command could be used.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="scraper" /><category term="cli" /><category term="selenium" /><category term="httpx" /><category term="yattag" /><category term="typer" /><category term="backend" /><summary type="html"><![CDATA[This post describes a command line tool I created that scrapes a website and generates reports.]]></summary></entry><entry><title type="html">Experiment 19</title><link href="https://dlfelps.github.io/2024/08/01/generative.html" rel="alternate" type="text/html" title="Experiment 19" /><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/08/01/generative</id><content type="html" xml:base="https://dlfelps.github.io/2024/08/01/generative.html"><![CDATA[<p>This post describes the stable diffusion architecture for generative AI.</p>

<h1 id="introduction">Introduction</h1>

<p>I am going to introduce this post with a real conversation I had with Microsoft‚Äôs <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you explain Generative AI to me like I am 5 years old?</p>

  <p>Copilot:</p>

  <p>Certainly! Imagine you have a magical drawing book. When you open it, it creates new pictures all by itself! That‚Äôs what generative AI does‚Äîit makes up things like drawings, stories, or even songs without anyone telling it exactly what to create. It‚Äôs like having a creative friend who invents cool stuff just by thinking about it! üé®‚ú®</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Are you an example of a Generative AI?</p>

  <p>Copilot:</p>

  <p>Indeed! I‚Äôm like a friendly digital artist. When you ask me for something, I use my magical algorithms to create new sentences, poems, or even drawings. Just like a painter mixes colors to create beautiful pictures, I mix words and ideas to make interesting content! üé®‚ú®</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you generate an image of an Eastern Bluebird drinking a milkshake?</p>

  <p>Copilot:</p>

  <p><img src="/assets/images/bluebird_milkshake_dalle.PNG" alt="bluebird milkshake" /></p>
</blockquote>

<p>END OF CONVERSATION</p>

<p>I was impressed by the generated images. Behind the scenes, Copilot uses an OpenAI product called <a href="https://openai.com/dall-e-3/">DALLE-3</a> to generate images. Unfortunately DALLE-3 is not publicly available. So, for the remainder of this post, we will instead examine a closely related model called Stable Diffusion. How do they compare? Here is the output of Stable Diffusion for the same prompt above:</p>

<p><img src="/assets/images/bluebird_milkshake_stable.webp" alt="stable diffusion" width="400" /></p>

<p>Generally Stable Diffusion tends to produce images that are more photorealistic whereas DALLE-3 creates images that feel like computer-generated art.</p>

<h1 id="unconditional-image-generation-denoising-diffusion-probabilistic-models">Unconditional Image Generation (Denoising Diffusion Probabilistic Models)</h1>

<p>I will explain Stable Diffusion through a series of increasingly complex models. The first - and most foundational model -  is the Denoising Diffusion Probabilistic Model (DDPM), which serves as the core technology behind Stable Diffusion and DALLE-3.</p>

<p>DDPMs are a class of generative models that work by iteratively denoising a diffusion process, which involves adding noise to an image and then trying to remove it. The underlying theory of DDPMs is based on the idea that transforming a simple distribution, such as a Gaussian, through a series of diffusion steps can result in a complex and expressive image data distribution. This allows the model to generate new images by reversing the diffusion process, starting from the full Gaussian distribution and ending up with the image distribution.</p>

<p><img src="https://huggingface.co/blog/assets/78_annotated-diffusion/diffusion_figure.png" alt="forward diffusion" /></p>

<p>To further demonstrate the contribution of each step I will show what kind of generative images are possible using the CUB-2011 dataset. Our final goal will be to produce fully synthetic birds. In this first step we will only be able to generate small bird-like shapes using the DDPM architecture alone.</p>

<p><img src="/assets/images/generative_figure1.png" alt="ddpm" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The output of training the model is shown below. Here I take a snapshot of the sample images every 10 epochs to show the learning progression of the model.</p>

<p><img src="/assets/images/ddpm-64.gif" alt="ddpm animation" /></p>

<p>You can see birds if you squint, but the low resolution (64x64) makes these images less useful. DDPMs top out around (256x256) and become increasingly slow to train on a single GPU. We next explore two options for increasing the resolution.</p>

<h1 id="generating-higher-resolution-images">Generating Higher Resolution Images</h1>

<p>The following two sections offer alternatives to increasing the resolution of the generated images.</p>

<h2 id="upscaling">Upscaling</h2>

<p>The first approach uses image upscaling to increase the resolution of an image. Traditional upscaling methods, such as interpolation, don‚Äôt increase the inherent resolution of the image. The upscaler used here actually fills in missing details to increase resolution (NOTE: the upscaler itself is based on stable diffusion architecture). The architecture for the approach is outlined below:</p>

<p><img src="/assets/images/generative_figure2.png" alt="upscaling" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The benefit of this approach is that it uses the same DDPM from step 1 (which produces images of 64x64) and then the upscaler increases the resolution to 256x256.</p>

<p>The disadvantage of this approach is that the upscaler is not tuned specifically for your data (although it could be with additional training). It also makes a slow process (sampling the DDPM) even slower because you are performing two indepdenent steps (sampling then upscaling).</p>

<h2 id="latent-model">Latent model</h2>

<p>The second approach to increasing resolution was introduced in <a href="https://arxiv.org/abs/2112.10752">‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models‚Äù</a>. The idea behind this approach is to surround the DDPM with a pretrained variational autoencoder<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. The DDPM then learns to denoise the latent space (which also happens to be 64x64) instead of the image space. To create a new sample:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. generate random noise in the shape of the latent space
2. use the DDPM to denoise the input
3. use the decoder to transform the latent space back into image space (512x512)
</code></pre></div></div>

<p><img src="/assets/images/generative_figure3.png" alt="latent" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Unfortunately I did not have enough Colab credits to fully train this variant<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Although the model outputs images that are 512x512, I admit that they don‚Äôt look very bird-like. Here is a single full image created after 50 epochs of training:</p>

<p><img src="/assets/images/latent_bird.png" alt="latent image" /></p>

<h1 id="conditional-image-generation-stable-diffusion">Conditional Image Generation (Stable Diffusion)</h1>

<p>Up to this point we have not had any control over what kind of bird gets synthesized. The final step is to condition the DDPM on another input (usually text, but it could also be another image). NOTE: In order to train such a model, the training data would require both text and captions. The CUB-2011 dataset does not provide captions and I also discovered that significantly more compute is needed to train the latent models so I will use a pretrained Stable Diffusion model.</p>

<p><img src="/assets/images/generative_figure4.png" alt="stable diffusion" width="600" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Here is an example output:</p>

<p><img src="/assets/images/eastern_bluebird_stable.png" alt="stable diffusion output" /></p>

<p>I generated the image above with the implementation provided by Lyumin Zhang called <a href="https://github.com/lllyasviel/Fooocus">Fooocus</a>. It is a pre-tuned version of <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">StabilityAI‚Äôs Stable Diffusion XL model</a>. You can generate your own images using the provided Colab notebook -&gt; 
<a target="_blank" href="https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a></p>

<h1 id="conclusion">Conclusion</h1>

<p>Generative AI is currently experiencing a boom, with new research and products coming out every day. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>. I would like to leave you with a final conversation from Microsoft‚Äôs <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you write a poem about generative ai?</p>

  <p>Copilot:</p>

  <p>Sure, I‚Äôd love to! Here‚Äôs a poem about generative AI:</p>

  <p>In circuits deep, where data flows, A spark ignites, a thought that grows. From silicon dreams, a mind takes flight, Crafting worlds in the dead of night.</p>

  <p>With algorithms as its guiding star, It reaches realms both near and far. From pixels bright to words that sing, Generative AI creates everything.</p>

  <p>It learns from patterns, vast and wide, In every byte, where secrets hide. A painter‚Äôs brush, a poet‚Äôs pen, It brings to life what once was then.</p>

  <p>Yet in its core, a humble start, A tool to aid the human heart. For in each line and crafted scene, It mirrors dreams, both bold and serene.</p>

  <p>So here‚Äôs to code that dreams and dares, To generative AI, with whom we share, A future bright, where minds entwine, In harmony, both yours and mine.</p>
</blockquote>

<p>END OF CONVERSATION</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>This particular autoencoder preserves spatial information by compressing a 512x512x3 image into a latent space of 64x64x4.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>As a comparison, in <a href="https://arxiv.org/abs/2112.10752">‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models‚Äù</a>, they trained a latent model on FFHQ for 283 hours on an A100. This would cost an equivalent of $370 in Colab compute units so you will have to forgive me for stopping early!¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="generative" /><category term="stable-diffusion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes the stable diffusion architecture for generative AI.]]></summary></entry><entry><title type="html">Experiment 18</title><link href="https://dlfelps.github.io/2024/07/01/adversarial.html" rel="alternate" type="text/html" title="Experiment 18" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/07/01/adversarial</id><content type="html" xml:base="https://dlfelps.github.io/2024/07/01/adversarial.html"><![CDATA[<p>This post describes simple approaches to attack and defend machine learning models.</p>

<h1 id="introduction-to-adversarial-machine-learning">Introduction to adversarial machine learning</h1>
<p>Adversarial machine learning refers to a set of techniques that adversaries use to attack machine learning systems. These attacks exploit vulnerabilities in ML models, aiming to manipulate their behavior or compromise their performance. Adversarial attacks can be used against any system that employs machine learning, including finance, security, and autonomous systems.</p>

<h2 id="types-of-adversarial-attacks">Types of Adversarial Attacks</h2>
<p>There are several types of known attacks against machine learning models. Please see the NIST  <a href="https://csrc.nist.gov/pubs/ai/100/2/e2023/final">taxonomy</a> for a comprehensive breakdown with references. The remainder of the post focuses on the evasion attack, which modifies the input in a way that alters the prediction of the model under attack.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center"><strong>Poisoning</strong></th>
      <th style="text-align: center"><strong>Evasion</strong></th>
      <th style="text-align: center"><strong>Extraction</strong></th>
      <th style="text-align: center"><strong>Inference</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>during</em></td>
      <td style="text-align: center">Training</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
    </tr>
    <tr>
      <td style="text-align: center"><em>purpose</em></td>
      <td style="text-align: center">Mistrain the model</td>
      <td style="text-align: center">Deceive the model</td>
      <td style="text-align: center">Steal the model</td>
      <td style="text-align: center">Infer training data</td>
    </tr>
  </tbody>
</table>

<h2 id="threat-models">Threat models</h2>

<p>An important consideration when attacking/defending a machine learning model is the threat model. The two primary threat models are white-box and black-box attacks.</p>

<p><img src="/assets/images/white_box-Photoroom.png" alt="white box" /></p>

<blockquote>
  <p><strong>White-Box Attack:</strong> The attacker has <strong>full knowledge</strong> of the ML model‚Äôs architecture, parameters, and training data. e.g. <a href="https://arxiv.org/pdf/1412.6572v3.pdf">L-BFGS Attack</a>, <a href="https://arxiv.org/abs/1312.6199">Fast Gradient Sign Method</a></p>
</blockquote>

<p><img src="/assets/images/black_box-Photoroom.png" alt="black box" /></p>

<blockquote>
  <p><strong>Black-Box Attacks:</strong> The attacker has <strong>limited information</strong> about the ML model (e.g., input-output interactions).  e.g. <a href="https://arxiv.org/abs/1809.02861">Transfer Attack</a>, <a href="https://arxiv.org/abs/1708.03999">Score-based Black-Box Attacks</a>, <a href="https://arxiv.org/abs/1712.04248">Decision-based Attack</a></p>
</blockquote>

<p>Note that adversarial exmaples can be used under either threat model, however they are much more effective when an adversary has access to the internals of your model (i.e. white-box) and can verify that the attack works prior to deploying it.</p>

<h2 id="experimental-setup">Experimental setup</h2>

<p>As with a number of my previous posts, I will be using a pretrained <a href="https://github.com/osmr/imgclsmob/tree/master/pytorch#cub-200-2011">ResNet-18 model</a> trained on the <a href="https://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB2011 dataset</a>; this will serve as the model under attack.</p>

<h1 id="scenario-1-attacking-a-model-with-non-adversarial-methods">Scenario 1: Attacking a model with non-adversarial methods</h1>
<p>This ‚Äúattack‚Äù can also be seen as a model hardening test. I recommend running it on all computer vision models prior to deploying them. It tests the extent to which the model can handle common image transformations in the wild. Depending on the results, we might discover a cheap form of the evasion attack (applies to both black-box and white-box settings). The attack works as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select a common image transform (e.g. rotation, brightness, contrast)
  2. Select a random test image
  3. Run the test image through the model and save the predicted class
  4. Run the transformed test image through the model and save the predicted class
  5. If #3 and #4 are different then increase counter
  6. Repeat steps 2-5 100 times to estimate the effect of the transform
</code></pre></div></div>

<h2 id="results">Results</h2>
<p>The following plots visually demonstrate the effect of a single transform. The results (represented by the blue bar to the right of each image) represent the average performance of 100 test images. A higher bar signifies that the model is robust to that transform (i.e. the counter in step#5 was low).</p>

<p><img src="/assets/images/blur.png" alt="blur" /></p>

<p><img src="/assets/images/hue.png" alt="hue" /></p>

<p><img src="/assets/images/rotate.png" alt="rotate" /></p>

<p><img src="/assets/images/brightness.png" alt="brightness" /></p>

<p><img src="/assets/images/contrast.png" alt="contrast" /></p>

<p><img src="/assets/images/gamma.png" alt="gamma" /></p>

<p><img src="/assets/images/saturation.png" alt="saturation" /></p>

<h2 id="discussion">Discussion</h2>
<p>The model under attack was relatively robust to subtle image transforms and should be expected to perform well in the wild. Hue was the only transform that significantly impacted performance and that is expected since color is a differentiating factor in bird species. Can we use hue as a cheap form of adversarial attack? No, because the hue transform would also fool a human. In other words, the hue transform fundamentally changes the input so it isn‚Äôt ‚Äúfooling‚Äù anything. We next turn to more advanced methods.</p>

<h1 id="scenario-2-attacking-a-model-with-adversarial-methods">Scenario 2: Attacking a model with adversarial methods</h1>

<p>The goal of an evasion attack is to minimally modify an input (i.e. image) in such a way that the model alters its class prediction, but a human still percieves the original class. It‚Äôs this combination that makes the evasion attack so potent - humans don‚Äôt know anything is wrong and the machine is fooled. The attack succeeds because the altered image lies outside of the training distribution.</p>

<h2 id="pgd">PGD</h2>
<p>The Projected Gradient Descent (PGD) attack <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> is an iterative optimization technique that seeks to find the most adversarial perturbation within a predefined constraint set. Here is psuedocode for the algorithm:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select an unmodified image as the target
  2. Select a random point within the allowed perturbation region of the target image
  3. Perform gradient descent on the model's loss function
  4. Project the perturbed input back into the feasible set after each iteration
</code></pre></div></div>
<p>This iterative process ensures that the adversarial perturbation remains within the defined constraints around the original input, making the adversarial example both effective and imperceptible to human observers.</p>

<p>However, the PGD attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Over-optimization against one constraint</li>
  <li>Potential for unstable optimization</li>
  <li>Limited transferability to other models</li>
</ul>

<h3 id="example">Example</h3>

<p><img src="/assets/images/pgd_400.png" alt="pgd_400" /></p>
<blockquote>
  <p>A successful example of a PGD attack (left). The modified image was classified differently than the original image, but the modifications are barely visible to the human eye. The modifications added to the original image are magnified 10x in the (right) image.</p>
</blockquote>

<h2 id="carlini-wagner">Carlini-Wagner</h2>
<p>The Carlini &amp; Wagner (C&amp;W) attack is a powerful adversarial evasion attack that aims to generate adversarial examples that are misclassified by a target model. Adversarial examples are created by formulating an optimization problem, where the goal is to minimize the perturbation while ensuring the classifier mislabels the adversarial example. C&amp;W is particularly notable for its ability to generate adversarial examples that are transferable across different models and defenses, making it a significant threat to the security of machine learning systems.</p>

<p>However, the C&amp;W attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Computational cost</li>
  <li>Requires domain-specific adaptations to shape the perturbations and the optimization problem depending on the type of data being manipulated</li>
</ul>

<h3 id="example-with-l0-norm">Example with L0 norm</h3>

<p><img src="/assets/images/carliniL0_392.png" alt="carliniL0_392" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. Although the modifications are mostly imperceptible, a small eye-like patch is visible in the magnitude-magnified image (right).</p>
</blockquote>

<h3 id="example-with-linf-norm">Example with LInf norm</h3>

<p><img src="/assets/images/carliniLinf_3207.png" alt="carliniLinf_3207" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. The modifications in this case are noticable and global in the original image and magnitude-magnified image (right).</p>
</blockquote>

<h1 id="scenario-3-defending-a-model-with-non-adversarial-methods">Scenario 3: Defending a model with non-adversarial methods</h1>

<p><img src="/assets/images/adv-flowchart.png" alt="adv-flowchart" /></p>

<p>This scenario explores the exent to which non-adversarial methods can be used to defend a model from a strong adversarial attack (e.g. PGD or Carlini-Wagner). The defense exploits the fact that the evasion attack itself is typically brittle. In other words, the attack noise was calculated to minimize the amount of change in the input space to achieve the desired result. Therefore, it stands to reason that the attack may not be able to withstand additional transformations. We constrain the suite of transformations considered in Scenario #1 to only those transformations that maintained greater than 99% performance on natural images. This allows us to limit the amount of false positives in our adversarial detector to less than 1%.</p>

<h2 id="results-1">Results</h2>
<p>In the tables below we report the prediction rates for the best parameter combination per transformation type (e.g. rotation, brightness).</p>

<h3 id="pgd-1">PGD</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.049505</td>
      <td style="text-align: center">0.950495</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.283019</td>
      <td style="text-align: center">0.716981</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.441441</td>
      <td style="text-align: center">0.553864</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.454545</td>
      <td style="text-align: center">0.545455</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.555556</td>
      <td style="text-align: center">0.444444</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-l0">CW-L0</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.114286</td>
      <td style="text-align: center">0.885714</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.6</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.558442</td>
      <td style="text-align: center">0.441558</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.561905</td>
      <td style="text-align: center">0.4334</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.741935</td>
      <td style="text-align: center">0.258065</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">0.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.815126</td>
      <td style="text-align: center">0.184874</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-linf">CW-LInf</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.819444</td>
      <td style="text-align: center">0.180556</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.910256</td>
      <td style="text-align: center">0.089744</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.939394</td>
      <td style="text-align: center">0.060606</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.943182</td>
      <td style="text-align: center">0.056818</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.944444</td>
      <td style="text-align: center">0.055556</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h2 id="discussion-1">Discussion</h2>

<p>The highest average difference between performance on natural images and adversarial images is achieved by rotating images by 7 degrees. This transformation is ideal from a false-positive point of view as all natural images are unphased by it. NOTE: this does not mean that the predicted class is correct, only that the predicted class is not altered by rotating the image 7 degrees. Our ability to detect adversarial examples depends on the attack:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">TP rate</th>
      <th style="text-align: center">FP rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">PGD</td>
      <td style="text-align: center">0.95</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-L0</td>
      <td style="text-align: center">0.885</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-LInf</td>
      <td style="text-align: center">0.18</td>
      <td style="text-align: center">0.0</td>
    </tr>
  </tbody>
</table>

<p>The results are quite promising for PGD and CW-L0, but the CW-LInf attack proved to be more resilient (to all tested transformations). The results appear to confirm our hypothesis about the fragility of the evasion attacks - more subtle attacks are more easily detected precisely because they are more subtle. In other words, if the attack has a low signal-to-noise ratio then rotating the image has a better chance to render it ineffective.</p>

<h1 id="scenario-4-defending-a-model-with-adversarial-methods">Scenario 4: Defending a model with adversarial methods</h1>

<p>Adversarial training is a technique used to enhance the robustness of machine learning models against adversarial examples. The simplest way to perform adversarial training is to include adversarial examples in the training set.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Train a model with the original training data
2. Use the model to generate adversarial examples
3. Add the adversarial examples (with correct labels) to the training data
4. Retrain the model
</code></pre></div></div>
<p>A more sophisticated approach called Robust training <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> alters the optimization process itself in order to make models more resilient to adversarial perturbations. These perturbations are carefully crafted to deceive the model into making incorrect predictions.</p>

<h2 id="comparing-detection-and-adversarial-training">Comparing detection and adversarial training</h2>

<p>Adversarial training strengthens models intrinsically, while detection approaches focus on identifying adversarial examples during inference. Both play crucial roles in addressing the challenges posed by adversarial attacks.</p>

<ul>
  <li>advantages of adversarial detection
    <ul>
      <li>can defend against white-box and black-box threat models</li>
      <li>can be used to protect existing/pretrained models</li>
      <li>may work against UNKNOWN attacks</li>
    </ul>
  </li>
  <li>advantages of adversarial training
    <ul>
      <li>moderate success against all KNOWN attacks</li>
    </ul>
  </li>
</ul>

<p>NOTE: If you are training your own model then you can do BOTH!</p>

<h1 id="conclusion">Conclusion</h1>
<p>This was a long post, so I will summarize the results here:</p>
<ol>
  <li>It is good practice to test your model‚Äôs performance against non-adversarial image transformations.</li>
  <li>Evasion attacks (e.g. PGD and CW) can be used to alter the predictions of a model.</li>
  <li>Some evasion attacks (e.g. PGD and CW-L0) can be detected at inference time due to their lack of robustness against non-adversarial image transformations.</li>
  <li>Other evasion attacks (e.g. CW-LInf) may be rendered inneffective using adversarial training.</li>
</ol>

<p>As ML adoption grows, understanding and addressing these vulnerabilities become crucial for maintaining system integrity and security. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="adversarial" /><category term="evasion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes simple approaches to attack and defend machine learning models.]]></summary></entry><entry><title type="html">Experiment 17</title><link href="https://dlfelps.github.io/2024/06/04/explainable.html" rel="alternate" type="text/html" title="Experiment 17" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/04/explainable</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/04/explainable.html"><![CDATA[<p>This post introduces Concept Bottleneck Models - an interpretable approach to machine learning.</p>

<h2 id="introduction-to-explainable-ai">Introduction to Explainable AI</h2>

<p>Explainable machine learning models describe how and why a prediction was made. Experts in explainable AI (XAI) make a distinction between interpretabiliy and explainability: <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253521002281">source</a></p>

<ul>
  <li>
    <p>Interpretability is the extraction of relevant sub-symbolic information from a machine-learning model concerning relationships either contained in data or learned by the model</p>
  </li>
  <li>
    <p>Explainability is the ability to translate this sub-symbolic information in a comprehensible manner through human-understandable language expressions</p>
  </li>
</ul>

<p>This blog post answers the following questions:</p>
<ol>
  <li>How do you make a machine learning model more interpretable and explainable?</li>
  <li>What are the tradeoffs and when should you do it?</li>
</ol>

<h2 id="experimental-study">Experimental study</h2>

<p>In order to demonstrate the benefits of XAI we will use a standard image classification task on well-known benchmark - <a href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a>, which contains 11,788 images of birds. This dataset is typically used for fine-grained classification since it contains over 200 species of birds. Additionally, for each image, it captures meta-data about each bird (i.e. 28 features including bill shape, wing shape, head color, etc.). This information will be critical to creating an interpretable model.
<img src="https://i0.wp.com/www.beyourownbirder.com/wp-content/uploads/2019/09/american-goldfinch-1.jpg" alt="American goldfinch" /></p>

<p><a href="https://i0.wp.com/www.beyourownbirder.com/wp-content/uploads/2019/09/american-goldfinch-1.jpg">Image source</a></p>

<h2 id="establishing-the-baseline">Establishing the baseline</h2>

<p>Before we build an interpretable model, we will establish the performance of a non-interpretable model. For this dataset we will use a ResNet-18 pretrained model from the <a href="https://github.com/osmr/imgclsmob">pytorchcv</a> package.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pytorchcv.model_provider import get_model 

cub_model = get_model('resnet18_cub', pretrained=True)
</code></pre></div></div>

<p>This model has 74.4% accuracy on the test set, which is impressive given that there are 200 classes and the classes are all similar because they are all birds. We will next describe how to adapt this model to be interpretable.</p>

<h2 id="concept-bottleneck-models">Concept Bottleneck Models</h2>

<p>Deep learning models are powerful due to their ability to learn features that are discriminative to the prediction task. However, these features are not interpretable by humans, leading to the so-called <a href="https://news.mit.edu/2022/machine-learning-explainability-0505">‚Äúblack box problem‚Äù</a>. In an effort to make deep learning models more transparent, Concept Bottleneck Models (CBMs) make use of a human-interpretable feature layer called the ‚Äúconcept layer‚Äù. This layer contains features that are meaningful to a human and representative of the features necessary for a human to perform the same kind of prediction. The final prediction is made based on the values of the concept layer. There are two primary advantages of this approach:</p>
<ol>
  <li>Users can understand the feature values immediately before the prediction layer</li>
  <li>Users can update the feature values to improve accuracy (sometimes called intervention)</li>
</ol>

<p><img src="/assets/images/cbm_cub.jpeg" alt="concept bottleneck model" title="concept bottleneck model" /></p>

<p><a href="https://arxiv.org/pdf/2007.04612">image reference</a></p>

<h3 id="training-a-concept-bottleneck-model">Training a Concept Bottleneck Model</h3>
<p>In the original <a href="https://arxiv.org/pdf/2007.04612">paper</a> the authors test three ways to train a Concept Bottleneck Model:</p>

<ul>
  <li>
    <p>Independent: learn a concept predictor (L<sub>C</sub>) and learn a task predictor (L<sub>Y</sub>)</p>
  </li>
  <li>
    <p>Sequential: learn a concept predictor (L<sub>C</sub>) then learn a task predictor (L<sub>Y</sub> <span>|</span> L<sub>C</sub>)</p>
  </li>
  <li>
    <p>Joint: simultaneously learn concept predictor (L<sub>C</sub>) and task predictor (L<sub>Y</sub> <span>|</span> L<sub>C</sub>)</p>
  </li>
</ul>

<p>The joint approach offered the best performance on the CUB dataset. However, because the concept and task predictors are trained simultaneously using a dual loss function is it prone to side-channel leaks, in which the task predictor drives the concept predictor to leak additional information about features that are not strictly related to the concepts. The extent to which this occurs affects the overall interpretability of the model.</p>

<p>Independent and sequential approaches offered similar performance on CUB and do not suffer from the side-channel leak problem. The independent approach is recommended for systems that anticipate high intervention rates because its task predictor is trained using ground truth concept values. The sequential approach is recommended when intervention is not likely and the concepts are difficult to predict. This allows the task predictor to learn to ignore concepts that may be difficult to predict.</p>

<h4 id="learning-the-concepts">Learning the concepts</h4>
<p>There are 28 concepts in the CUB-200 dataset. These are expert-selected concepts relevant to the problem of bird species identification.</p>

<p><img src="/assets/images/cub_attributes.jpeg" alt="bird attributes" title="bird attributes" /></p>

<p><a href="https://authors.library.caltech.edu/records/cvm3y-5hh21/files/CUB_200_2011.pdf?download=1">image reference</a></p>

<p>We will use the training data to learn to predict these features by creating one classifier per concept. The embedding features from the baseline ResNet-18 model serve as the input to each concept classifier. As you can see in the figure below, the ability to predict each concept varies greatly. This is partially due to the fact that the embedding space is optimized to differentiate at the class level (global prediction) rather than the individual concepts (local predictions).</p>

<p><img src="/assets/images/concept_plot.png" alt="concept predictions" title="concept predictions" /></p>

<p>Let‚Äôs take a closer look at a few examples:</p>

<ul>
  <li>Head Pattern (33% accuracy) - this concept is difficult to predict from the embedded features; the classifier only performs marginally better than the base rate (+7% better)</li>
  <li>Eye Color (80% accuracy) - this concept is the most accurately predicted concept; but it does not perform better than the base rate, suggesting that this feature is not informative and predicts that birds have black eyes regardless of input</li>
  <li>Crown color (55% accuracy) - although this concept is not among the most accurately predicted, it does perform significantly higher than the base rate (+30% better) suggesting that the embedded features at least partially capture this information</li>
</ul>

<h4 id="learning-the-classes">Learning the classes</h4>
<p>Because the performance of the concept predictors was relatively low, we will use a sequential training approach. After learning one concept predictor per concept L<sub>C<sub>i</sub></sub>, we use the predicted concepts (rather than the ground truth concepts) to learn the task predictor.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Interpretable features</th>
      <th style="text-align: center">Concept labels required</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">0.594</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.744</td>
    </tr>
  </tbody>
</table>

<p>We added interpretability to our model at the cost of about 25 points of accuracy. This would be an unacceptable tradeoff for most real-world applications. Even though the overall accuracy is low, if either the accuracy of the concept predictor or task predictor is independently high then they can still be useful in certain contexts.</p>
<ul>
  <li><strong>Expert assistant</strong>:   This scenario assumes the system is being used by a highly trained user (e.g. ornithologist for CUB). The concept predictor (with high accuracy) can save the expert time by pre-populating values for the concept features. The expert then performs a final review of the concept features and assigns the final class label (e.g. species of bird for CUB).</li>
  <li><strong>Psuedo-expert</strong>: This scenario assumes the system is being used by non-skilled users (e.g. Mechanical Turk workers). The worker is able to make simple observations about the image (e.g. what is the color of the bird‚Äôs wing, which of the following shapes most closely matches the bird‚Äôs beak, etc.). The task predictor (with high accuracy) then assigns the species of bird given the observations.</li>
</ul>

<h2 id="discussion">Discussion</h2>

<h3 id="limitations-of-cbms">Limitations of CBMs</h3>

<p>We have already observed the first limitation of Concept Bottleneck Models - they don‚Äôt tend to be as accurate as non-interpretable models for real-world applications. This gap can be reduced using a residual modeling technique such as the one proposed in <a href="https://arxiv.org/abs/2205.15480">Yuksekgonul, Wang, &amp; Zou, (2022)</a>, which splits the model‚Äôs predictions into an interpretable part and a non-interpretable part.</p>

<p>Another limitation is the assumption of the concept labels themselves. The data labeling cost for such a dataset is significantly higher than one that only collects the task labels. Furthermore, the concepts themselves are difficult to get right without expert input. One solution, proposed in <a href="https://arxiv.org/abs/2304.06129">Oikarinen et. al (2023)</a>, uses a combination of a large language model (GPT-3) and a multimodal foundation model (CLIP) to extract concepts for any task. See <a href="https://arxiv.org/abs/2304.06129">paper</a> for details. The reported accuracy of their approach is included in the table below.</p>

<h3 id="limitations-of-resnet-18-backbone">Limitations of ResNet-18 backbone</h3>

<p>I have made it tradition to incorporate DINOv2 into my ml-portfolio series posts. In this post, I replaced the ResNet-18 backbone with DINOv2 and the accuracy improves 15 points. I can‚Äôt think of many applications that would prefer an interpretable model with 60% accuracy over a non-interpretable model with 90% accuracy. NOTE: Although the DINOv2 features were better for overall task accuracy, they did not significantly improve concept accuracy.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Interpretable features</th>
      <th style="text-align: center">Concept labels required</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">0.594</td>
    </tr>
    <tr>
      <td style="text-align: center">Label-free CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">*Y</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">0.743</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.744</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">DINOv2</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.905</td>
    </tr>
  </tbody>
</table>

<p>*Features in this space take on continuous values correlated with human relatable concepts.</p>

<h3 id="alternative-approaches-to-interpretability">Alternative approaches to interpretability</h3>

<p>Saliency/attention maps are a common post-hoc interpretability technique that can be applied to any deep learning model. The purpose of these maps is to show the user which input features the model used for to perform most of their computations (i.e. paid the most attention to). They have one advantage over Concept Bottleneck Models in that they do not affect accuracy.</p>

<p><img src="/assets/images/dino_attention_map.png" alt="attention map" title="attention map" /></p>

<p><a href="https://arxiv.org/pdf/2309.16588">image reference</a></p>

<p>But for the purposes of interpretability, I find that the approach falls short. At best it confirms for the user that the expected parts of the input were used to make a decision and not a spurious object (i.e. a common example is that the class ‚Äúfrisbee‚Äù is assigned to images containing large amounts of green grass). But attention maps fail to explain anything about the internal features of the model and its decision process. DINOv2 attention maps tend to highlight a single, main subject in an image. But it doesn‚Äôt do that because it knows the difference between a cow and a blue heron; it was trained in a totally unsupervised way! Furthermore, attention maps do not provide a mechanism for intervention.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post I demonstrated how to make a machine learning model more interpretable and explainable. I also discussed the tradeoffs and alternative uses for such models when their accuracies do not perform at practical levels. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="explainable" /><category term="interpretable" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post introduces Concept Bottleneck Models - an interpretable approach to machine learning.]]></summary></entry><entry><title type="html">Experiment 16</title><link href="https://dlfelps.github.io/2024/06/03/few-shot.html" rel="alternate" type="text/html" title="Experiment 16" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/03/few-shot</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/03/few-shot.html"><![CDATA[<p>This post explores unique concepts in few-shot learning.</p>

<h2 id="introduction">Introduction</h2>

<p>Few-shot learning describes the situation where a classifier must generalize to new classes using only a few examples of each new class. It represents scenarios where data collection (or annotation) is costly for the classes you care about, but you may have access to related data. There are some unique terms associated with few-shot learning, which we will introduce next.</p>

<p>In standard supervised learning, the training and testing sets contain the SAME classes. The classifier is tested for its ability to discriminate between KNOWN classes.</p>

<p><img src="/assets/images/supervised_learning.PNG" alt="supervised learning" title="supervised learning" /></p>

<p><a href="https://ieeexplore.ieee.org/document/10287966">image reference</a></p>

<p>In few-shot learning, the training and testing sets contain DIFFERENT classes. A few-shot classifier is tested for its ability to discriminate UNKNOWN classes given only a few examples. These samples are further divided into the support set and query set. The support set contains labeled examples (for tuning the few-shot classifier) and the query set contains unlabeled samples for evaluation. The number of classes in the support set is denoted by N and the number of examples per class is denoted by K. A 5-way 1-shot method describes a method that has (N=5) classes with (K=1) examples per class.</p>

<p><img src="/assets/images/few_shot_learning.PNG" alt="few_shot_learning" title="few_shot_learning" /></p>

<p><a href="https://ieeexplore.ieee.org/document/10287966">image reference</a></p>

<h2 id="experiment">Experiment</h2>

<p>This experiment will test a model in a 5-way 5-shot task (i.e. 5 new classes, 5 examples each). We will use a standard benchmark in few-shot learning -  <a href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a> dataset. It is a fine-grained image classification task with over 200 different species of birds. The large number of classes make it suitable for few-shot learning as several can be withheld for testing while maintaining a difficult baseline. We next explain our basic approach to few-shot learning (prototype models) and then compare the performance of three ways to train the model.</p>

<h3 id="few-shot-framework-prototype-model">Few-shot framework: Prototype model</h3>

<p>We adopt a single framework for all variations of our experiment - <a href="https://arxiv.org/abs/1703.05175">Prototypical networks</a>. The concept is based on the idea that there is a single prototype representation for each class. A prototype for each class is calculated from the mean of its support set in the embedding space of a neural network. Our three approaches explore three different ways to train this neural network. Classification in a prototypical model is simply a nearest neighbors using only the prototypes<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. The code uses models and task loaders from <a href="https://github.com/sicara/easy-few-shot-learning">Easy Few-shot learning</a>.</p>

<p><img src="/assets/images/prototypical_network.jpeg" alt="prototypical network" title="prototypical network" /></p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221001818">image reference</a></p>

<h3 id="approach-1-pretrained-model">Approach #1: Pretrained model</h3>

<p>The first approach adapts a pretrained model using transfer learning. The transfer learning process starts with a model that is trained on a large and general dataset (e.g. Imagenet). The purpose of this model is to rely on the learned feature maps - which should be robust - and adapt the later layers to classify your new classes (see image below).</p>

<p><img src="/assets/images/transfer_learning.png" alt="transfer model" title="transfer model" /></p>

<p><a href="https://www.nature.com/articles/s41598-024-54923-y">image reference</a></p>

<p>This experiment uses a ResNet-18 model with pretrained weights from an Imagenet dataset. For each task in the evaluation phase, the support set is used to calculate the prototypes and the the query sets are classified from the prototypes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18(weights=ResNet18_Weights.DEFAULT)
model.fc = nn.Flatten()
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="approach-2-classical-model">Approach #2: Classical model</h3>

<p>This approach uses a subset of the training data to generate a pretrained model. NOTE: the test data contains classes that were not included in the training data. This approach is not always viable since it requires prior knowledge of the few-shot task data distribution. However, it can be powerful if you work in a specific domain and have labeled data from that domain readily available. To simulate it here, we split the 200 species in CUB into 140 for training and 60 for testing. During test time, a 5-way 5-shot task is created by randomly sampling 5 classes from the 60 test classes. This process is repeated 500 times to get a statistically significant measure of generalization performance.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18()
model.fc = nn.Linear(512, 140)
model.load_state_dict(torch.load('/content/easy-few-shot-learning/classical_model_18_acc_744.pt')) # pretrained just for you!
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="approach-3-episodic-model">Approach #3: Episodic model</h3>

<p>Episodic training (also called meta-learning) mirrors the few-shot tasks that will be used to test the final model during the training phase. Each ‚Äúepisode‚Äù is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization. This strategy typically assumes prior knowledge of N and K that will be used at test time. <a href="https://arxiv.org/pdf/2204.11181">ref1</a> <a href="https://arxiv.org/pdf/1703.05175">ref2</a></p>

<p>Episodic training can be performed following a classic training regime if additional data is available or using pretrained-weights if it is not. In this case we reserve the same 140 classes as in Approach #2 for training. But instead of performing classic training, which reduces cross entropy across all classes, we adjust weights based on many training episodes (5-way 5-shot). This process is slower as there is additional overhead required for each episode.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18()
model.fc = nn.Flatten()
model.load_state_dict(torch.load('/content/easy-few-shot-learning/episodic_model_18_acc_779.pt')) # pretrained just for you!
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="bonus-approach-pretrained-model-dinov2-backbone">BONUS Approach: Pretrained model (DINOv2 backbone)</h3>

<p>The three primary variations above can be compared directly because they are all variations of Resnet-18 and use Prototypical models for evaluation. In this bonus approach, we replace the Resnet-18 backbone of the pretrained model (approach #1) with DINOv2. The rest of the setup is the same.</p>

<h2 id="results">Results</h2>

<p>For this purposes of this discussion we will focus on Approaches #1-#3. The accuracy improves with each successive approach, but the assumptions also increase. Namely, in moving from transfer learning to classic learning we assume knowledge of the domain of the test classes and also assume access to labeled examples from that domain. In moving from classic learning to episodic learning we further assume knowledge of the task itself (5-way 5-shot) and the distribution of the data within it (i.e. uniformly sampled). This does not always reflect the real word - a <a href="https://arxiv.org/abs/2204.11181">study</a> showed that if these assumptions are incorrect they can impact performance significantly.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Training approach</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">transfer learning</td>
      <td style="text-align: center">0.684</td>
    </tr>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">classic</td>
      <td style="text-align: center">0.773</td>
    </tr>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">episodic</td>
      <td style="text-align: center">0.779</td>
    </tr>
    <tr>
      <td style="text-align: center">DINOv2</td>
      <td style="text-align: center">transfer learning</td>
      <td style="text-align: center">0.964</td>
    </tr>
  </tbody>
</table>

<h2 id="conclusion">Conclusion</h2>

<p>We compared three approaches to few-shot learning and discussed the conditions and assumptions necessary to use them. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>There is some similarity between Prototypical models and Image retrieval tasks. Image retrieval can be viewed as a prototypical model where each image is its own prototype. And instead of returing a single class, the top-N closest prototypes are returned.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="few-shot" /><category term="computer-vision" /><category term="ml-portfolio" /><category term="meta-learning" /><summary type="html"><![CDATA[This post explores unique concepts in few-shot learning.]]></summary></entry><entry><title type="html">Experiment 15</title><link href="https://dlfelps.github.io/2024/06/02/VIT-amster.html" rel="alternate" type="text/html" title="Experiment 15" /><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/02/VIT-amster</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/02/VIT-amster.html"><![CDATA[<p>This post explores DINOv2 - a foundational vision model from FAIR.</p>

<h2 id="introduction">Introduction</h2>

<p>Yann LeCun is one of the godfathers of deep learning<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. He believes that one of the most critical challenges to solve in the current era is how to give AI systems common sense. Common sense is difficult to define, but I believe that it keeps me alive and allows me to learn new skills helps with relatively few attempts (compared to deep learning). LeCun calls this elusive knowledge the dark matter of AI.</p>

<p>How do we impart this knowledge to our AI systems? LeCun believes the solution lies in self-supervised learning. Self-supervised learning is a technique that adapts tasks that conventionally require labels into an unsupervised learning task (e.g. next word prediction). Self-supervised learning tasks can take advantage of massive amounts of data. If designed properly, the model will learn accurate, meaningful representations of the data. Large Language Models have been popular in the news lately due to their uncanny ability to achieve human-level performance on many tasks. Their sucess is due, in part, to their ability to leverage large amounts of unnannotated text through self-supervised learning.</p>

<p>It is no surprise that the most successful self-supervised computer vision models come from LeCun‚Äôs lab (Fundamental AI Research team at Meta). They published <a href="https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/">DINO (2021)</a> and <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/">DINOv2 (2023)</a> as foundational image models. Foundational models are meant to serve as a resusable backbone for many tasks.</p>

<p><img src="/assets/images/dinov2.png" alt="DINO" title="DINO" />
<a href="https://aipapersacademy.com/dinov2-from-meta-ai-finally-a-foundational-model-in-computer-vision/">source</a></p>

<h2 id="image-retrieval">Image retrieval</h2>

<p>In their <a href="https://arxiv.org/abs/2304.07193">paper</a>, DINOv2 is benchmarked against many SOTA computer vision tasks. One of those tasks is image retrieval. Image retrieval uses images to search for other relevant images. It can help users discover new images guided by visual similarity. Popular benchmarks for image retrieval methods include the <a href="https://paperswithcode.com/dataset/oxford5k">Oxford building</a> and <a href="https://paperswithcode.com/dataset/google-landmarks-dataset-v2">Google Landmarks</a>. In this post I wanted to explore a dataset that was designed to retrieve images through time - the <a href="https://paperswithcode.com/dataset/amstertime">AmsterTime</a> dataset.</p>

<h3 id="amstertime-dataset">AmsterTime dataset</h3>

<p>The AmsterTime dataset offers a collection of 2,500 well-curated images matching the same scene from a street view matched to historical archival image data from Amsterdam city. This is a challenging dataset for image retrieval because the query and gallery sets were taken many years apart so there is often structural changes for the same location.</p>

<p><img src="/assets/images/amstertime.PNG" alt="AmsterTime" title="AmsterTime" /></p>

<h3 id="image-retrieval-with-dinov2">Image retrieval with DINOv2</h3>

<p>Finding similar images with DINOv2 is simple. First, compute the embedding vector for each image in the gallery by passing it through the DINOv2 model. Then, given a query image, compare its embedding vector with those from the gallery using cosine similarity:</p>

<p><img src="/assets/images/cosine.svg" alt="cosine" title="cosine" /></p>

<h4 id="the-embedding-vector">The embedding vector</h4>

<p>DINOv2 is a vision transformer model. As such, it transforms an image into patches and processes those patches a sequence of patches.</p>

<p><img src="/assets/images/vit.png" alt="vit" title="vit" />
<a href="https://arxiv.org/abs/2010.11929v2">source</a></p>

<p>The sequence is processed by the model and the embedding vector referenced here refers to the [CLS] token of the final layer (represented by the ‚Äú*‚Äù in the figure above). Since the [CLS] token does not represent an actual token, the transformer learns to encode a general representation of the entire image into that token. One of the most desirable  features of DINOv2 is that the attention maps associated with the [CLS] token of the last layer tend to be aligned with salient foreground objects (i.e. the model is attending to the primary subject of the image). It is worth looking at the code to see how to extract the [CLS] token:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_cls_token_for_images(images, processor, model):

  collect = []

  for i in tqdm(images):
    inputs = processor(images=i, return_tensors="pt")
    with torch.no_grad():
      outputs = model(**inputs)
    last_hidden_states = outputs.last_hidden_state
    collect.append(last_hidden_states[:,0,:]) # cls token

  return torch.cat(collect)
</code></pre></div></div>

<h2 id="results">Results</h2>

<p>A common metric for reporting results in image retrieval is Recall@N. Here are the results for the AmsterTime dataset:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">query</th>
      <th style="text-align: center">gallery</th>
      <th style="text-align: center">recall@1</th>
      <th style="text-align: center">recall@5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">AmsterTime</td>
      <td style="text-align: center">archival</td>
      <td style="text-align: center">streetview</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: center">0.68</td>
    </tr>
  </tbody>
</table>

<p>Not bad considering the difficulty of the dataset. This means that 43% of the time the top matching image is the correct one. Let‚Äôs look at some of the cases where this worked well (query image on left, correct result from gallery on right).</p>

<p><img src="/assets/images/match_on.png" alt="TP" title="TP" /></p>

<p>It is also informative to look at the cases where the approach failed. Below are the top-10 worst performing queries (largest distances between query and gallery). Each row contains the query image, the top predicted result, and finally the correct result.</p>

<p><img src="/assets/images/mismatch_on.png" alt="FP" title="FP" /></p>

<p>We see that many of the correct results contain occlusions or severe camera distortions, preventing the structural features of the building from being matched. In most of the cases, it is difficult for even a human to match the images.</p>

<p>Since the AmsterTime dataset contains pairs of images across time (1250 archival, 1250 streetview) we can also reverse the gallery and query sets. Surprisingly, its easier to predict the future than the past ;)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">query</th>
      <th style="text-align: center">gallery</th>
      <th style="text-align: center">recall@1</th>
      <th style="text-align: center">recall@5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">AmsterTime</td>
      <td style="text-align: center">archival</td>
      <td style="text-align: center">streetview</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: center">0.68</td>
    </tr>
    <tr>
      <td style="text-align: center">AmsterTime</td>
      <td style="text-align: center">streetview</td>
      <td style="text-align: center">archival</td>
      <td style="text-align: center">0.35</td>
      <td style="text-align: center">0.57</td>
    </tr>
  </tbody>
</table>

<h3 id="conclusion">Conclusion</h3>

<p>This post demonstrated the potential power of the DINOv2 foundational vision model. The results show that the model is capable of capturing the salient information in an image retrieval task without any prior training. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>

<h4 id="footnotes">Footnotes</h4>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Yoshua Bengio and Geoffrey Hinton are also given this title. In 2018 LeCun, Bengio, and Hinton won the <a href="https://www.acm.org/media-center/2019/march/turing-award-2018">Turing award</a> for their contributions in deep learing.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="vit" /><category term="computer-vision" /><category term="image-retrieval" /><category term="ml-portfolio" /><category term="self-supervised" /><summary type="html"><![CDATA[This post explores DINOv2 - a foundational vision model from FAIR.]]></summary></entry><entry><title type="html">Experiment 14</title><link href="https://dlfelps.github.io/2024/06/01/quantum-smalltalk5.html" rel="alternate" type="text/html" title="Experiment 14" /><published>2024-06-01T00:00:00+00:00</published><updated>2024-06-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/01/quantum-smalltalk5</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/01/quantum-smalltalk5.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Our final post in the Quantum Smalltalk series explores a thought experiment proposed by Avshalom Elitzur and Lev Vaidman to demonstrate an unusual quantum feature - interaction-free measurement.</p>

<h1 id="experiment-14">Experiment #14</h1>
<p>Elitzur and  Vaidman proposed the following thought experiment:</p>

<blockquote>
  <p>You are given 100 EV bombs. Due to a manufacturing problem that was caught too late, some of the triggers are defective. Your job is to try to salvage as many good bombs as possible.</p>
</blockquote>

<h2 id="what-is-an-ev-bomb">What is an EV Bomb?</h2>
<p>This experiment requires us to add a new piece of equipment to our workbench - the EV bomb. This bomb is special because it has a very sensitive trigger: a photon detector. Turn out the lights now because a single photon can cause this bomb to explode!</p>

<p>The following rules describe an EV bomb:</p>
<ol>
  <li>If the trigger is not defective, when a photon enters trigger tube the bomb explodes and you cannot salvage the bomb.</li>
  <li>If the trigger is defective, when a photon enters the trigger tube the bomb does not explode and the photon continues undisturbed out the trigger tube exit.</li>
  <li>The distance from the bomb to the rest of the equipment is sufficiently large to protect the equipment; only the bomb is destroyed if an explosion occurs.</li>
</ol>

<h2 id="attempt-1">Attempt #1</h2>
<p><img src="/assets/images/exp14.png" alt="Experiment #14" title="Experiment #14" /></p>

<p>Our first attempt to salvage the good EV bombs isn‚Äôt very clever. Let‚Äôs just shoot some photons at it and see what happens.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Count</th>
      <th style="text-align: center">Explosion</th>
      <th style="text-align: center">Photon detected at D1</th>
      <th style="text-align: center">Bombs salvaged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">80</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">N/A</td>
    </tr>
  </tbody>
</table>

<p>We tested all the bombs and determined with 100% accuracy which ones were defective and which ones were good. However, we failed to salvage any of the good bombs because they were blown up in the process. If it seems like this thought experiment has no solution, then you are likely restricting yourself to solutions that follow the principle of locality. (For extra details on locality see Appendix B &amp; C below.)</p>

<h2 id="attempt-2">Attempt #2</h2>
<p><img src="/assets/images/exp15.png" alt="Experiment #15" title="Experiment #15" /></p>

<p>Our second attempt should look familiar by now - the Mach-Zehnder setup. We place the bomb on the path between the first beamsplitter and the lower mirror and observe the following results:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Count</th>
      <th style="text-align: center">Explosion</th>
      <th style="text-align: center">Photon detected at D0</th>
      <th style="text-align: center">Photon detected at D1</th>
      <th style="text-align: center">Bombs salvaged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>Clearly we can‚Äôt salvage any of the 40 good bombs that blew up. But what about the remaining 60 that didn‚Äôt explode? Were they all defective? To answer this question, let‚Äôs look at our simulation output. We first observe what happens to photons that pass through a defective EV bomb:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passes through defective EV Bomb placed on the lower path. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.9999999999999996 0.0)
Photon finally detected with following probabilities: &gt;&gt; #(0.9999999999999996 0.0 )

</code></pre></div></div>

<p>Defective EV bombs act as if they aren‚Äôt there (just like Experiment #11) so there is a 100% chance that it will be detected at D0. Good EV bombs give a different result:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon detonates EV Bomb on the lower path with 50% probability. &gt;&gt; #(0.4999999999999999 0)
Photon bouncing off of a mirror. &gt;&gt; #(0.0 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.2499999999999999 0.2499999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.2499999999999999 0.2499999999999999)
</code></pre></div></div>

<p>From the simulation we see that half of the time the EV bomb detonates. Therefore, we can deduce that there were 80 good bombs (because we observed 40 detonating) and 20 bad bombs (the remainder). The output probability for the bombs that don‚Äôt detonate is 25% at D0 and 25% at D1. This means that the 20 bombs we observed at D1 are good! The other 40 bombs that didn‚Äôt detonate and that we observed at D0 have a 50% probability of being good, but we can‚Äôt tell which is which.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Count</th>
      <th style="text-align: center">Explosion</th>
      <th style="text-align: center">Photon detected at D0</th>
      <th style="text-align: center">Photon detected at D1</th>
      <th style="text-align: center">Bombs salvaged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">20</td>
    </tr>
  </tbody>
</table>

<h2 id="attempt-3-optional">Attempt #3 (optional)</h2>

<p>It‚Äôs a shame that we were only able to recover 25% of the good bombs. We can achieve the theoretical limit of 50% (or 40 in our example) by using something called the <a href="https://en.wikipedia.org/wiki/Quantum_Zeno_effect?useskin=vector">Quantum Zeno Effect</a>. The details of how to extend our setup using this effect are left up to the reader. I recommend <a href="https://youtu.be/fus1nJ6JaTk?si=Y6ljo3NrULArOx5H&amp;t=375">this video</a>.</p>

<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*bjGo9HLPhqvxcVbdZGnuOA.gif" alt="Quantum Zeno effect" />
<a href="https://medium.com/qiskit/building-quantum-bomb-testers-and-other-thought-experiments-with-quantum-computers-c160060fdde4">source</a></p>

<h1 id="interaction-free-measurement">Interaction-free measurement</h1>

<p>The EV bomb experiment demonstrates a non-local feature of quantum mechanics; we were able to learn something about a path that the photon didn‚Äôt take. Furthermore, we did so without any prior information. This is called an interaction-free measurement.</p>

<p>This is in contrast to, for example, the case where it is known that an object is located in one of two boxes. Looking and <em>not</em> finding it in one box tells us that the object is located inside the other box. This is also an interaction-free measurement, but it does not violate non-locality because we used prior information.</p>

<h1 id="summary">Summary</h1>
<p>Experiment #14 presented another weird quantum feature of interaction-free measurements. Although our treatment was theoretical, know that these results have been duplicated in the <a href="https://www.oeaw.ac.at/fileadmin/Institute/IQOQI-Vienna/PDF/publications-zeilinger/1994_-_Fundamental_Problems_in_Quantum_Theory_-_Experimental_Realization_of_Interaction-Free_Measurement.pdf">lab</a>.</p>

<h1 id="conclusion">Conclusion</h1>
<p>This post concludes the Quantum Smalltalk series. I am amazed at how much of the quantum world we were able to explore using the Mach-Zehnder setup. I hope that these posts piqued your interest in quantum mechanics. There are a lot of great resources available. As a next step, I recommend the book <a href="https://www.amazon.com/Through-Two-Doors-Once-Experiment/dp/1101986093">Through Two Doors at Once: The Elegant Experiment That Captures the Enigma of Our Quantum Reality</a> by Anil Ananthaswamy. Alternatively, if you find general relativity generally fascinating then you might want to explore more advanced topics like the <a href="https://en.wikipedia.org/wiki/Black_hole_information_paradox?useskin=vector">black hole information paradox</a>.</p>

<h1 id="appendix-a-the-smalltalk-code">Appendix A: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>

<h1 id="appendix-b-einstein-and-locality">Appendix B: Einstein and locality</h1>
<p>The principle of locality states that for one point to have an effect one another point, something must travel between the points to cause the effect. The special theory of relativity limits the speed of travel to the speed of light. Therefore an event at point <strong>A</strong> cannot cause a result at point <strong>B</strong> in a time less than <strong>D/c</strong>, where <strong>D</strong> is the distance between the points and <strong>c</strong> is the speed of light in vacuum.</p>

<p>Einstein believed in locality. In one of his 1935 papers, Einstein (along with co-authors Podolsky and Rosen) describe a thought experiment that demonstrates a scenario where quantum mechanics violates locality and concluded that quantum theory does not provide a complete description of reality. The paper ends by saying:</p>

<blockquote>
  <p>‚ÄúWhile we have thus shown that the wave function does not provide a complete description of the physical reality, we left open the question of whether or not such a description exists. We believe, however, that such a theory is possible.‚Äù</p>
</blockquote>

<p>Einstein believed that the solution to the paradox lay in introducing additional (possibly inaccessible) variables. Such a theory is known as a hidden variable theory.</p>

<h1 id="appendix-c-bell-and-locality">Appendix C: Bell and locality</h1>
<p>Almost 30 years after Einstein‚Äôs paper, John Stewart Bell proposed a theory that states that no theory of hidden local variables can ever reproduce all the predictions of quantum mechanics. To date, all such experiments have supported the theory of quantum physics and not the hypothesis of local hidden variables. The 2022 Nobel Prize in Physics was awarded to three scientists (John Clauser, Alain Aspect, and Anton Zeilinger) for their efforts to experimentally validate violations of the Bell inequalities. Bell‚Äôs test proves that quantum mechanics is either non-local itself or has non-local hidden variables.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction Our final post in the Quantum Smalltalk series explores a thought experiment proposed by Avshalom Elitzur and Lev Vaidman to demonstrate an unusual quantum feature - interaction-free measurement.]]></summary></entry><entry><title type="html">Experiment 13</title><link href="https://dlfelps.github.io/2024/05/01/quantum-smalltalk4.html" rel="alternate" type="text/html" title="Experiment 13" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/05/01/quantum-smalltalk4</id><content type="html" xml:base="https://dlfelps.github.io/2024/05/01/quantum-smalltalk4.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>In Experiments #10 and #12 we observed photons acting like particles. In Experiment #11 a photon‚Äôs actions could only be explained using wave-like behavior. In 1978, John Wheeler proposed an experiment to test the limits this paradox. He designed the experiment to answer the following questions:</p>
<ul>
  <li>Does the photon decide to travel as a wave or a particle depending on the experimental setup?</li>
  <li>And if yes, when does the photon decide?</li>
</ul>

<h1 id="experiment-13">Experiment #13</h1>
<p><img src="/assets/images/exp4.gif" alt="Experiment #13" title="Experiment #13" /></p>

<p>Combine the setups from Experiment #11 and #12 by randomly inserting the second beamsplitter AFTER the photon has interacted with the first beam splitter. This allows the photon to take one of six paths:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">BS2 present</th>
      <th style="text-align: center">Path at BS2</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">RX</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TX</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">RR</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">RT</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TR</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TT</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>We have seen both of the setups independently, but now the decision to add the second beamsplitter takes place during the experiment.</p>

<h2 id="results">Results</h2>

<p>Let‚Äôs examine two of the Smalltalk simulation runs. In the first example, the second beamsplitter was omitted from the setup.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Second beamsplitter omitted from setup. 
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.4999999999999999 0.4999999999999999)

</code></pre></div></div>
<p>We observe detection probabilities similar to those in Experiment #12. In the next example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Second beamsplitter inserted into setup after mirror. 
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.9999999999999996 0.0 )
Photon finally detected with following probabilities: &gt;&gt; #(0.9999999999999996 0.0)
</code></pre></div></div>

<p>We observe detection probabilities similar to those in Experiment #11. Filling in the results chart we notice that the table reads as if there were two different experiments going on (i.e. the total observation probability adds up to 2.0!)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">BS2 present</th>
      <th style="text-align: center">Path at BS2</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">RX</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">0.50</td>
    </tr>
    <tr>
      <td style="text-align: center">TX</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">0.50</td>
    </tr>
    <tr>
      <td style="text-align: center">RR/TT</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">RT/TR</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">1.0</td>
    </tr>
  </tbody>
</table>

<p>This is because there are two experiments happening - as soon as you insert the second beamsplitter you change the experiment.</p>

<h1 id="interpretation">Interpretation</h1>
<p>It is clear that the behavior of the photon in the interferometer depends on the choice of second beamsplitter, even when that choice is made after the photon enters the interferometer. In Wheeler‚Äôs words,</p>

<blockquote>
  <p>‚Äúwe have a strange inversion of the normal order of time. We, now, by moving the mirror in or out have an unavoidable effect on what we have a right to say about the already past history of that photon‚Äù</p>
</blockquote>

<p>It may be tempting to interpret this experiment as a case of <a href="https://en.wikipedia.org/wiki/Retrocausality">retrocausality</a>, whereby the insertion of the second beamsplitter causes the photon to retroactively act like a wave instead of a particle. However, this is a leap too far.</p>

<p>Experiment #13, also known as the <a href="https://en.wikipedia.org/wiki/Wheeler%27s_delayed-choice_experiment">delayed-choice experiment</a>, demonstrates a  subtle point of <a href="https://en.wikipedia.org/w/index.php?title=Wave%E2%80%93particle_duality">wave-particle duality</a> - a photon‚Äôs behavior is <em>always</em> governed by the Schr√∂dinger equation, yet when measured, it is observed as a particle whose location is determined by the <a href="https://en.wikipedia.org/wiki/Born_rule">Born rule</a>.</p>

<p>This is what is meant by a photon acting like a <em>wave</em> and a <em>particle</em>. But what about Experiments #10 and #12, where the photon acted only like a particle and not like a wave? WRONG. The photon was both, but the wavefunction predicted behavior that did not have any interference (i.e. it is particle-like).</p>

<h1 id="summary">Summary</h1>
<p>Experiment #13 presented John Wheeler‚Äôs <a href="https://en.wikipedia.org/wiki/Wheeler%27s_delayed-choice_experiment">delayed-choice experiment</a>. An interesting variation of this experiment is called the <a href="https://en.wikipedia.org/wiki/Delayed-choice_quantum_eraser">delayed-choice quantum eraser</a>.</p>

<h1 id="appendix-the-smalltalk-code">Appendix: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction In Experiments #10 and #12 we observed photons acting like particles. In Experiment #11 a photon‚Äôs actions could only be explained using wave-like behavior. In 1978, John Wheeler proposed an experiment to test the limits this paradox. He designed the experiment to answer the following questions: Does the photon decide to travel as a wave or a particle depending on the experimental setup? And if yes, when does the photon decide?]]></summary></entry><entry><title type="html">Experiment 12</title><link href="https://dlfelps.github.io/2024/04/01/quantum-smalltalk3.html" rel="alternate" type="text/html" title="Experiment 12" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/04/01/quantum-smalltalk3</id><content type="html" xml:base="https://dlfelps.github.io/2024/04/01/quantum-smalltalk3.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Experiment #12 doesn‚Äôt introduce any new concepts, but it prepares us for a surprising result in Experiment #13.</p>

<h1 id="experiment-12">Experiment #12</h1>
<p><img src="/assets/images/exp3.png" alt="Experiment #12" title="Experiment #12" /></p>

<p>Modify the setup from Experiment #11 by removing the second beamsplitter. This allows the photon to take one of two paths:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">R</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">T</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>We don‚Äôt yet know the outcome of the experiment, but based on our observations from Experiment 10 we might expect an equal distribution between D0 and D1. Let‚Äôs run the Smalltalk simulation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.4999999999999999 0.4999999999999999)
</code></pre></div></div>

<p>Our intuition was correct! After the first beamsplitter the photon is equally likely to be on the upper and lower path, but this time the paths are not recombined using the second beamsplitter. So any photons that are reflected at BS1 end up at D0 and any photons that are transmitted at BS1 end up at D1. We always know which path the photon took.</p>

<h2 id="the-wavefunction">The wavefunction</h2>

<p>How does this affect the photon‚Äôs wavefunction? Initially, after interacting with the beamsplitter we do not know which path the photon is on. Therefore its wavefunction is a superposition of states of the upper and lower paths.  At the moment just before the photon would reach D0 or D1, its wavefunction predicts with 50% probability that the photon will be detected at D0 and 50% probability that the photon will be detected at D1. An instant later it is detected at either D0 or D1. But how does the wavefunction predict two equally likely locations in one instant and then the next instant, the probabilities are gone and the photon is detected at D0 or D1?</p>

<h2 id="measurement-of-a-quantum-system">Measurement of a quantum system</h2>

<p>When scientists say they are taking a ‚Äúmeasurement‚Äù they are describing the action or process of observing something in order to gain information. But in quantum mechanics, the term ‚Äúmeasurement‚Äù has both practical and philosophical implications. From a practical point of view, measuring a quantum system changes the quantum state that describes that system; this is sometimes described as causing the wavefunction to collapse (i.e. it is no longer in a superposition of states). From a philosophical point of view, how did the system know it was being measured? In other words, what caused the collapse? How are the probabilities converted into an actual, well-defined classical outcome? The answer to these questions depends on your interpretation of quantum mechanics and is beyond the scope of this post. For those interested, I thoroughly enjoyed listening to this <a href="https://youtu.be/GdqC2bVLesQ?si=yhlTgSpCbw2Gfv74">panel discussion</a> on the topic.</p>

<h1 id="summary">Summary</h1>
<p>Experiment 12 explored the measurement problem - the instant that the photon becomes a particle (as described by the wavefunction). The next experiment challenges our everyday understanding of reality.</p>

<h1 id="appendix-the-smalltalk-code">Appendix: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction Experiment #12 doesn‚Äôt introduce any new concepts, but it prepares us for a surprising result in Experiment #13.]]></summary></entry></feed>