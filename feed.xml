<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dlfelps.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dlfelps.github.io/" rel="alternate" type="text/html" /><updated>2024-08-15T23:25:33+00:00</updated><id>https://dlfelps.github.io/feed.xml</id><title type="html">.NET Experiments</title><subtitle>Bill Nye meets Bill Gates ;)</subtitle><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><entry><title type="html">Experiment 19</title><link href="https://dlfelps.github.io/2024/08/01/generative.html" rel="alternate" type="text/html" title="Experiment 19" /><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/08/01/generative</id><content type="html" xml:base="https://dlfelps.github.io/2024/08/01/generative.html"><![CDATA[<p>This post describes the stable diffusion architecture for generative AI.</p>

<h1 id="introduction">Introduction</h1>

<p>I am going to introduce this post with a real conversation I had with Microsoft‚Äôs <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you explain Generative AI to me like I am 5 years old?</p>

  <p>Copilot:</p>

  <p>Certainly! Imagine you have a magical drawing book. When you open it, it creates new pictures all by itself! That‚Äôs what generative AI does‚Äîit makes up things like drawings, stories, or even songs without anyone telling it exactly what to create. It‚Äôs like having a creative friend who invents cool stuff just by thinking about it! üé®‚ú®</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Are you an example of a Generative AI?</p>

  <p>Copilot:</p>

  <p>Indeed! I‚Äôm like a friendly digital artist. When you ask me for something, I use my magical algorithms to create new sentences, poems, or even drawings. Just like a painter mixes colors to create beautiful pictures, I mix words and ideas to make interesting content! üé®‚ú®</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you generate an image of an Eastern Bluebird drinking a milkshake?</p>

  <p>Copilot:</p>

  <p><img src="/assets/images/bluebird_milkshake_dalle.PNG" alt="bluebird milkshake" /></p>
</blockquote>

<p>END OF CONVERSATION</p>

<p>I was impressed by the generated images. Behind the scenes, Copilot uses an OpenAI product called <a href="https://openai.com/dall-e-3/">DALLE-3</a> to generate images. Unfortunately DALLE-3 is not publicly available. So, for the remainder of this post, we will instead examine a closely related model called Stable Diffusion. How do they compare? Here is the output of Stable Diffusion for the same prompt above:</p>

<p><img src="/assets/images/bluebird_milkshake_stable.webp" alt="stable diffusion" width="400" /></p>

<p>Generally Stable Diffusion tends to produce images that are more photorealistic whereas DALLE-3 creates images that feel like computer-generated art.</p>

<h1 id="unconditional-image-generation-denoising-diffusion-probabilistic-models">Unconditional Image Generation (Denoising Diffusion Probabilistic Models)</h1>

<p>I will explain Stable Diffusion through a series of increasingly complex models. The first - and most foundational model -  is the Denoising Diffusion Probabilistic Model (DDPM), which serves as the core technology behind Stable Diffusion and DALLE-3.</p>

<p>DDPMs are a class of generative models that work by iteratively denoising a diffusion process, which involves adding noise to an image and then trying to remove it. The underlying theory of DDPMs is based on the idea that transforming a simple distribution, such as a Gaussian, through a series of diffusion steps can result in a complex and expressive image data distribution. This allows the model to generate new images by reversing the diffusion process, starting from the full Gaussian distribution and ending up with the image distribution.</p>

<p><img src="https://huggingface.co/blog/assets/78_annotated-diffusion/diffusion_figure.png" alt="forward diffusion" /></p>

<p>To further demonstrate the contribution of each step I will show what kind of generative images are possible using the CUB-2011 dataset. Our final goal will be to produce fully synthetic birds. In this first step we will only be able to generate small bird-like shapes using the DDPM architecture alone.</p>

<p><img src="/assets/images/generative_figure1.png" alt="ddpm" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The output of training the model is shown below. Here I take a snapshot of the sample images every 10 epochs to show the learning progression of the model.</p>

<p><img src="/assets/images/ddpm-64.gif" alt="ddpm animation" /></p>

<p>You can see birds if you squint, but the low resolution (64x64) makes these images less useful. DDPMs top out around (256x256) and become increasingly slow to train on a single GPU. We next explore two options for increasing the resolution.</p>

<h1 id="generating-higher-resolution-images">Generating Higher Resolution Images</h1>

<p>The following two sections offer alternatives to increasing the resolution of the generated images.</p>

<h2 id="upscaling">Upscaling</h2>

<p>The first approach uses image upscaling to increase the resolution of an image. Traditional upscaling methods, such as interpolation, don‚Äôt increase the inherent resolution of the image. The upscaler used here actually fills in missing details to increase resolution (NOTE: the upscaler itself is based on stable diffusion architecture). The architecture for the approach is outlined below:</p>

<p><img src="/assets/images/generative_figure2.png" alt="upscaling" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The benefit of this approach is that it uses the same DDPM from step 1 (which produces images of 64x64) and then the upscaler increases the resolution to 256x256.</p>

<p>The disadvantage of this approach is that the upscaler is not tuned specifically for your data (although it could be with additional training). It also makes a slow process (sampling the DDPM) even slower because you are performing two indepdenent steps (sampling then upscaling).</p>

<h2 id="latent-model">Latent model</h2>

<p>The second approach to increasing resolution was introduced in <a href="https://arxiv.org/abs/2112.10752">‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models‚Äù</a>. The idea behind this approach is to surround the DDPM with a pretrained variational autoencoder<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. The DDPM then learns to denoise the latent space (which also happens to be 64x64) instead of the image space. To create a new sample:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. generate random noise in the shape of the latent space
2. use the DDPM to denoise the input
3. use the decoder to transform the latent space back into image space (512x512)
</code></pre></div></div>

<p><img src="/assets/images/generative_figure3.png" alt="latent" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Unfortunately I did not have enough Colab credits to fully train this variant<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Although the model outputs images that are 512x512, I admit that they don‚Äôt look very bird-like. Here is a single full image created after 50 epochs of training:</p>

<p><img src="/assets/images/latent_bird.png" alt="latent image" /></p>

<h1 id="conditional-image-generation-stable-diffusion">Conditional Image Generation (Stable Diffusion)</h1>

<p>Up to this point we have not had any control over what kind of bird gets synthesized. The final step is to condition the DDPM on another input (usually text, but it could also be another image). NOTE: In order to train such a model, the training data would require both text and captions. The CUB-2011 dataset does not provide captions and I also discovered that significantly more compute is needed to train the latent models so I will use a pretrained Stable Diffusion model.</p>

<p><img src="/assets/images/generative_figure4.png" alt="stable diffusion" width="600" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Here is an example output:</p>

<p><img src="/assets/images/eastern_bluebird_stable.png" alt="stable diffusion output" /></p>

<p>I generated the image above with the implementation provided by Lyumin Zhang called <a href="https://github.com/lllyasviel/Fooocus">Fooocus</a>. It is a pre-tuned version of <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">StabilityAI‚Äôs Stable Diffusion XL model</a>. You can generate your own images using the provided Colab notebook -&gt; 
<a target="_blank" href="https://colab.research.google.com/github/https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a></p>

<h1 id="conclusion">Conclusion</h1>

<p>Generative AI is currently experiencing a boom, with new research and products coming out every day. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>. I would like to leave you with a final conversation from Microsoft‚Äôs <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you write a poem about generative ai?</p>

  <p>Copilot:</p>

  <p>Sure, I‚Äôd love to! Here‚Äôs a poem about generative AI:</p>

  <p>In circuits deep, where data flows, A spark ignites, a thought that grows. From silicon dreams, a mind takes flight, Crafting worlds in the dead of night.</p>

  <p>With algorithms as its guiding star, It reaches realms both near and far. From pixels bright to words that sing, Generative AI creates everything.</p>

  <p>It learns from patterns, vast and wide, In every byte, where secrets hide. A painter‚Äôs brush, a poet‚Äôs pen, It brings to life what once was then.</p>

  <p>Yet in its core, a humble start, A tool to aid the human heart. For in each line and crafted scene, It mirrors dreams, both bold and serene.</p>

  <p>So here‚Äôs to code that dreams and dares, To generative AI, with whom we share, A future bright, where minds entwine, In harmony, both yours and mine.</p>
</blockquote>

<p>END OF CONVERSATION</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This particular autoencoder preserves spatial information by compressing a 512x512x3 image into a latent space of 64x64x4.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>As a comparison, in <a href="https://arxiv.org/abs/2112.10752">‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models‚Äù</a>, they trained a latent model on FFHQ for 283 hours on an A100. This would cost an equivalent of $370 in Colab compute units so you will have to forgive me for stopping early!¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="generative" /><category term="stable-diffusion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes the stable diffusion architecture for generative AI.]]></summary></entry><entry><title type="html">Experiment 18</title><link href="https://dlfelps.github.io/2024/07/01/adversarial.html" rel="alternate" type="text/html" title="Experiment 18" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/07/01/adversarial</id><content type="html" xml:base="https://dlfelps.github.io/2024/07/01/adversarial.html"><![CDATA[<p>This post describes simple approaches to attack and defend machine learning models.</p>

<h1 id="introduction-to-adversarial-machine-learning">Introduction to adversarial machine learning</h1>
<p>Adversarial machine learning refers to a set of techniques that adversaries use to attack machine learning systems. These attacks exploit vulnerabilities in ML models, aiming to manipulate their behavior or compromise their performance. Adversarial attacks can be used against any system that employs machine learning, including finance, security, and autonomous systems.</p>

<h2 id="types-of-adversarial-attacks">Types of Adversarial Attacks</h2>
<p>There are several types of known attacks against machine learning models. Please see the NIST  <a href="https://csrc.nist.gov/pubs/ai/100/2/e2023/final">taxonomy</a> for a comprehensive breakdown with references. The remainder of the post focuses on the evasion attack, which modifies the input in a way that alters the prediction of the model under attack.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center"><strong>Poisoning</strong></th>
      <th style="text-align: center"><strong>Evasion</strong></th>
      <th style="text-align: center"><strong>Extraction</strong></th>
      <th style="text-align: center"><strong>Inference</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>during</em></td>
      <td style="text-align: center">Training</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
    </tr>
    <tr>
      <td style="text-align: center"><em>purpose</em></td>
      <td style="text-align: center">Mistrain the model</td>
      <td style="text-align: center">Deceive the model</td>
      <td style="text-align: center">Steal the model</td>
      <td style="text-align: center">Infer training data</td>
    </tr>
  </tbody>
</table>

<h2 id="threat-models">Threat models</h2>

<p>An important consideration when attacking/defending a machine learning model is the threat model. The two primary threat models are white-box and black-box attacks.</p>

<p><img src="/assets/images/white_box-Photoroom.png" alt="white box" /></p>

<blockquote>
  <p><strong>White-Box Attack:</strong> The attacker has <strong>full knowledge</strong> of the ML model‚Äôs architecture, parameters, and training data. e.g. <a href="https://arxiv.org/pdf/1412.6572v3.pdf">L-BFGS Attack</a>, <a href="https://arxiv.org/abs/1312.6199">Fast Gradient Sign Method</a></p>
</blockquote>

<p><img src="/assets/images/black_box-Photoroom.png" alt="black box" /></p>

<blockquote>
  <p><strong>Black-Box Attacks:</strong> The attacker has <strong>limited information</strong> about the ML model (e.g., input-output interactions).  e.g. <a href="https://arxiv.org/abs/1809.02861">Transfer Attack</a>, <a href="https://arxiv.org/abs/1708.03999">Score-based Black-Box Attacks</a>, <a href="https://arxiv.org/abs/1712.04248">Decision-based Attack</a></p>
</blockquote>

<p>Note that adversarial exmaples can be used under either threat model, however they are much more effective when an adversary has access to the internals of your model (i.e. white-box) and can verify that the attack works prior to deploying it.</p>

<h2 id="experimental-setup">Experimental setup</h2>

<p>As with a number of my previous posts, I will be using a pretrained <a href="https://github.com/osmr/imgclsmob/tree/master/pytorch#cub-200-2011">ResNet-18 model</a> trained on the <a href="https://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB2011 dataset</a>; this will serve as the model under attack.</p>

<h1 id="scenario-1-attacking-a-model-with-non-adversarial-methods">Scenario 1: Attacking a model with non-adversarial methods</h1>
<p>This ‚Äúattack‚Äù can also be seen as a model hardening test. I recommend running it on all computer vision models prior to deploying them. It tests the extent to which the model can handle common image transformations in the wild. Depending on the results, we might discover a cheap form of the evasion attack (applies to both black-box and white-box settings). The attack works as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select a common image transform (e.g. rotation, brightness, contrast)
  2. Select a random test image
  3. Run the test image through the model and save the predicted class
  4. Run the transformed test image through the model and save the predicted class
  5. If #3 and #4 are different then increase counter
  6. Repeat steps 2-5 100 times to estimate the effect of the transform
</code></pre></div></div>

<h2 id="results">Results</h2>
<p>The following plots visually demonstrate the effect of a single transform. The results (represented by the blue bar to the right of each image) represent the average performance of 100 test images. A higher bar signifies that the model is robust to that transform (i.e. the counter in step#5 was low).</p>

<p><img src="/assets/images/blur.png" alt="blur" /></p>

<p><img src="/assets/images/hue.png" alt="hue" /></p>

<p><img src="/assets/images/rotate.png" alt="rotate" /></p>

<p><img src="/assets/images/brightness.png" alt="brightness" /></p>

<p><img src="/assets/images/contrast.png" alt="contrast" /></p>

<p><img src="/assets/images/gamma.png" alt="gamma" /></p>

<p><img src="/assets/images/saturation.png" alt="saturation" /></p>

<h2 id="discussion">Discussion</h2>
<p>The model under attack was relatively robust to sublte image transforms and should be expected to perform well in the wild. Hue was the only transform that significantly impacted performance and that is expected since color is a differentiating factor in bird species. Can we use hue as a cheap form of adversarial attack? No, because the hue transform would also fool a human. In other words, the hue transform fundamentally changes the input so it isn‚Äôt ‚Äúfooling‚Äù anything. We next turn to more advanced methods.</p>

<h1 id="scenario-2-attacking-a-model-with-adversarial-methods">Scenario 2: Attacking a model with adversarial methods</h1>

<p>The goal of an evasion attack is to minimally modify an input (i.e. image) in such a way that the model alters its class prediction, but a human still percieves the original class. It‚Äôs this combination that makes the evasion attack so potent - humans don‚Äôt know anything is wrong and the machine is fooled. The attack succeeds because the altered image lies outside of the training distribution.</p>

<h2 id="pgd">PGD</h2>
<p>The Projected Gradient Descent (PGD) attack <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> is an iterative optimization technique that seeks to find the most adversarial perturbation within a predefined constraint set. Here is psuedocode for the algorithm:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select an unmodified image as the target
  2. Select a random point within the allowed perturbation region of the target image
  3. Perform gradient descent on the model's loss function
  4. Project the perturbed input back into the feasible set after each iteration
</code></pre></div></div>
<p>This iterative process ensures that the adversarial perturbation remains within the defined constraints around the original input, making the adversarial example both effective and imperceptible to human observers.</p>

<p>However, the PGD attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Over-optimization against one constraint</li>
  <li>Potential for unstable optimization</li>
  <li>Limited transferability to other models</li>
</ul>

<h3 id="example">Example</h3>

<p><img src="/assets/images/pgd_400.png" alt="pgd_400" /></p>
<blockquote>
  <p>A successful example of a PGD attack (left). The modified image was classified differently than the original image, but the modifications are barely visible to the human eye. The modifications added to the original image are magnified 10x in the (right) image.</p>
</blockquote>

<h2 id="carlini-wagner">Carlini-Wagner</h2>
<p>The Carlini &amp; Wagner (C&amp;W) attack is a powerful adversarial evasion attack that aims to generate adversarial examples that are misclassified by a target model. Adversarial examples are created by formulating an optimization problem, where the goal is to minimize the perturbation while ensuring the classifier mislabels the adversarial example. C&amp;W is particularly notable for its ability to generate adversarial examples that are transferable across different models and defenses, making it a significant threat to the security of machine learning systems.</p>

<p>However, the C&amp;W attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Computational cost</li>
  <li>Requires domain-specific adaptations to shape the perturbations and the optimization problem depending on the type of data being manipulated</li>
</ul>

<h3 id="example-with-l0-norm">Example with L0 norm</h3>

<p><img src="/assets/images/carliniL0_392.png" alt="carliniL0_392" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. Although the modifications are mostly imperceptible, a small eye-like patch is visible in the magnitude-magnified image (right).</p>
</blockquote>

<h3 id="example-with-linf-norm">Example with LInf norm</h3>

<p><img src="/assets/images/carliniLinf_3207.png" alt="carliniLinf_3207" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. The modifications in this case are noticable and global in the original image and magnitude-magnified image (right).</p>
</blockquote>

<h1 id="scenario-3-defending-a-model-with-non-adversarial-methods">Scenario 3: Defending a model with non-adversarial methods</h1>

<p><img src="/assets/images/adv-flowchart.png" alt="adv-flowchart" /></p>

<p>This scenario explores the exent to which non-adversarial methods can be used to defend a model from a strong adversarial attack (e.g. PGD or Carlini-Wagner). The defense exploits the fact that the evasion attack itself is typically brittle. In other words, the attack noise was calculated to minimize the amount of change in the input space to achieve the desired result. Therefore, it stands to reason that the attack may not be able to withstand additional transformations. We constrain the suite of transformations considered in Scenario #1 to only those transformations that maintained greater than 99% performance on natural images. This allows us to limit the amount of false positives in our adversarial detector to less than 1%.</p>

<h2 id="results-1">Results</h2>
<p>In the tables below we report the prediction rates for the best parameter combination per transformation type (e.g. rotation, brightness).</p>

<h3 id="pgd-1">PGD</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.049505</td>
      <td style="text-align: center">0.950495</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.283019</td>
      <td style="text-align: center">0.716981</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.441441</td>
      <td style="text-align: center">0.553864</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.454545</td>
      <td style="text-align: center">0.545455</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.555556</td>
      <td style="text-align: center">0.444444</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-l0">CW-L0</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.114286</td>
      <td style="text-align: center">0.885714</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.6</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.558442</td>
      <td style="text-align: center">0.441558</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.561905</td>
      <td style="text-align: center">0.4334</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.741935</td>
      <td style="text-align: center">0.258065</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">0.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.815126</td>
      <td style="text-align: center">0.184874</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-linf">CW-LInf</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.819444</td>
      <td style="text-align: center">0.180556</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.910256</td>
      <td style="text-align: center">0.089744</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.939394</td>
      <td style="text-align: center">0.060606</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.943182</td>
      <td style="text-align: center">0.056818</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.944444</td>
      <td style="text-align: center">0.055556</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h2 id="discussion-1">Discussion</h2>

<p>The highest average difference between performance on natural images and adversarial images is achieved by rotating images by 7 degrees. This transformation is ideal from a false-positive point of view as all natural images are unphased by it. NOTE: this does not mean that the predicted class is correct, only that the predicted class is not altered by rotating the image 7 degrees. Our ability to detect adversarial examples depends on the attack:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">TP rate</th>
      <th style="text-align: center">FP rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">PGD</td>
      <td style="text-align: center">0.95</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-L0</td>
      <td style="text-align: center">0.885</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-LInf</td>
      <td style="text-align: center">0.18</td>
      <td style="text-align: center">0.0</td>
    </tr>
  </tbody>
</table>

<p>The results are quite promising for PGD and CW-L0, but the CW-LInf attack proved to be more resilient (to all tested transformations). The results appear to confirm our hypothesis about the fragility of the evasion attacks - more subtle attacks are more easily detected precisely because they are more subtle. In other words, if the attack has a low signal-to-noise ratio then rotating the image has a better chance to render it ineffective.</p>

<h1 id="scenario-4-defending-a-model-with-adversarial-methods">Scenario 4: Defending a model with adversarial methods</h1>

<p>Adversarial training is a technique used to enhance the robustness of machine learning models against adversarial examples. The simplest way to perform adversarial training is to include adversarial examples in the training set.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Train a model with the original training data
2. Use the model to generate adversarial examples
3. Add the adversarial examples (with correct labels) to the training data
4. Retrain the model
</code></pre></div></div>
<p>A more sophisticated approach called Robust training <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> alters the optimization process itself in order to make models more resilient to adversarial perturbations. These perturbations are carefully crafted to deceive the model into making incorrect predictions.</p>

<h2 id="comparing-detection-and-adversarial-training">Comparing detection and adversarial training</h2>

<p>Adversarial training strengthens models intrinsically, while detection approaches focus on identifying adversarial examples during inference. Both play crucial roles in addressing the challenges posed by adversarial attacks.</p>

<ul>
  <li>advantages of adversarial detection
    <ul>
      <li>can defend against white-box and black-box threat models</li>
      <li>can be used to protect existing/pretrained models</li>
      <li>may work against UNKNOWN attacks</li>
    </ul>
  </li>
  <li>advantages of adversarial training
    <ul>
      <li>moderate success against all KNOWN attacks</li>
    </ul>
  </li>
</ul>

<p>NOTE: If you are training your own model then you can do BOTH!</p>

<h1 id="conclusion">Conclusion</h1>
<p>This was a long post, so I will summarize the results here:</p>
<ol>
  <li>It is good practice to test your model‚Äôs performance against non-adversarial image transformations.</li>
  <li>Evasion attacks (e.g. PGD and CW) can be used to alter the predictions of a model.</li>
  <li>Some evasion attacks (e.g. PGD and CW-L0) can be detected at inference time due to their lack of robustness against non-adversarial image transformations.</li>
  <li>Other evasion attacks (e.g. CW-LInf) may be rendered inneffective using adversarial training.</li>
</ol>

<p>As ML adoption grows, understanding and addressing these vulnerabilities become crucial for maintaining system integrity and security. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="adversarial" /><category term="evasion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes simple approaches to attack and defend machine learning models.]]></summary></entry><entry><title type="html">Experiment 17</title><link href="https://dlfelps.github.io/2024/06/04/explainable.html" rel="alternate" type="text/html" title="Experiment 17" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/04/explainable</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/04/explainable.html"><![CDATA[<p>This post introduces Concept Bottleneck Models - an interpretable approach to machine learning.</p>

<h2 id="introduction-to-explainable-ai">Introduction to Explainable AI</h2>

<p>Explainable machine learning models describe how and why a prediction was made. Experts in explainable AI (XAI) make a distinction between interpretabiliy and explainability: <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253521002281">source</a></p>

<ul>
  <li>
    <p>Interpretability is the extraction of relevant sub-symbolic information from a machine-learning model concerning relationships either contained in data or learned by the model</p>
  </li>
  <li>
    <p>Explainability is the ability to translate this sub-symbolic information in a comprehensible manner through human-understandable language expressions</p>
  </li>
</ul>

<p>This blog post answers the following questions:</p>
<ol>
  <li>How do you make a machine learning model more interpretable and explainable?</li>
  <li>What are the tradeoffs and when should you do it?</li>
</ol>

<h2 id="experimental-study">Experimental study</h2>

<p>In order to demonstrate the benefits of XAI we will use a standard image classification task on well-known benchmark - <a href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a>, which contains 11,788 images of birds. This dataset is typically used for fine-grained classification since it contains over 200 species of birds. Additionally, for each image, it captures meta-data about each bird (i.e. 28 features including bill shape, wing shape, head color, etc.). This information will be critical to creating an interpretable model.
<img src="https://i0.wp.com/www.beyourownbirder.com/wp-content/uploads/2019/09/american-goldfinch-1.jpg" alt="American goldfinch" /></p>

<p><a href="https://i0.wp.com/www.beyourownbirder.com/wp-content/uploads/2019/09/american-goldfinch-1.jpg">Image source</a></p>

<h2 id="establishing-the-baseline">Establishing the baseline</h2>

<p>Before we build an interpretable model, we will establish the performance of a non-interpretable model. For this dataset we will use a ResNet-18 pretrained model from the <a href="https://github.com/osmr/imgclsmob">pytorchcv</a> package.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pytorchcv.model_provider import get_model 

cub_model = get_model('resnet18_cub', pretrained=True)
</code></pre></div></div>

<p>This model has 74.4% accuracy on the test set, which is impressive given that there are 200 classes and the classes are all similar because they are all birds. We will next describe how to adapt this model to be interpretable.</p>

<h2 id="concept-bottleneck-models">Concept Bottleneck Models</h2>

<p>Deep learning models are powerful due to their ability to learn features that are discriminative to the prediction task. However, these features are not interpretable by humans, leading to the so-called <a href="https://news.mit.edu/2022/machine-learning-explainability-0505">‚Äúblack box problem‚Äù</a>. In an effort to make deep learning models more transparent, Concept Bottleneck Models (CBMs) make use of a human-interpretable feature layer called the ‚Äúconcept layer‚Äù. This layer contains features that are meaningful to a human and representative of the features necessary for a human to perform the same kind of prediction. The final prediction is made based on the values of the concept layer. There are two primary advantages of this approach:</p>
<ol>
  <li>Users can understand the feature values immediately before the prediction layer</li>
  <li>Users can update the feature values to improve accuracy (sometimes called intervention)</li>
</ol>

<p><img src="/assets/images/cbm_cub.jpeg" alt="concept bottleneck model" title="concept bottleneck model" /></p>

<p><a href="https://arxiv.org/pdf/2007.04612">image reference</a></p>

<h3 id="training-a-concept-bottleneck-model">Training a Concept Bottleneck Model</h3>
<p>In the original <a href="https://arxiv.org/pdf/2007.04612">paper</a> the authors test three ways to train a Concept Bottleneck Model:</p>

<ul>
  <li>
    <p>Independent: learn a concept predictor (L<sub>C</sub>) and learn a task predictor (L<sub>Y</sub>)</p>
  </li>
  <li>
    <p>Sequential: learn a concept predictor (L<sub>C</sub>) then learn a task predictor (L<sub>Y</sub> <span>|</span> L<sub>C</sub>)</p>
  </li>
  <li>
    <p>Joint: simultaneously learn concept predictor (L<sub>C</sub>) and task predictor (L<sub>Y</sub> <span>|</span> L<sub>C</sub>)</p>
  </li>
</ul>

<p>The joint approach offered the best performance on the CUB dataset. However, because the concept and task predictors are trained simultaneously using a dual loss function is it prone to side-channel leaks, in which the task predictor drives the concept predictor to leak additional information about features that are not strictly related to the concepts. The extent to which this occurs affects the overall interpretability of the model.</p>

<p>Independent and sequential approaches offered similar performance on CUB and do not suffer from the side-channel leak problem. The independent approach is recommended for systems that anticipate high intervention rates because its task predictor is trained using ground truth concept values. The sequential approach is recommended when intervention is not likely and the concepts are difficult to predict. This allows the task predictor to learn to ignore concepts that may be difficult to predict.</p>

<h4 id="learning-the-concepts">Learning the concepts</h4>
<p>There are 28 concepts in the CUB-200 dataset. These are expert-selected concepts relevant to the problem of bird species identification.</p>

<p><img src="/assets/images/cub_attributes.jpeg" alt="bird attributes" title="bird attributes" /></p>

<p><a href="https://authors.library.caltech.edu/records/cvm3y-5hh21/files/CUB_200_2011.pdf?download=1">image reference</a></p>

<p>We will use the training data to learn to predict these features by creating one classifier per concept. The embedding features from the baseline ResNet-18 model serve as the input to each concept classifier. As you can see in the figure below, the ability to predict each concept varies greatly. This is partially due to the fact that the embedding space is optimized to differentiate at the class level (global prediction) rather than the individual concepts (local predictions).</p>

<p><img src="/assets/images/concept_plot.png" alt="concept predictions" title="concept predictions" /></p>

<p>Let‚Äôs take a closer look at a few examples:</p>

<ul>
  <li>Head Pattern (33% accuracy) - this concept is difficult to predict from the embedded features; the classifier only performs marginally better than the base rate (+7% better)</li>
  <li>Eye Color (80% accuracy) - this concept is the most accurately predicted concept; but it does not perform better than the base rate, suggesting that this feature is not informative and predicts that birds have black eyes regardless of input</li>
  <li>Crown color (55% accuracy) - although this concept is not among the most accurately predicted, it does perform significantly higher than the base rate (+30% better) suggesting that the embedded features at least partially capture this information</li>
</ul>

<h4 id="learning-the-classes">Learning the classes</h4>
<p>Because the performance of the concept predictors was relatively low, we will use a sequential training approach. After learning one concept predictor per concept L<sub>C<sub>i</sub></sub>, we use the predicted concepts (rather than the ground truth concepts) to learn the task predictor.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Interpretable features</th>
      <th style="text-align: center">Concept labels required</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">0.594</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.744</td>
    </tr>
  </tbody>
</table>

<p>We added interpretability to our model at the cost of about 25 points of accuracy. This would be an unacceptable tradeoff for most real-world applications. Even though the overall accuracy is low, if either the accuracy of the concept predictor or task predictor is independently high then they can still be useful in certain contexts.</p>
<ul>
  <li><strong>Expert assistant</strong>:   This scenario assumes the system is being used by a highly trained user (e.g. ornithologist for CUB). The concept predictor (with high accuracy) can save the expert time by pre-populating values for the concept features. The expert then performs a final review of the concept features and assigns the final class label (e.g. species of bird for CUB).</li>
  <li><strong>Psuedo-expert</strong>: This scenario assumes the system is being used by non-skilled users (e.g. Mechanical Turk workers). The worker is able to make simple observations about the image (e.g. what is the color of the bird‚Äôs wing, which of the following shapes most closely matches the bird‚Äôs beak, etc.). The task predictor (with high accuracy) then assigns the species of bird given the observations.</li>
</ul>

<h2 id="discussion">Discussion</h2>

<h3 id="limitations-of-cbms">Limitations of CBMs</h3>

<p>We have already observed the first limitation of Concept Bottleneck Models - they don‚Äôt tend to be as accurate as non-interpretable models for real-world applications. This gap can be reduced using a residual modeling technique such as the one proposed in <a href="https://arxiv.org/abs/2205.15480">Yuksekgonul, Wang, &amp; Zou, (2022)</a>, which splits the model‚Äôs predictions into an interpretable part and a non-interpretable part.</p>

<p>Another limitation is the assumption of the concept labels themselves. The data labeling cost for such a dataset is significantly higher than one that only collects the task labels. Furthermore, the concepts themselves are difficult to get right without expert input. One solution, proposed in <a href="https://arxiv.org/abs/2304.06129">Oikarinen et. al (2023)</a>, uses a combination of a large language model (GPT-3) and a multimodal foundation model (CLIP) to extract concepts for any task. See <a href="https://arxiv.org/abs/2304.06129">paper</a> for details. The reported accuracy of their approach is included in the table below.</p>

<h3 id="limitations-of-resnet-18-backbone">Limitations of ResNet-18 backbone</h3>

<p>I have made it tradition to incorporate DINOv2 into my ml-portfolio series posts. In this post, I replaced the ResNet-18 backbone with DINOv2 and the accuracy improves 15 points. I can‚Äôt think of many applications that would prefer an interpretable model with 60% accuracy over a non-interpretable model with 90% accuracy. NOTE: Although the DINOv2 features were better for overall task accuracy, they did not significantly improve concept accuracy.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Interpretable features</th>
      <th style="text-align: center">Concept labels required</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">0.594</td>
    </tr>
    <tr>
      <td style="text-align: center">Label-free CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">*Y</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">0.743</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.744</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">DINOv2</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.905</td>
    </tr>
  </tbody>
</table>

<p>*Features in this space take on continuous values correlated with human relatable concepts.</p>

<h3 id="alternative-approaches-to-interpretability">Alternative approaches to interpretability</h3>

<p>Saliency/attention maps are a common post-hoc interpretability technique that can be applied to any deep learning model. The purpose of these maps is to show the user which input features the model used for to perform most of their computations (i.e. paid the most attention to). They have one advantage over Concept Bottleneck Models in that they do not affect accuracy.</p>

<p><img src="/assets/images/dino_attention_map.png" alt="attention map" title="attention map" /></p>

<p><a href="https://arxiv.org/pdf/2309.16588">image reference</a></p>

<p>But for the purposes of interpretability, I find that the approach falls short. At best it confirms for the user that the expected parts of the input were used to make a decision and not a spurious object (i.e. a common example is that the class ‚Äúfrisbee‚Äù is assigned to images containing large amounts of green grass). But attention maps fail to explain anything about the internal features of the model and its decision process. DINOv2 attention maps tend to highlight a single, main subject in an image. But it doesn‚Äôt do that because it knows the difference between a cow and a blue heron; it was trained in a totally unsupervised way! Furthermore, attention maps do not provide a mechanism for intervention.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post I demonstrated how to make a machine learning model more interpretable and explainable. I also discussed the tradeoffs and alternative uses for such models when their accuracies do not perform at practical levels. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="explainable" /><category term="interpretable" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post introduces Concept Bottleneck Models - an interpretable approach to machine learning.]]></summary></entry><entry><title type="html">Experiment 16</title><link href="https://dlfelps.github.io/2024/06/03/few-shot.html" rel="alternate" type="text/html" title="Experiment 16" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/03/few-shot</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/03/few-shot.html"><![CDATA[<p>This post explores unique concepts in few-shot learning.</p>

<h2 id="introduction">Introduction</h2>

<p>Few-shot learning describes the situation where a classifier must generalize to new classes using only a few examples of each new class. It represents scenarios where data collection (or annotation) is costly for the classes you care about, but you may have access to related data. There are some unique terms associated with few-shot learning, which we will introduce next.</p>

<p>In standard supervised learning, the training and testing sets contain the SAME classes. The classifier is tested for its ability to discriminate between KNOWN classes.</p>

<p><img src="/assets/images/supervised_learning.PNG" alt="supervised learning" title="supervised learning" /></p>

<p><a href="https://ieeexplore.ieee.org/document/10287966">image reference</a></p>

<p>In few-shot learning, the training and testing sets contain DIFFERENT classes. A few-shot classifier is tested for its ability to discriminate UNKNOWN classes given only a few examples. These samples are further divided into the support set and query set. The support set contains labeled examples (for tuning the few-shot classifier) and the query set contains unlabeled samples for evaluation. The number of classes in the support set is denoted by N and the number of examples per class is denoted by K. A 5-way 1-shot method describes a method that has (N=5) classes with (K=1) examples per class.</p>

<p><img src="/assets/images/few_shot_learning.PNG" alt="few_shot_learning" title="few_shot_learning" /></p>

<p><a href="https://ieeexplore.ieee.org/document/10287966">image reference</a></p>

<h2 id="experiment">Experiment</h2>

<p>This experiment will test a model in a 5-way 5-shot task (i.e. 5 new classes, 5 examples each). We will use a standard benchmark in few-shot learning -  <a href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a> dataset. It is a fine-grained image classification task with over 200 different species of birds. The large number of classes make it suitable for few-shot learning as several can be withheld for testing while maintaining a difficult baseline. We next explain our basic approach to few-shot learning (prototype models) and then compare the performance of three ways to train the model.</p>

<h3 id="few-shot-framework-prototype-model">Few-shot framework: Prototype model</h3>

<p>We adopt a single framework for all variations of our experiment - <a href="https://arxiv.org/abs/1703.05175">Prototypical networks</a>. The concept is based on the idea that there is a single prototype representation for each class. A prototype for each class is calculated from the mean of its support set in the embedding space of a neural network. Our three approaches explore three different ways to train this neural network. Classification in a prototypical model is simply a nearest neighbors using only the prototypes<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. The code uses models and task loaders from <a href="https://github.com/sicara/easy-few-shot-learning">Easy Few-shot learning</a>.</p>

<p><img src="/assets/images/prototypical_network.jpeg" alt="prototypical network" title="prototypical network" /></p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221001818">image reference</a></p>

<h3 id="approach-1-pretrained-model">Approach #1: Pretrained model</h3>

<p>The first approach adapts a pretrained model using transfer learning. The transfer learning process starts with a model that is trained on a large and general dataset (e.g. Imagenet). The purpose of this model is to rely on the learned feature maps - which should be robust - and adapt the later layers to classify your new classes (see image below).</p>

<p><img src="/assets/images/transfer_learning.png" alt="transfer model" title="transfer model" /></p>

<p><a href="https://www.nature.com/articles/s41598-024-54923-y">image reference</a></p>

<p>This experiment uses a ResNet-18 model with pretrained weights from an Imagenet dataset. For each task in the evaluation phase, the support set is used to calculate the prototypes and the the query sets are classified from the prototypes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18(weights=ResNet18_Weights.DEFAULT)
model.fc = nn.Flatten()
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="approach-2-classical-model">Approach #2: Classical model</h3>

<p>This approach uses a subset of the training data to generate a pretrained model. NOTE: the test data contains classes that were not included in the training data. This approach is not always viable since it requires prior knowledge of the few-shot task data distribution. However, it can be powerful if you work in a specific domain and have labeled data from that domain readily available. To simulate it here, we split the 200 species in CUB into 140 for training and 60 for testing. During test time, a 5-way 5-shot task is created by randomly sampling 5 classes from the 60 test classes. This process is repeated 500 times to get a statistically significant measure of generalization performance.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18()
model.fc = nn.Linear(512, 140)
model.load_state_dict(torch.load('/content/easy-few-shot-learning/classical_model_18_acc_744.pt')) # pretrained just for you!
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="approach-3-episodic-model">Approach #3: Episodic model</h3>

<p>Episodic training (also called meta-learning) mirrors the few-shot tasks that will be used to test the final model during the training phase. Each ‚Äúepisode‚Äù is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization. This strategy typically assumes prior knowledge of N and K that will be used at test time. <a href="https://arxiv.org/pdf/2204.11181">ref1</a> <a href="https://arxiv.org/pdf/1703.05175">ref2</a></p>

<p>Episodic training can be performed following a classic training regime if additional data is available or using pretrained-weights if it is not. In this case we reserve the same 140 classes as in Approach #2 for training. But instead of performing classic training, which reduces cross entropy across all classes, we adjust weights based on many training episodes (5-way 5-shot). This process is slower as there is additional overhead required for each episode.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18()
model.fc = nn.Flatten()
model.load_state_dict(torch.load('/content/easy-few-shot-learning/episodic_model_18_acc_779.pt')) # pretrained just for you!
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="bonus-approach-pretrained-model-dinov2-backbone">BONUS Approach: Pretrained model (DINOv2 backbone)</h3>

<p>The three primary variations above can be compared directly because they are all variations of Resnet-18 and use Prototypical models for evaluation. In this bonus approach, we replace the Resnet-18 backbone of the pretrained model (approach #1) with DINOv2. The rest of the setup is the same.</p>

<h2 id="results">Results</h2>

<p>For this purposes of this discussion we will focus on Approaches #1-#3. The accuracy improves with each successive approach, but the assumptions also increase. Namely, in moving from transfer learning to classic learning we assume knowledge of the domain of the test classes and also assume access to labeled examples from that domain. In moving from classic learning to episodic learning we further assume knowledge of the task itself (5-way 5-shot) and the distribution of the data within it (i.e. uniformly sampled). This does not always reflect the real word - a <a href="https://arxiv.org/abs/2204.11181">study</a> showed that if these assumptions are incorrect they can impact performance significantly.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Training approach</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">transfer learning</td>
      <td style="text-align: center">0.684</td>
    </tr>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">classic</td>
      <td style="text-align: center">0.773</td>
    </tr>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">episodic</td>
      <td style="text-align: center">0.779</td>
    </tr>
    <tr>
      <td style="text-align: center">DINOv2</td>
      <td style="text-align: center">transfer learning</td>
      <td style="text-align: center">0.964</td>
    </tr>
  </tbody>
</table>

<h2 id="conclusion">Conclusion</h2>

<p>We compared three approaches to few-shot learning and discussed the conditions and assumptions necessary to use them. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>There is some similarity between Prototypical models and Image retrieval tasks. Image retrieval can be viewed as a prototypical model where each image is its own prototype. And instead of returing a single class, the top-N closest prototypes are returned.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="few-shot" /><category term="computer-vision" /><category term="ml-portfolio" /><category term="meta-learning" /><summary type="html"><![CDATA[This post explores unique concepts in few-shot learning.]]></summary></entry><entry><title type="html">Experiment 15</title><link href="https://dlfelps.github.io/2024/06/02/VIT-amster.html" rel="alternate" type="text/html" title="Experiment 15" /><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/02/VIT-amster</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/02/VIT-amster.html"><![CDATA[<p>This post explores DINOv2 - a foundational vision model from FAIR.</p>

<h2 id="introduction">Introduction</h2>

<p>Yann LeCun is one of the godfathers of deep learning<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. He believes that one of the most critical challenges to solve in the current era is how to give AI systems common sense. Common sense is difficult to define, but I believe that it keeps me alive and allows me to learn new skills helps with relatively few attempts (compared to deep learning). LeCun calls this elusive knowledge the dark matter of AI.</p>

<p>How do we impart this knowledge to our AI systems? LeCun believes the solution lies in self-supervised learning. Self-supervised learning is a technique that adapts tasks that conventionally require labels into an unsupervised learning task (e.g. next word prediction). Self-supervised learning tasks can take advantage of massive amounts of data. If designed properly, the model will learn accurate, meaningful representations of the data. Large Language Models have been popular in the news lately due to their uncanny ability to achieve human-level performance on many tasks. Their sucess is due, in part, to their ability to leverage large amounts of unnannotated text through self-supervised learning.</p>

<p>It is no surprise that the most successful self-supervised computer vision models come from LeCun‚Äôs lab (Fundamental AI Research team at Meta). They published <a href="https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/">DINO (2021)</a> and <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/">DINOv2 (2023)</a> as foundational image models. Foundational models are meant to serve as a resusable backbone for many tasks.</p>

<p><img src="/assets/images/dinov2.png" alt="DINO" title="DINO" />
<a href="https://aipapersacademy.com/dinov2-from-meta-ai-finally-a-foundational-model-in-computer-vision/">source</a></p>

<h2 id="image-retrieval">Image retrieval</h2>

<p>In their <a href="https://arxiv.org/abs/2304.07193">paper</a>, DINOv2 is benchmarked against many SOTA computer vision tasks. One of those tasks is image retrieval. Image retrieval uses images to search for other relevant images. It can help users discover new images guided by visual similarity. Popular benchmarks for image retrieval methods include the <a href="https://paperswithcode.com/dataset/oxford5k">Oxford building</a> and <a href="https://paperswithcode.com/dataset/google-landmarks-dataset-v2">Google Landmarks</a>. In this post I wanted to explore a dataset that was designed to retrieve images through time - the <a href="https://paperswithcode.com/dataset/amstertime">AmsterTime</a> dataset.</p>

<h3 id="amstertime-dataset">AmsterTime dataset</h3>

<p>The AmsterTime dataset offers a collection of 2,500 well-curated images matching the same scene from a street view matched to historical archival image data from Amsterdam city. This is a challenging dataset for image retrieval because the query and gallery sets were taken many years apart so there is often structural changes for the same location.</p>

<p><img src="/assets/images/amstertime.PNG" alt="AmsterTime" title="AmsterTime" /></p>

<h3 id="image-retrieval-with-dinov2">Image retrieval with DINOv2</h3>

<p>Finding similar images with DINOv2 is simple. First, compute the embedding vector for each image in the gallery by passing it through the DINOv2 model. Then, given a query image, compare its embedding vector with those from the gallery using cosine similarity:</p>

<p><img src="/assets/images/cosine.svg" alt="cosine" title="cosine" /></p>

<h4 id="the-embedding-vector">The embedding vector</h4>

<p>DINOv2 is a vision transformer model. As such, it transforms an image into patches and processes those patches a sequence of patches.</p>

<p><img src="/assets/images/vit.png" alt="vit" title="vit" />
<a href="https://arxiv.org/abs/2010.11929v2">source</a></p>

<p>The sequence is processed by the model and the embedding vector referenced here refers to the [CLS] token of the final layer (represented by the ‚Äú*‚Äù in the figure above). Since the [CLS] token does not represent an actual token, the transformer learns to encode a general representation of the entire image into that token. One of the most desirable  features of DINOv2 is that the attention maps associated with the [CLS] token of the last layer tend to be aligned with salient foreground objects (i.e. the model is attending to the primary subject of the image). It is worth looking at the code to see how to extract the [CLS] token:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_cls_token_for_images(images, processor, model):

  collect = []

  for i in tqdm(images):
    inputs = processor(images=i, return_tensors="pt")
    with torch.no_grad():
      outputs = model(**inputs)
    last_hidden_states = outputs.last_hidden_state
    collect.append(last_hidden_states[:,0,:]) # cls token

  return torch.cat(collect)
</code></pre></div></div>

<h2 id="results">Results</h2>

<p>A common metric for reporting results in image retrieval is Recall@N. Here are the results for the AmsterTime dataset:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">query</th>
      <th style="text-align: center">gallery</th>
      <th style="text-align: center">recall@1</th>
      <th style="text-align: center">recall@5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">AmsterTime</td>
      <td style="text-align: center">archival</td>
      <td style="text-align: center">streetview</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: center">0.68</td>
    </tr>
  </tbody>
</table>

<p>Not bad considering the difficulty of the dataset. This means that 43% of the time the top matching image is the correct one. Let‚Äôs look at some of the cases where this worked well (query image on left, correct result from gallery on right).</p>

<p><img src="/assets/images/match_on.png" alt="TP" title="TP" /></p>

<p>It is also informative to look at the cases where the approach failed. Below are the top-10 worst performing queries (largest distances between query and gallery). Each row contains the query image, the top predicted result, and finally the correct result.</p>

<p><img src="/assets/images/mismatch_on.png" alt="FP" title="FP" /></p>

<p>We see that many of the correct results contain occlusions or severe camera distortions, preventing the structural features of the building from being matched. In most of the cases, it is difficult for even a human to match the images.</p>

<p>Since the AmsterTime dataset contains pairs of images across time (1250 archival, 1250 streetview) we can also reverse the gallery and query sets. Surprisingly, its easier to predict the future than the past ;)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">query</th>
      <th style="text-align: center">gallery</th>
      <th style="text-align: center">recall@1</th>
      <th style="text-align: center">recall@5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">AmsterTime</td>
      <td style="text-align: center">archival</td>
      <td style="text-align: center">streetview</td>
      <td style="text-align: center">0.43</td>
      <td style="text-align: center">0.68</td>
    </tr>
    <tr>
      <td style="text-align: center">AmsterTime</td>
      <td style="text-align: center">streetview</td>
      <td style="text-align: center">archival</td>
      <td style="text-align: center">0.35</td>
      <td style="text-align: center">0.57</td>
    </tr>
  </tbody>
</table>

<h3 id="conclusion">Conclusion</h3>

<p>This post demonstrated the potential power of the DINOv2 foundational vision model. The results show that the model is capable of capturing the salient information in an image retrieval task without any prior training. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>

<h4 id="footnotes">Footnotes</h4>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Yoshua Bengio and Geoffrey Hinton are also given this title. In 2018 LeCun, Bengio, and Hinton won the <a href="https://www.acm.org/media-center/2019/march/turing-award-2018">Turing award</a> for their contributions in deep learing.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="vit" /><category term="computer-vision" /><category term="image-retrieval" /><category term="ml-portfolio" /><category term="self-supervised" /><summary type="html"><![CDATA[This post explores DINOv2 - a foundational vision model from FAIR.]]></summary></entry><entry><title type="html">Experiment 14</title><link href="https://dlfelps.github.io/2024/06/01/quantum-smalltalk5.html" rel="alternate" type="text/html" title="Experiment 14" /><published>2024-06-01T00:00:00+00:00</published><updated>2024-06-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/01/quantum-smalltalk5</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/01/quantum-smalltalk5.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Our final post in the Quantum Smalltalk series explores a thought experiment proposed by Avshalom Elitzur and Lev Vaidman to demonstrate an unusual quantum feature - interaction-free measurement.</p>

<h1 id="experiment-14">Experiment #14</h1>
<p>Elitzur and  Vaidman proposed the following thought experiment:</p>

<blockquote>
  <p>You are given 100 EV bombs. Due to a manufacturing problem that was caught too late, some of the triggers are defective. Your job is to try to salvage as many good bombs as possible.</p>
</blockquote>

<h2 id="what-is-an-ev-bomb">What is an EV Bomb?</h2>
<p>This experiment requires us to add a new piece of equipment to our workbench - the EV bomb. This bomb is special because it has a very sensitive trigger: a photon detector. Turn out the lights now because a single photon can cause this bomb to explode!</p>

<p>The following rules describe an EV bomb:</p>
<ol>
  <li>If the trigger is not defective, when a photon enters trigger tube the bomb explodes and you cannot salvage the bomb.</li>
  <li>If the trigger is defective, when a photon enters the trigger tube the bomb does not explode and the photon continues undisturbed out the trigger tube exit.</li>
  <li>The distance from the bomb to the rest of the equipment is sufficiently large to protect the equipment; only the bomb is destroyed if an explosion occurs.</li>
</ol>

<h2 id="attempt-1">Attempt #1</h2>
<p><img src="/assets/images/exp14.png" alt="Experiment #14" title="Experiment #14" /></p>

<p>Our first attempt to salvage the good EV bombs isn‚Äôt very clever. Let‚Äôs just shoot some photons at it and see what happens.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Count</th>
      <th style="text-align: center">Explosion</th>
      <th style="text-align: center">Photon detected at D1</th>
      <th style="text-align: center">Bombs salvaged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">80</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">N/A</td>
    </tr>
  </tbody>
</table>

<p>We tested all the bombs and determined with 100% accuracy which ones were defective and which ones were good. However, we failed to salvage any of the good bombs because they were blown up in the process. If it seems like this thought experiment has no solution, then you are likely restricting yourself to solutions that follow the principle of locality. (For extra details on locality see Appendix B &amp; C below.)</p>

<h2 id="attempt-2">Attempt #2</h2>
<p><img src="/assets/images/exp15.png" alt="Experiment #15" title="Experiment #15" /></p>

<p>Our second attempt should look familiar by now - the Mach-Zehnder setup. We place the bomb on the path between the first beamsplitter and the lower mirror and observe the following results:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Count</th>
      <th style="text-align: center">Explosion</th>
      <th style="text-align: center">Photon detected at D0</th>
      <th style="text-align: center">Photon detected at D1</th>
      <th style="text-align: center">Bombs salvaged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>Clearly we can‚Äôt salvage any of the 40 good bombs that blew up. But what about the remaining 60 that didn‚Äôt explode? Were they all defective? To answer this question, let‚Äôs look at our simulation output. We first observe what happens to photons that pass through a defective EV bomb:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passes through defective EV Bomb placed on the lower path. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.9999999999999996 0.0)
Photon finally detected with following probabilities: &gt;&gt; #(0.9999999999999996 0.0 )

</code></pre></div></div>

<p>Defective EV bombs act as if they aren‚Äôt there (just like Experiment #11) so there is a 100% chance that it will be detected at D0. Good EV bombs give a different result:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon detonates EV Bomb on the lower path with 50% probability. &gt;&gt; #(0.4999999999999999 0)
Photon bouncing off of a mirror. &gt;&gt; #(0.0 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.2499999999999999 0.2499999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.2499999999999999 0.2499999999999999)
</code></pre></div></div>

<p>From the simulation we see that half of the time the EV bomb detonates. Therefore, we can deduce that there were 80 good bombs (because we observed 40 detonating) and 20 bad bombs (the remainder). The output probability for the bombs that don‚Äôt detonate is 25% at D0 and 25% at D1. This means that the 20 bombs we observed at D1 are good! The other 40 bombs that didn‚Äôt detonate and that we observed at D0 have a 50% probability of being good, but we can‚Äôt tell which is which.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Count</th>
      <th style="text-align: center">Explosion</th>
      <th style="text-align: center">Photon detected at D0</th>
      <th style="text-align: center">Photon detected at D1</th>
      <th style="text-align: center">Bombs salvaged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">40</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">20</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">20</td>
    </tr>
  </tbody>
</table>

<h2 id="attempt-3-optional">Attempt #3 (optional)</h2>

<p>It‚Äôs a shame that we were only able to recover 25% of the good bombs. We can achieve the theoretical limit of 50% (or 40 in our example) by using something called the <a href="https://en.wikipedia.org/wiki/Quantum_Zeno_effect?useskin=vector">Quantum Zeno Effect</a>. The details of how to extend our setup using this effect are left up to the reader. I recommend <a href="https://youtu.be/fus1nJ6JaTk?si=Y6ljo3NrULArOx5H&amp;t=375">this video</a>.</p>

<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*bjGo9HLPhqvxcVbdZGnuOA.gif" alt="Quantum Zeno effect" />
<a href="https://medium.com/qiskit/building-quantum-bomb-testers-and-other-thought-experiments-with-quantum-computers-c160060fdde4">source</a></p>

<h1 id="interaction-free-measurement">Interaction-free measurement</h1>

<p>The EV bomb experiment demonstrates a non-local feature of quantum mechanics; we were able to learn something about a path that the photon didn‚Äôt take. Furthermore, we did so without any prior information. This is called an interaction-free measurement.</p>

<p>This is in contrast to, for example, the case where it is known that an object is located in one of two boxes. Looking and <em>not</em> finding it in one box tells us that the object is located inside the other box. This is also an interaction-free measurement, but it does not violate non-locality because we used prior information.</p>

<h1 id="summary">Summary</h1>
<p>Experiment #14 presented another weird quantum feature of interaction-free measurements. Although our treatment was theoretical, know that these results have been duplicated in the <a href="https://www.oeaw.ac.at/fileadmin/Institute/IQOQI-Vienna/PDF/publications-zeilinger/1994_-_Fundamental_Problems_in_Quantum_Theory_-_Experimental_Realization_of_Interaction-Free_Measurement.pdf">lab</a>.</p>

<h1 id="conclusion">Conclusion</h1>
<p>This post concludes the Quantum Smalltalk series. I am amazed at how much of the quantum world we were able to explore using the Mach-Zehnder setup. I hope that these posts piqued your interest in quantum mechanics. There are a lot of great resources available. As a next step, I recommend the book <a href="https://www.amazon.com/Through-Two-Doors-Once-Experiment/dp/1101986093">Through Two Doors at Once: The Elegant Experiment That Captures the Enigma of Our Quantum Reality</a> by Anil Ananthaswamy. Alternatively, if you find general relativity generally fascinating then you might want to explore more advanced topics like the <a href="https://en.wikipedia.org/wiki/Black_hole_information_paradox?useskin=vector">black hole information paradox</a>.</p>

<h1 id="appendix-a-the-smalltalk-code">Appendix A: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>

<h1 id="appendix-b-einstein-and-locality">Appendix B: Einstein and locality</h1>
<p>The principle of locality states that for one point to have an effect one another point, something must travel between the points to cause the effect. The special theory of relativity limits the speed of travel to the speed of light. Therefore an event at point <strong>A</strong> cannot cause a result at point <strong>B</strong> in a time less than <strong>D/c</strong>, where <strong>D</strong> is the distance between the points and <strong>c</strong> is the speed of light in vacuum.</p>

<p>Einstein believed in locality. In one of his 1935 papers, Einstein (along with co-authors Podolsky and Rosen) describe a thought experiment that demonstrates a scenario where quantum mechanics violates locality and concluded that quantum theory does not provide a complete description of reality. The paper ends by saying:</p>

<blockquote>
  <p>‚ÄúWhile we have thus shown that the wave function does not provide a complete description of the physical reality, we left open the question of whether or not such a description exists. We believe, however, that such a theory is possible.‚Äù</p>
</blockquote>

<p>Einstein believed that the solution to the paradox lay in introducing additional (possibly inaccessible) variables. Such a theory is known as a hidden variable theory.</p>

<h1 id="appendix-c-bell-and-locality">Appendix C: Bell and locality</h1>
<p>Almost 30 years after Einstein‚Äôs paper, John Stewart Bell proposed a theory that states that no theory of hidden local variables can ever reproduce all the predictions of quantum mechanics. To date, all such experiments have supported the theory of quantum physics and not the hypothesis of local hidden variables. The 2022 Nobel Prize in Physics was awarded to three scientists (John Clauser, Alain Aspect, and Anton Zeilinger) for their efforts to experimentally validate violations of the Bell inequalities. Bell‚Äôs test proves that quantum mechanics is either non-local itself or has non-local hidden variables.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction Our final post in the Quantum Smalltalk series explores a thought experiment proposed by Avshalom Elitzur and Lev Vaidman to demonstrate an unusual quantum feature - interaction-free measurement.]]></summary></entry><entry><title type="html">Experiment 13</title><link href="https://dlfelps.github.io/2024/05/01/quantum-smalltalk4.html" rel="alternate" type="text/html" title="Experiment 13" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/05/01/quantum-smalltalk4</id><content type="html" xml:base="https://dlfelps.github.io/2024/05/01/quantum-smalltalk4.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>In Experiments #10 and #12 we observed photons acting like particles. In Experiment #11 a photon‚Äôs actions could only be explained using wave-like behavior. In 1978, John Wheeler proposed an experiment to test the limits this paradox. He designed the experiment to answer the following questions:</p>
<ul>
  <li>Does the photon decide to travel as a wave or a particle depending on the experimental setup?</li>
  <li>And if yes, when does the photon decide?</li>
</ul>

<h1 id="experiment-13">Experiment #13</h1>
<p><img src="/assets/images/exp4.gif" alt="Experiment #13" title="Experiment #13" /></p>

<p>Combine the setups from Experiment #11 and #12 by randomly inserting the second beamsplitter AFTER the photon has interacted with the first beam splitter. This allows the photon to take one of six paths:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">BS2 present</th>
      <th style="text-align: center">Path at BS2</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">RX</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TX</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">RR</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">RT</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TR</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TT</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>We have seen both of the setups independently, but now the decision to add the second beamsplitter takes place during the experiment.</p>

<h2 id="results">Results</h2>

<p>Let‚Äôs examine two of the Smalltalk simulation runs. In the first example, the second beamsplitter was omitted from the setup.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Second beamsplitter omitted from setup. 
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.4999999999999999 0.4999999999999999)

</code></pre></div></div>
<p>We observe detection probabilities similar to those in Experiment #12. In the next example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Second beamsplitter inserted into setup after mirror. 
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.9999999999999996 0.0 )
Photon finally detected with following probabilities: &gt;&gt; #(0.9999999999999996 0.0)
</code></pre></div></div>

<p>We observe detection probabilities similar to those in Experiment #11. Filling in the results chart we notice that the table reads as if there were two different experiments going on (i.e. the total observation probability adds up to 2.0!)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">BS2 present</th>
      <th style="text-align: center">Path at BS2</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">RX</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">0.50</td>
    </tr>
    <tr>
      <td style="text-align: center">TX</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">0.50</td>
    </tr>
    <tr>
      <td style="text-align: center">RR/TT</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">RT/TR</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">?</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">1.0</td>
    </tr>
  </tbody>
</table>

<p>This is because there are two experiments happening - as soon as you insert the second beamsplitter you change the experiment.</p>

<h1 id="interpretation">Interpretation</h1>
<p>It is clear that the behavior of the photon in the interferometer depends on the choice of second beamsplitter, even when that choice is made after the photon enters the interferometer. In Wheeler‚Äôs words,</p>

<blockquote>
  <p>‚Äúwe have a strange inversion of the normal order of time. We, now, by moving the mirror in or out have an unavoidable effect on what we have a right to say about the already past history of that photon‚Äù</p>
</blockquote>

<p>It may be tempting to interpret this experiment as a case of <a href="https://en.wikipedia.org/wiki/Retrocausality">retrocausality</a>, whereby the insertion of the second beamsplitter causes the photon to retroactively act like a wave instead of a particle. However, this is a leap too far.</p>

<p>Experiment #13, also known as the <a href="https://en.wikipedia.org/wiki/Wheeler%27s_delayed-choice_experiment">delayed-choice experiment</a>, demonstrates a  subtle point of <a href="https://en.wikipedia.org/w/index.php?title=Wave%E2%80%93particle_duality">wave-particle duality</a> - a photon‚Äôs behavior is <em>always</em> governed by the Schr√∂dinger equation, yet when measured, it is observed as a particle whose location is determined by the <a href="https://en.wikipedia.org/wiki/Born_rule">Born rule</a>.</p>

<p>This is what is meant by a photon acting like a <em>wave</em> and a <em>particle</em>. But what about Experiments #10 and #12, where the photon acted only like a particle and not like a wave? WRONG. The photon was both, but the wavefunction predicted behavior that did not have any interference (i.e. it is particle-like).</p>

<h1 id="summary">Summary</h1>
<p>Experiment #13 presented John Wheeler‚Äôs <a href="https://en.wikipedia.org/wiki/Wheeler%27s_delayed-choice_experiment">delayed-choice experiment</a>. An interesting variation of this experiment is called the <a href="https://en.wikipedia.org/wiki/Delayed-choice_quantum_eraser">delayed-choice quantum eraser</a>.</p>

<h1 id="appendix-the-smalltalk-code">Appendix: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction In Experiments #10 and #12 we observed photons acting like particles. In Experiment #11 a photon‚Äôs actions could only be explained using wave-like behavior. In 1978, John Wheeler proposed an experiment to test the limits this paradox. He designed the experiment to answer the following questions: Does the photon decide to travel as a wave or a particle depending on the experimental setup? And if yes, when does the photon decide?]]></summary></entry><entry><title type="html">Experiment 12</title><link href="https://dlfelps.github.io/2024/04/01/quantum-smalltalk3.html" rel="alternate" type="text/html" title="Experiment 12" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/04/01/quantum-smalltalk3</id><content type="html" xml:base="https://dlfelps.github.io/2024/04/01/quantum-smalltalk3.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Experiment #12 doesn‚Äôt introduce any new concepts, but it prepares us for a surprising result in Experiment #13.</p>

<h1 id="experiment-12">Experiment #12</h1>
<p><img src="/assets/images/exp3.png" alt="Experiment #12" title="Experiment #12" /></p>

<p>Modify the setup from Experiment #11 by removing the second beamsplitter. This allows the photon to take one of two paths:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">R</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">T</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>We don‚Äôt yet know the outcome of the experiment, but based on our observations from Experiment 10 we might expect an equal distribution between D0 and D1. Let‚Äôs run the Smalltalk simulation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.4999999999999999 0.4999999999999999)
</code></pre></div></div>

<p>Our intuition was correct! After the first beamsplitter the photon is equally likely to be on the upper and lower path, but this time the paths are not recombined using the second beamsplitter. So any photons that are reflected at BS1 end up at D0 and any photons that are transmitted at BS1 end up at D1. We always know which path the photon took.</p>

<h2 id="the-wavefunction">The wavefunction</h2>

<p>How does this affect the photon‚Äôs wavefunction? Initially, after interacting with the beamsplitter we do not know which path the photon is on. Therefore its wavefunction is a superposition of states of the upper and lower paths.  At the moment just before the photon would reach D0 or D1, its wavefunction predicts with 50% probability that the photon will be detected at D0 and 50% probability that the photon will be detected at D1. An instant later it is detected at either D0 or D1. But how does the wavefunction predict two equally likely locations in one instant and then the next instant, the probabilities are gone and the photon is detected at D0 or D1?</p>

<h2 id="measurement-of-a-quantum-system">Measurement of a quantum system</h2>

<p>When scientists say they are taking a ‚Äúmeasurement‚Äù they are describing the action or process of observing something in order to gain information. But in quantum mechanics, the term ‚Äúmeasurement‚Äù has both practical and philosophical implications. From a practical point of view, measuring a quantum system changes the quantum state that describes that system; this is sometimes described as causing the wavefunction to collapse (i.e. it is no longer in a superposition of states). From a philosophical point of view, how did the system know it was being measured? In other words, what caused the collapse? How are the probabilities converted into an actual, well-defined classical outcome? The answer to these questions depends on your interpretation of quantum mechanics and is beyond the scope of this post. For those interested, I thoroughly enjoyed listening to this <a href="https://youtu.be/GdqC2bVLesQ?si=yhlTgSpCbw2Gfv74">panel discussion</a> on the topic.</p>

<h1 id="summary">Summary</h1>
<p>Experiment 12 explored the measurement problem - the instant that the photon becomes a particle (as described by the wavefunction). The next experiment challenges our everyday understanding of reality.</p>

<h1 id="appendix-the-smalltalk-code">Appendix: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction Experiment #12 doesn‚Äôt introduce any new concepts, but it prepares us for a surprising result in Experiment #13.]]></summary></entry><entry><title type="html">Experiment 11</title><link href="https://dlfelps.github.io/2024/03/01/quantum-smalltalk2.html" rel="alternate" type="text/html" title="Experiment 11" /><published>2024-03-01T00:00:00+00:00</published><updated>2024-03-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/03/01/quantum-smalltalk2</id><content type="html" xml:base="https://dlfelps.github.io/2024/03/01/quantum-smalltalk2.html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Experiment 11 introduces the concepts of quantum superposition by making a slight modification to Experiment 10.</p>

<h1 id="experiment-11">Experiment #11</h1>
<p><img src="/assets/images/exp2.PNG" alt="Experiment #11" title="Experiment #11" /></p>

<p>Modify the setup from Experiment #10 by removing the block on the lower path. This allows the photon to now take one of four paths:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">State name</th>
      <th style="text-align: center">Path at BS1</th>
      <th style="text-align: center">Path at BS2</th>
      <th style="text-align: center">Detector</th>
      <th style="text-align: center">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">RR</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">RT</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TR</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">Reflected</td>
      <td style="text-align: center">D0</td>
      <td style="text-align: center">?</td>
    </tr>
    <tr>
      <td style="text-align: center">TT</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">Transmitted</td>
      <td style="text-align: center">D1</td>
      <td style="text-align: center">?</td>
    </tr>
  </tbody>
</table>

<p>We don‚Äôt yet know the outcome of the experiment, but based on our observations from Experiment 10 we might expect an equal distribution between D0 and D1. However, that is not what we observe. Let‚Äôs run the Smalltalk simulation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon bouncing off of a mirror. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.9999999999999996 0.0)
Photon finally detected with following probabilities: &gt;&gt; #(0.9999999999999996 0.0)
</code></pre></div></div>

<p>After the first beamsplitter the photon is equally likely to be on the upper and lower path (i.e. 50/50) - the same as in Experiment 10. But this time the lower path is not blocked and the second beam splitter sends all of the photons to D0. How is this possible?</p>

<h2 id="what-is-superposition">WHAT is Superposition</h2>

<p>Quantum mechanics explains this phenomenon by saying that the wavefunction of the photon was in a superposition of two states (i.e. one state going through the upper path and one state going through the lower path). NOTE: this is different that saying that it went through both paths or neither path.</p>

<p>The experiment was designed in such a way that the wavefunction for state RR lags behind state TT by half a wavelength, resulting in destructive interference. The probability amplitude goes to zero and there are no photons at D1. States described by RT and TR arrive with their waves in sync, resulting in constructive interference. The probability amplitude doubles and all the light reaches D0.</p>

<p>The photon itself doesn‚Äôt split because it can‚Äôt. The wavefunction of the photon captures the fact that the photon could be in two equally likely states (lower path and upper path). Then using this wavefunction you can calculate the probability that the photon is detected at D0 and D1 and you get:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Photon outcome</th>
      <th style="text-align: center">probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Detected at D0</td>
      <td style="text-align: center">1.0</td>
    </tr>
    <tr>
      <td style="text-align: center">Detected at D1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<p>But importantly, we cannot say whether the photon was in state TR (upper path) or RT (lower path) only that it was detected at D0. This is an example of particle-wave duality - the photon‚Äôs wavefunction acts like a wave (destructive and constructive interference), but is ultimately measured as a single photon.</p>

<h2 id="why-is-superposition">WHY is Superposition</h2>

<p>No one knows for sure and those that think they do can‚Äôt agree.</p>

<h2 id="when-is-superposition">WHEN is Superposition</h2>

<p>Although we don‚Äôt know why it happens, we can describe the conditions when we observe it. In this case, it happened because the observers (e.g. you, me, or the universe) couldn‚Äôt tell which path the photon took. Each path was sufficiently ambiguous to allow the photon‚Äôs wavefunction to remain in superposition. In Experiment 10, when we blocked the lower path, the photon couldn‚Äôt be in superposition because we knew it must have taken the upper path to arrive at the detectors.</p>

<h1 id="summary">Summary</h1>
<p>Experiment 11 explored the wave-like nature of a photon demonstrated by the destructive interference that occurs between the two superposition states of the system. The next experiment explores what happens when we remove the second beam splitter.</p>

<h1 id="appendix-the-smalltalk-code">Appendix: The Smalltalk code</h1>
<p>The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introduction Experiment 11 introduces the concepts of quantum superposition by making a slight modification to Experiment 10.]]></summary></entry><entry><title type="html">Experiment 10</title><link href="https://dlfelps.github.io/2024/02/01/quantum-smalltalk1.html" rel="alternate" type="text/html" title="Experiment 10" /><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/02/01/quantum-smalltalk1</id><content type="html" xml:base="https://dlfelps.github.io/2024/02/01/quantum-smalltalk1.html"><![CDATA[<h1 id="introducing-the-quantum-smalltalk-series">Introducing the Quantum Smalltalk series</h1>
<p>In this series I am going to introduce you to some of my favorite quantum experiments while modeling those experiments in <a href="https://pharo.org/">Pharo Smalltalk</a>. We will explore the quantum properties of superposition and entanglement. Our experimental model is rather simple - the optical devices typically used in such experiments (i.e. beam splitters, polarizers, mirrors) will be modeled using complex-valued linear transformations. Although the results of matrix multiplication may not surprise you, I hope that the results of these experiments will. I could have called this the ‚ÄúComplex-valued matrix multiplication with Smalltalk series,‚Äù but that doesn‚Äôt have quite the same ring.</p>

<h2 id="why-smalltalk">Why Smalltalk?</h2>
<p>I chose Smalltalk for this series because it allows me to quickly implement an internal (or embedded) domain specific language (DSL) to describe the quantum experiments. You will become more familiar with the DSL as we progress, but here is a sample:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Photon new
	beamSplitter;
	blockLowerPath;
	mirror;
	beamSplitter;
	detector.
</code></pre></div></div>
<p>A good DSL simplifies the code to allow you to focus on the concepts rather than the syntax. A great DSL is self-explanatory, allowing a domain expert (in this case a physicist) to use it without any previous programming experience. I don‚Äôt know any physicists so this is at least a good DSL.</p>

<h2 id="the-mach-zehnder-setup">The Mach-Zehnder setup</h2>
<p>All of the experiments in this series will be variations of the <a href="https://en.wikipedia.org/wiki/Mach%E2%80%93Zehnder_interferometer">Mach-Zehnder interferometer</a>. It was originally proposed in 1891 to measure phase shifts between the two paths caused by a sample, but it has since been adapted to study a variety of quantum effects. Why aren‚Äôt we using the double-slit experiment? It may be the most iconic quantum experiment, but it is also difficult to model because it requires differential equations. We can demonstrate multiple quantum properties (i.e. superposition and entanglement) using the much simpler Mach-Zehnder setup.</p>

<blockquote>
  <p>Feynman once claimed that any question in quantum mechanics could be answered using the double-slit experiment. (But of course he said it as only he could, ‚ÄúYou remember the experiment with the two holes? It‚Äôs the same thing.‚Äù)</p>
</blockquote>

<p>The purpose of the first experiment is to familiarize yourself with the:</p>
<ul>
  <li>experimental setup and optical components used</li>
  <li>modeling approach (i.e. matrix multiplication)</li>
  <li>Smalltalk DSL describing the experimental setup</li>
</ul>

<h3 id="basic-optical-components">Basic optical components</h3>
<p>This section describes the optical components used in the first 2 experiments. Each component is described in common language as well as its equivalent mathematical  transformation. Credit: All of the diagrams and mathematical notation used in this series follows from <a href="https://ocw.mit.edu/courses/8-04-quantum-physics-i-spring-2016">MIT‚Äôs Quantum Physics 1</a>; this is an excellent course taught by Dr. Barton Zwiebach.</p>

<h3 id="photon-emitter">Photon Emitter</h3>
<p>Luckily for us, all of the components have rather descriptive names. In this experiment, we will not be using a coherent laser as the source, but instead a single photon will traverse the optical path (or paths) of the experiment. When a photon is emitted on the upper beam it is represented by the following probability amplitudes:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" data-latex="lower_{path}=\begin{bmatrix}0\\1\end{bmatrix}, upper_{path}=\begin{bmatrix}1\\0\end{bmatrix}" display="block">
  <mrow>
    <mrow>
      <mi data-latex="l">l</mi>
      <mo>&#x2062;</mo>
      <mi data-latex="o">o</mi>
      <mo>&#x2062;</mo>
      <mi data-latex="w">w</mi>
      <mo>&#x2062;</mo>
      <mi data-latex="e">e</mi>
      <mo>&#x2062;</mo>
      <msub data-latex="r_{p a t h}">
        <mi data-latex="r">r</mi>
        <mrow data-mjx-texclass="ORD" data-latex="{p a t h}">
          <mi data-latex="p">p</mi>
          <mo>&#x2062;</mo>
          <mi data-latex="a">a</mi>
          <mo>&#x2062;</mo>
          <mi data-latex="t">t</mi>
          <mo>&#x2062;</mo>
          <mi data-latex="h">h</mi>
        </mrow>
      </msub>
    </mrow>
    <mo data-latex="=">=</mo>
    <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mtable columnspacing="1em" rowspacing="4pt">
        <mtr>
          <mtd>
            <mn data-latex="0">0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn data-latex="1">1</mn>
          </mtd>
        </mtr>
      </mtable>
      <mo data-mjx-texclass="CLOSE">]</mo>
    </mrow>
  </mrow>
  <mo data-latex=",">,</mo>
  <mrow>
    <mrow>
      <mi data-latex="u">u</mi>
      <mo>&#x2062;</mo>
      <mi data-latex="p">p</mi>
      <mo>&#x2062;</mo>
      <mi data-latex="p">p</mi>
      <mo>&#x2062;</mo>
      <mi data-latex="e">e</mi>
      <mo>&#x2062;</mo>
      <msub data-latex="r_{p a t h}">
        <mi data-latex="r">r</mi>
        <mrow data-mjx-texclass="ORD" data-latex="{p a t h}">
          <mi data-latex="p">p</mi>
          <mo>&#x2062;</mo>
          <mi data-latex="a">a</mi>
          <mo>&#x2062;</mo>
          <mi data-latex="t">t</mi>
          <mo>&#x2062;</mo>
          <mi data-latex="h">h</mi>
        </mrow>
      </msub>
    </mrow>
    <mo data-latex="=">=</mo>
    <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mtable columnspacing="1em" rowspacing="4pt">
        <mtr>
          <mtd>
            <mn data-latex="1">1</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn data-latex="0">0</mn>
          </mtd>
        </mtr>
      </mtable>
      <mo data-mjx-texclass="CLOSE">]</mo>
    </mrow>
  </mrow>
</math>

<h3 id="beamsplitter">Beamsplitter</h3>
<p><img src="/assets/images/beamSplitter.PNG" alt="Beamsplitter" title="Beamsplitter" /></p>

<p>If you shine a laser at a balanced beam splitter (the only kind we model here) then exactly 50% of the light is transmitted and the remaining 50% is reflected (also undergoing a phase shift of PI). What happens if you send a single photon instead of a laser? Hopefully you will be able to answer that after the second experiment, but for now assume that there is a 50% chance that it gets reflected and a 50% chance that it gets transmitted. This is represented mathematically by:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" data-latex="\frac{1}{\sqrt{2}}\begin{bmatrix}1 &amp; i\\i &amp; 1\end{bmatrix}" display="block">
  <mfrac data-latex="\frac{1}{\sqrt{2}}">
    <mn data-latex="1">1</mn>
    <msqrt data-latex="\sqrt{2}">
      <mn data-latex="2">2</mn>
    </msqrt>
  </mfrac>
  <mo>&#x2062;</mo>
  <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
    <mo data-mjx-texclass="OPEN">[</mo>
    <mtable columnspacing="1em" rowspacing="4pt">
      <mtr>
        <mtd>
          <mn data-latex="1">1</mn>
        </mtd>
        <mtd>
          <mi data-latex="i">i</mi>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mi data-latex="i">i</mi>
        </mtd>
        <mtd>
          <mn data-latex="1">1</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo data-mjx-texclass="CLOSE">]</mo>
  </mrow>
</math>

<blockquote>
  <h3 id="an-aside-on-probabilities">An aside on probabilities</h3>
  <p>The careful reader will notice that the result of a photon passing through a beamsplitter yields 1/sqrt(2) instead of 1/2. That is because this number represents a <strong>probability amplitude</strong>.</p>
</blockquote>

<math xmlns="http://www.w3.org/1998/Math/MathML" data-latex="\frac{1}{\sqrt{2}}\begin{bmatrix}1 &amp; i\\i &amp; 1\end{bmatrix}\begin{bmatrix}0\\1\end{bmatrix}=\begin{bmatrix}0+\sqrt{2}i\\\sqrt{2}+0i\end{bmatrix}" display="block">
  <mrow>
    <mfrac data-latex="\frac{1}{\sqrt{2}}">
      <mn data-latex="1">1</mn>
      <msqrt data-latex="\sqrt{2}">
        <mn data-latex="2">2</mn>
      </msqrt>
    </mfrac>
    <mo>&#x2062;</mo>
    <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mtable columnspacing="1em" rowspacing="4pt">
        <mtr>
          <mtd>
            <mn data-latex="1">1</mn>
          </mtd>
          <mtd>
            <mi data-latex="i">i</mi>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mi data-latex="i">i</mi>
          </mtd>
          <mtd>
            <mn data-latex="1">1</mn>
          </mtd>
        </mtr>
      </mtable>
      <mo data-mjx-texclass="CLOSE">]</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mtable columnspacing="1em" rowspacing="4pt">
        <mtr>
          <mtd>
            <mn data-latex="0">0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn data-latex="1">1</mn>
          </mtd>
        </mtr>
      </mtable>
      <mo data-mjx-texclass="CLOSE">]</mo>
    </mrow>
  </mrow>
  <mo data-latex="=">=</mo>
  <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
    <mo data-mjx-texclass="OPEN">[</mo>
    <mtable columnspacing="1em" rowspacing="4pt">
      <mtr>
        <mtd>
          <mrow>
            <mn data-latex="0">0</mn>
            <mo data-latex="+">+</mo>
            <mrow>
              <msqrt data-latex="\sqrt{2}">
                <mn data-latex="2">2</mn>
              </msqrt>
              <mo>&#x2062;</mo>
              <mi data-latex="i">i</mi>
            </mrow>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mrow>
            <msqrt data-latex="\sqrt{2}">
              <mn data-latex="2">2</mn>
            </msqrt>
            <mo data-latex="+">+</mo>
            <mrow>
              <mn data-latex="0">0</mn>
              <mo>&#x2062;</mo>
              <mi data-latex="i">i</mi>
            </mrow>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
    <mo data-mjx-texclass="CLOSE">]</mo>
  </mrow>
</math>

<blockquote>
  <p>You can convert a probability amplitude to a probability by taking its magnitude and squaring it.</p>
</blockquote>

<math xmlns="http://www.w3.org/1998/Math/MathML" data-latex="\lvert0+\sqrt{2}i\rvert^2=0.5" display="block">
  <msup data-latex="\rvert^2">
    <mrow>
      <mo data-mjx-texclass="OPEN" fence="false" stretchy="false" data-latex="\lvert">|</mo>
      <mrow>
        <mn data-latex="0">0</mn>
        <mo data-latex="+">+</mo>
        <mrow>
          <msqrt data-latex="\sqrt{2}">
            <mn data-latex="2">2</mn>
          </msqrt>
          <mo>&#x2062;</mo>
          <mi data-latex="i">i</mi>
        </mrow>
      </mrow>
      <mo data-mjx-texclass="CLOSE" fence="false" stretchy="false" data-latex="\rvert">|</mo>
    </mrow>
    <mn data-latex="2">2</mn>
  </msup>
  <mo data-latex="=">=</mo>
  <mn data-latex=".5">0.5</mn>
</math>

<h3 id="mirror">Mirror</h3>
<p><img src="/assets/images/mirror.PNG" alt="Mirror" title="Mirror" /></p>

<p>A mirror reflects an incoming photon while undergoing a phase shift of <em>pi</em>.</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" data-latex="\begin{bmatrix}0 &amp; 1\\1 &amp; 0\end{bmatrix}" display="block">
  <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="\begin{bmatrix}0 &amp; 1\\1 &amp; 0\end{bmatrix}">
    <mo data-mjx-texclass="OPEN">[</mo>
    <mtable columnspacing="1em" rowspacing="4pt">
      <mtr>
        <mtd>
          <mn data-latex="0">0</mn>
        </mtd>
        <mtd>
          <mn data-latex="1">1</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn data-latex="1">1</mn>
        </mtd>
        <mtd>
          <mn data-latex="0">0</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo data-mjx-texclass="CLOSE">]</mo>
  </mrow>
</math>

<h3 id="block">Block</h3>

<p><img src="/assets/images/blockLower.PNG" alt="Block" title="Block" /></p>

<p>A block absorbs an incoming photon, preventing it from reaching any downstream components. In the diagram above, the beamsplitter produces a 50% chance that the photon is absorbed by the block and a 50% chance that the photon is allowed to continue on the upper path.</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" data-latex="block_{lower}=\begin{bmatrix}1 &amp; 0\\0 &amp; 0\end{bmatrix}" display="block">
  <mrow>
    <mi data-latex="b">b</mi>
    <mo>&#x2062;</mo>
    <mi data-latex="l">l</mi>
    <mo>&#x2062;</mo>
    <mi data-latex="o">o</mi>
    <mo>&#x2062;</mo>
    <mi data-latex="c">c</mi>
    <mo>&#x2062;</mo>
    <msub data-latex="k_{l o w e r}">
      <mi data-latex="k">k</mi>
      <mrow data-mjx-texclass="ORD" data-latex="{l o w e r}">
        <mi data-latex="l">l</mi>
        <mo>&#x2062;</mo>
        <mi data-latex="o">o</mi>
        <mo>&#x2062;</mo>
        <mi data-latex="w">w</mi>
        <mo>&#x2062;</mo>
        <mi data-latex="e">e</mi>
        <mo>&#x2062;</mo>
        <mi data-latex="r">r</mi>
      </mrow>
    </msub>
  </mrow>
  <mo data-latex="=">=</mo>
  <mrow data-mjx-texclass="INNER" data-latex-item="{bmatrix}" data-latex="{bmatrix}">
    <mo data-mjx-texclass="OPEN">[</mo>
    <mtable columnspacing="1em" rowspacing="4pt">
      <mtr>
        <mtd>
          <mn data-latex="1">1</mn>
        </mtd>
        <mtd>
          <mn data-latex="0">0</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn data-latex="0">0</mn>
        </mtd>
        <mtd>
          <mn data-latex="0">0</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo data-mjx-texclass="CLOSE">]</mo>
  </mrow>
</math>

<h3 id="photon-detector">Photon Detector</h3>
<p>This component detects a single photon. Most of the experimental setups used in this series will involve 2 detectors: D0 measuring the upper path and D1 measuring the lower path. Just as the emitter emits a real photon, the detector detects a real photon (i.e. it does not measure probability amplitudes or probabilities). The mathematical equivalent of the photon detector would be to sample the probabilities of all possible end-states of the photon (they should add up to 1). We will forgo this step and instead just report the associated probabilities.</p>

<h2 id="experiment-10">Experiment #10</h2>

<p><img src="/assets/images/exp1.PNG" alt="Experiment #10" title="Experiment #10" /></p>

<p>This experiment is relatively straightforward - its result is intuitive and agrees with a more classical interpretation. The setup includes:</p>
<ul>
  <li>1 photon emitter</li>
  <li>1 block</li>
  <li>2 beam-splitters</li>
  <li>2 mirrors</li>
  <li>2 detectors</li>
</ul>

<p>Before we do the math, let‚Äôs guess what might happen if we run this experiment 100 times. If the beamsplitters act like a random coin (heads the photon transmits, tails it reflects) then one might expect the following outcome:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Photon outcome</th>
      <th style="text-align: center">counts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Detected at D0</td>
      <td style="text-align: center">25</td>
    </tr>
    <tr>
      <td style="text-align: center">Detected at D1</td>
      <td style="text-align: center">25</td>
    </tr>
    <tr>
      <td style="text-align: center">Absorbed by block</td>
      <td style="text-align: center">50</td>
    </tr>
  </tbody>
</table>

<p>Next let‚Äôs run the experiment and see if the calculated probabilities agree with our intuition (I‚Äôll defer the Smalltalk code for now and just show the output report).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Emitting a photon from lower path. &gt;&gt; #(0 1)
Photon passing through a beam splitter. &gt;&gt; #(0.4999999999999999 0.4999999999999999)
Photon's path blocked on lower path. &gt;&gt; #(0.4999999999999999 0)
Photon bouncing off of a mirror. &gt;&gt; #(0.0 0.4999999999999999)
Photon passing through a beam splitter. &gt;&gt; #(0.2499999999999999 0.2499999999999999)
Photon finally detected with following probabilities: &gt;&gt; #(0.2499999999999999 0.2499999999999999)
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Photon outcome</th>
      <th style="text-align: center">probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Detected at D0</td>
      <td style="text-align: center">0.25</td>
    </tr>
    <tr>
      <td style="text-align: center">Detected at D1</td>
      <td style="text-align: center">0.25</td>
    </tr>
    <tr>
      <td style="text-align: center">Absorbed by block</td>
      <td style="text-align: center">0.50</td>
    </tr>
  </tbody>
</table>

<p>The computed probabilities agree with our intuition!</p>

<h2 id="summary">Summary</h2>
<p>Experiment 10 laid the groundwork for the quantum properties we want to explore in the rest of the series. It introduced  the optical components and how they are modeled mathematically. However, I have not yet shown the Smalltalk code because it is not as important as the rest. If you want to better understand how to compute the probability amplitudes at an arbitrary point in the path then I would encourage you to consider watching the first few lectures of <a href="https://ocw.mit.edu/courses/8-04-quantum-physics-i-spring-2016">MIT‚Äôs Quantum Physics 1</a>.</p>

<h2 id="appendix-the-smalltalk-code">Appendix: The Smalltalk code</h2>
<p>I was surprised at how simple the Smalltalk DSL code turned out to be. It‚Äôs almost anti-climactic at this point, but the code almost line-by-line describes the order of the components in the experiment. Please keep in mind that the mathematical complexity lies beneath this layer and is hidden from the user. The full Pharo package can be found at <a href="https://github.com/dlfelps/MZI">dlfeps/MZI</a>.</p>

<p><img src="/assets/images/exp1-annotated.PNG" alt="Experiment #1" title="Experiment #1" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1 Photon new
2   beamSplitter;
3   blockLowerPath;
4   mirror;
5   beamSplitter;
6   detector.
</code></pre></div></div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="quantum" /><category term="smalltalk" /><summary type="html"><![CDATA[Introducing the Quantum Smalltalk series In this series I am going to introduce you to some of my favorite quantum experiments while modeling those experiments in Pharo Smalltalk. We will explore the quantum properties of superposition and entanglement. Our experimental model is rather simple - the optical devices typically used in such experiments (i.e. beam splitters, polarizers, mirrors) will be modeled using complex-valued linear transformations. Although the results of matrix multiplication may not surprise you, I hope that the results of these experiments will. I could have called this the ‚ÄúComplex-valued matrix multiplication with Smalltalk series,‚Äù but that doesn‚Äôt have quite the same ring.]]></summary></entry></feed>