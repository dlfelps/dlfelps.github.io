<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://dlfelps.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dlfelps.github.io/" rel="alternate" type="text/html" /><updated>2025-12-15T16:24:26+00:00</updated><id>https://dlfelps.github.io/feed.xml</id><title type="html">.NET Experiments</title><subtitle>Bill Nye meets Bill Gates ;)</subtitle><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><entry><title type="html">Experiment 27</title><link href="https://dlfelps.github.io/2026/01/02/kalman.html" rel="alternate" type="text/html" title="Experiment 27" /><published>2026-01-02T00:00:00+00:00</published><updated>2026-01-02T00:00:00+00:00</updated><id>https://dlfelps.github.io/2026/01/02/kalman</id><content type="html" xml:base="https://dlfelps.github.io/2026/01/02/kalman.html"><![CDATA[<p>This post presents an inventory tracking simulation that demonstrates how Kalman filters optimally estimate total inventory across partially-observable shelves, especially when the system dynamics change over time as items leak to unobserved locations.</p>
<h1 id="understanding-kalman-filters-through-inventory-management">Understanding Kalman Filters Through Inventory Management</h1>

<h2 id="introduction">Introduction</h2>

<p>Imagine managing a warehouse with 20 shelves where you can only check one shelf at a time while items constantly move between adjacent shelves. By the time you’ve checked all the shelves, your early observations are outdated. How do we estimate the total accurately?</p>

<p>This is a partial observability problem, and it’s exactly the kind of challenge that Kalman filters excel at solving. In this post, we’ll explore Kalman filters through a hands-on inventory simulation that makes abstract concepts concrete and intuitive.</p>

<h2 id="what-is-a-kalman-filter">What is a Kalman Filter?</h2>

<p>The <strong>Kalman filter</strong> is a mathematical algorithm that estimates the state of a system from a series of noisy measurements. First developed by Rudolf Kalman in 1960, it has become one of the most widely used algorithms in control theory, robotics, and signal processing.</p>

<h3 id="the-core-idea">The Core Idea</h3>

<p>The Kalman filter answers one question over and over: <em>Given what I believed before and what I just observed, what should I believe now?</em></p>

<p>It maintains a belief about the system state, quantifies uncertainty in that belief, predicts how things will evolve, then updates when new measurements arrive. The balance between trusting predictions versus trusting measurements shifts automatically based on their relative uncertainties.</p>

<p>When the system is linear and noise is Gaussian, the Kalman filter gives you the <strong>optimal</strong> estimate—minimum mean squared error, provably the best you can do. Real systems violate these assumptions constantly, but the filter often works anyway.</p>

<p><img src="/assets/images/blog_figure_6_innovation.png" alt="Innovation Analysis" />
<em>Figure 1: Innovation (prediction error) over time. Large innovations early on show rapid learning; small innovations later indicate convergence.</em></p>

<h2 id="where-youll-find-them">Where You’ll Find Them</h2>

<p>Kalman filters are everywhere, though you usually don’t notice them. Your phone’s GPS is constantly running one to combine satellite signals with accelerometer data. Tesla’s Autopilot uses them to fuse camera and radar readings. Quantitative traders use them to estimate volatility in options pricing. Even old-school applications like radar tracking in air traffic control still rely on Kalman filtering—it’s been the standard solution since the 1960s.</p>

<p>The math works out to be provably optimal when noise is Gaussian and dynamics are linear (minimum mean squared error). In practice, real systems violate these assumptions, but Kalman filters still work surprisingly well. They’re also cheap to run—O(n²) where n is the state dimension, which means even embedded systems can handle them in real time.</p>

<p>What makes them useful is how they handle uncertainty. Unlike simple averaging, they track confidence explicitly through the covariance matrix. When sensors disagree, the filter automatically weights them by reliability. If a sensor drops out entirely, the filter keeps running on predictions alone while uncertainty grows until measurements return.</p>

<h2 id="the-inventory-simulator">The Inventory Simulator</h2>

<p>I built this example specifically to make the abstract math concrete. It’s got all the hard parts—partial observability, noisy measurements, a changing system—but the state is just a single number, so you can actually see what’s happening without drowning in linear algebra.</p>

<h3 id="the-problem-setup">The Problem Setup</h3>

<p>Imagine managing a warehouse with 20 shelves arranged in a circle, containing 300 items that constantly move between adjacent shelves at a rate of 1% per item per timestep. We can only observe one shelf at a time, and shelf #0 is completely hidden from observation. Critically, shelf #0 acts as a dynamic source or sink—items can leak to or from it, and in “leak-then-trap” mode it becomes a one-way sink that captures items permanently.</p>

<p>The goal: estimate the total number of items on the observable shelves (1-19)—a value that changes over time as items leak to the hidden shelf.</p>

<h3 id="what-makes-this-hard">What Makes This Hard</h3>

<p>The obvious problem is partial observability—we can only look at one shelf per timestep. By the time we complete a round-robin cycle through all 19 observable shelves, those early observations are stale. Items have moved.</p>

<p>But the real challenge is that we’re tracking a moving target. The total we’re estimating changes as items leak to or from shelf #0, which we never observe. When the estimate drifts from reality, is that measurement noise or items disappearing into the hidden shelf? The filter has to figure this out.</p>

<p>Then there’s the “leak-then-trap” mode. At step 150, shelf #0 becomes a one-way sink. Items can enter but never leave. The observable total starts declining systematically. The Kalman filter has never seen shelf #0, has no direct evidence of the trap activating, but somehow needs to track the declining total anyway.</p>

<h3 id="why-use-this-example">Why Use This Example?</h3>

<p>The state is just one number—the total item count on observable shelves. No matrices, no complex math, just <code class="language-plaintext highlighter-rouge">x = total</code>. This simplicity lets you actually see what the Kalman gain is doing instead of getting lost in linear algebra.</p>

<p>Starting from complete ignorance (estimate = 0, uncertainty = 1000), you can watch the filter converge. After 200 steps, error drops below 5%. After 1000 steps, it’s stable. The process is visible in a way that multi-dimensional problems never are.</p>

<p><img src="/assets/images/blog_figure_1_convergence.png" alt="Convergence Demonstration" />
<em>Figure 2: Kalman filter convergence from complete ignorance to accurate estimation. The blue line shows the evolving estimate with uncertainty bands (shaded region), converging toward the true total (green dashed line) within 200 steps.</em></p>

<p>Uncertainty is concrete here. A shelf we just observed has uncertainty = 0. A shelf we haven’t checked in 10 timesteps has accumulated uncertainty. Shelf #0, which we never observe, has infinite uncertainty. The measurement noise R literally depends on how stale our observations are.</p>

<p><img src="/assets/images/blog_figure_5_staleness.png" alt="Staleness Visualization" />
<em>Figure 3: Staleness heatmap showing uncertainty across shelves. Recently observed shelves have low uncertainty (yellow), while shelf #0 (never observed) has maximum uncertainty (dark red).</em></p>

<h2 id="the-implementation">The Implementation</h2>

<p>Let’s dive into how the Kalman filter is implemented in the inventory simulator.</p>

<h3 id="state-model">State Model</h3>

<p>Our state is simple:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>State: x = total_items_on_observed_shelves (a single number)
</code></pre></div></div>

<p>Key insight: We’re estimating the total of shelves 1-19 only, not the system total. This value changes as items leak to/from shelf #0.</p>

<p>State transition model:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>State transition: x_next = F × x = 1 × x = x (assume total persists)
Process noise: Q = configurable (default 10.0 for dynamic systems)
</code></pre></div></div>

<p>Process noise <code class="language-plaintext highlighter-rouge">Q</code> quantifies how much we expect the state to change between timesteps beyond what our model predicts. In our simple model, we assume the total persists (<code class="language-plaintext highlighter-rouge">x_next = x</code>), but in reality, items leak to shelf #0, causing the observed total to change unpredictably.</p>

<p><code class="language-plaintext highlighter-rouge">Q</code> directly affects prediction uncertainty: <code class="language-plaintext highlighter-rouge">P_pred = P + Q</code>. Each prediction step, we add <code class="language-plaintext highlighter-rouge">Q</code> to our uncertainty, acknowledging that the state might have changed. This increased uncertainty makes the Kalman gain larger, giving more weight to new measurements and enabling the filter to track changes.</p>

<p>Why is <code class="language-plaintext highlighter-rouge">Q</code> so important now?</p>
<ul>
  <li>Low Q (0.1): Assumes total is nearly constant → <code class="language-plaintext highlighter-rouge">P</code> stays low → <code class="language-plaintext highlighter-rouge">K</code> stays low → filter slow to adapt → lags behind changes</li>
  <li>High Q (10.0): Expects total to change → <code class="language-plaintext highlighter-rouge">P</code> increases faster → <code class="language-plaintext highlighter-rouge">K</code> stays higher → filter more responsive → tracks leaking items</li>
  <li>Too high Q (100.0): Expects too much change → <code class="language-plaintext highlighter-rouge">P</code> grows too large → <code class="language-plaintext highlighter-rouge">K</code> too high → over-responsive, jittery estimates</li>
</ul>

<p>The key insight: <code class="language-plaintext highlighter-rouge">Q</code> controls how quickly uncertainty accumulates when we don’t measure. Higher <code class="language-plaintext highlighter-rouge">Q</code> means “my model might be wrong, listen to the data.” Lower <code class="language-plaintext highlighter-rouge">Q</code> means “my model is reliable, stick with predictions.”</p>

<p>Initial conditions:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_0 = 0.0 (we start knowing nothing)
P_0 = 1000.0 (very uncertain initially)
</code></pre></div></div>

<h3 id="measurement-model">Measurement Model</h3>

<p>Each timestep, we observe one shelf and update our estimates for all shelves in a DataFrame:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Measurement: z = sum(estimated_quantities for all shelves)
Measurement matrix: H = 1 (we measure the total directly as a sum)
Measurement noise: R = 10.0 + 0.5 × sum(uncertainties)
</code></pre></div></div>

<p>The measurement noise <code class="language-plaintext highlighter-rouge">R</code> is critical:</p>
<ul>
  <li><strong>Base noise</strong> (10.0): Accounts for items in motion during observation</li>
  <li><strong>Staleness term</strong> (0.5 × sum of uncertainties): Increases when observations are old</li>
  <li>This makes <code class="language-plaintext highlighter-rouge">R</code> <strong>time-varying</strong> and <strong>data-driven</strong></li>
</ul>

<h3 id="the-kalman-filter-equations">The Kalman Filter Equations</h3>

<p>Since we’re working in 1D, the standard Kalman filter equations become simple arithmetic:</p>

<p><strong>Predict Step:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_pred = x                    # Total doesn't change
P_pred = P + Q                # Add process noise (P + 0.1)
</code></pre></div></div>

<p><strong>Update Step:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z = sum(all shelf estimates)  # Measurement from observations
y = z - x_pred                # Innovation (how wrong was our prediction?)
R = 10.0 + 0.5 × staleness    # Measurement noise (function of uncertainty)
S = P_pred + R                # Innovation covariance
K = P_pred / S                # Kalman gain (0 to 1)
x = x_pred + K × y            # Updated state estimate
P = (1 - K) × P_pred          # Updated uncertainty
</code></pre></div></div>

<h3 id="understanding-the-kalman-gain">Understanding the Kalman Gain</h3>

<p>The Kalman gain <code class="language-plaintext highlighter-rouge">K = P_pred / (P_pred + R)</code> is the heart of the filter. This elegant equation represents the optimal balance between trusting our prediction versus trusting new measurements.</p>

<p>Think of <code class="language-plaintext highlighter-rouge">K</code> as a “trust dial” ranging from 0 to 1:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">K = 1</code> means trust the measurement 100%, ignore the prediction</li>
  <li><code class="language-plaintext highlighter-rouge">K = 0</code> means trust the prediction 100%, ignore the measurement</li>
  <li><code class="language-plaintext highlighter-rouge">K = 0.3</code> means take 30% of the measurement’s correction, 70% of the prediction</li>
</ul>

<p>The denominator <code class="language-plaintext highlighter-rouge">(P_pred + R)</code> represents total uncertainty in our estimate: prediction uncertainty <code class="language-plaintext highlighter-rouge">P_pred</code> plus measurement noise <code class="language-plaintext highlighter-rouge">R</code>. The numerator <code class="language-plaintext highlighter-rouge">P_pred</code> is just our prediction uncertainty. So <code class="language-plaintext highlighter-rouge">K</code> literally answers: “What fraction of total uncertainty comes from our prediction?”</p>

<p>Extreme cases make the intuition clear:</p>

<ul>
  <li>When prediction uncertainty is high (large <code class="language-plaintext highlighter-rouge">P_pred</code>): <code class="language-plaintext highlighter-rouge">K → 1</code>
    <ul>
      <li>Example: <code class="language-plaintext highlighter-rouge">P_pred = 100</code>, <code class="language-plaintext highlighter-rouge">R = 1</code> → <code class="language-plaintext highlighter-rouge">K = 100/101 ≈ 1.0</code></li>
      <li>Our prediction is unreliable, so trust the measurement completely</li>
      <li>Large correction: <code class="language-plaintext highlighter-rouge">x = x_pred + 1 × y = z</code></li>
    </ul>
  </li>
  <li>When measurement noise is high (large <code class="language-plaintext highlighter-rouge">R</code>): <code class="language-plaintext highlighter-rouge">K → 0</code>
    <ul>
      <li>Example: <code class="language-plaintext highlighter-rouge">P_pred = 1</code>, <code class="language-plaintext highlighter-rouge">R = 100</code> → <code class="language-plaintext highlighter-rouge">K = 1/101 ≈ 0.01</code></li>
      <li>The measurement is unreliable, so trust the prediction completely</li>
      <li>Small correction: <code class="language-plaintext highlighter-rouge">x = x_pred + 0.01 × y ≈ x_pred</code></li>
    </ul>
  </li>
</ul>

<p>The filter automatically adapts. Early in the simulation:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">P_pred</code> is large (we’re uncertain)</li>
  <li><code class="language-plaintext highlighter-rouge">K</code> is close to 1 (trust measurements)</li>
  <li>Estimates change rapidly</li>
</ul>

<p>After convergence:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">P_pred</code> is small (we’re confident)</li>
  <li><code class="language-plaintext highlighter-rouge">K</code> is close to 0 (trust prediction)</li>
  <li>Estimates change slowly, smoothly tracking dynamics</li>
</ul>

<p><img src="/assets/images/blog_figure_2_kalman_gain.png" alt="Kalman Gain Evolution" />
<em>Figure 5: Adaptive Kalman gain behavior. Top panel shows K starting near 1.0 (trust measurements) and decreasing to ~0.2 (trust predictions). Bottom panel shows the uncertainty components (P and R) that drive this adaptation. The filter automatically learns when to trust measurements vs. predictions.</em></p>

<h3 id="convergence-behavior">Convergence Behavior</h3>

<p>With default settings (20 shelves, 300 items, 1% movement probability), the filter converges beautifully:</p>

<table>
  <thead>
    <tr>
      <th>Steps</th>
      <th>Estimated Total</th>
      <th>Error (%)</th>
      <th>Uncertainty (P)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.00</td>
      <td>100.0</td>
      <td>1000.0</td>
    </tr>
    <tr>
      <td>100</td>
      <td>95.71</td>
      <td>4.3</td>
      <td>3.2</td>
    </tr>
    <tr>
      <td>200</td>
      <td>94.28</td>
      <td>5.7</td>
      <td>4.3</td>
    </tr>
    <tr>
      <td>500</td>
      <td>95.89</td>
      <td>4.1</td>
      <td>6.0</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>94.61</td>
      <td>5.4</td>
      <td>7.5</td>
    </tr>
    <tr>
      <td>5000</td>
      <td>95.83</td>
      <td>4.2</td>
      <td>9.8</td>
    </tr>
  </tbody>
</table>

<p>Notice:</p>
<ul>
  <li><strong>Rapid initial convergence</strong>: From 100% error to &lt;10% in 200 steps</li>
  <li><strong>Stable performance</strong>: Error stays around 4-6% despite continuous item movement</li>
  <li><strong>Uncertainty stabilizes</strong>: <code class="language-plaintext highlighter-rouge">P</code> converges to ~7-10 after sufficient observations</li>
  <li><strong>Never perfect</strong>: 5% error persists because shelf #0 is never observed</li>
</ul>

<p>This is exactly what we expect from a well-tuned Kalman filter.</p>

<p><img src="/assets/images/blog_figure_3_error_analysis.png" alt="Error Convergence Analysis" />
<em>Figure 6: Multi-panel error analysis. Left: Percentage error drops below 10% threshold rapidly. Middle: Absolute error decreases and stabilizes. Right: Mean Absolute Error (MAE) across observed shelves shows shelf-level accuracy improving over time.</em></p>

<p>The plots reveal:</p>
<ol>
  <li><strong>Convergence plot</strong>: Estimated total (blue line) converges to true total (green line) with uncertainty bands</li>
  <li><strong>Uncertainty plot</strong>: Kalman covariance <code class="language-plaintext highlighter-rouge">P</code> decreases rapidly then stabilizes</li>
  <li><strong>Error plot</strong>: Percentage error drops below 10% and remains stable</li>
</ol>

<h2 id="the-power-of-dynamic-tracking">The Power of Dynamic Tracking</h2>

<p>The real strength of Kalman filters emerges when tracking changing values, not static constants. Our inventory simulator demonstrates this beautifully through the shelf #0 dynamics.</p>

<h3 id="static-vs-dynamic-systems">Static vs. Dynamic Systems</h3>

<p><strong>Old Problem (Boring)</strong>:</p>
<ul>
  <li>Estimate total of ALL shelves (0-19)</li>
  <li>Total is constant at 300 items (conservation law)</li>
  <li>Process noise Q = 0.1 (low, because nothing changes)</li>
  <li>Result: Filter converges to 300 and stays there</li>
  <li>Lesson: Not much to learn - just averaging with fancy math</li>
</ul>

<p><strong>New Problem (Interesting)</strong>:</p>
<ul>
  <li>Estimate total of OBSERVED shelves (1-19) only</li>
  <li>Total changes as items leak to/from shelf #0</li>
  <li>Process noise Q = 10.0 (high, because total is dynamic)</li>
  <li>Result: Filter tracks a moving target</li>
  <li>Lesson: See Kalman filters’ true power - adaptation to changing conditions</li>
</ul>

<h3 id="process-noise-q-the-adaptation-knob">Process Noise Q: The Adaptation Knob</h3>

<p>Process noise <code class="language-plaintext highlighter-rouge">Q</code> determines how much the filter expects the state to change between timesteps. This parameter is critical for dynamic systems:</p>

<p><strong>Q Too Low (Q = 0.1)</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Step 250: True =  92, Estimated =  95.3  (trap activates)
Step 300: True =  75, Estimated =  91.7  (lagging badly)
Step 350: True =  61, Estimated =  85.2  (still lagging)
Step 400: True =  53, Estimated =  76.8  (can't keep up)
</code></pre></div></div>
<p>The filter assumes the total is nearly constant, so it resists change. It lags far behind the declining total.</p>

<p><strong>Q Just Right (Q = 10.0)</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Step 250: True =  92, Estimated =  91.6  (trap activates)
Step 300: True =  75, Estimated =  74.3  (tracking well)
Step 350: True =  61, Estimated =  60.2  (close match)
Step 400: True =  53, Estimated =  52.8  (excellent)
</code></pre></div></div>
<p>The filter expects changes, so it adapts quickly to the declining total while remaining stable.</p>

<p><strong>Q Too High (Q = 100.0)</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Step 250: True =  92, Estimated =  87.2  (trap activates)
Step 300: True =  75, Estimated =  79.1  (overshoots)
Step 350: True =  61, Estimated =  55.8  (overshoots again)
Step 400: True =  53, Estimated =  58.3  (unstable)
</code></pre></div></div>
<p>The filter is over-responsive to measurements, leading to jittery, unreliable estimates that overshoot.</p>

<h3 id="the-kalman-gain-perspective">The Kalman Gain Perspective</h3>

<p>Process noise <code class="language-plaintext highlighter-rouge">Q</code> directly affects the Kalman gain <code class="language-plaintext highlighter-rouge">K = P / (P + R)</code>:</p>

<ul>
  <li>Low Q → Low <code class="language-plaintext highlighter-rouge">P</code> (low uncertainty) → Low <code class="language-plaintext highlighter-rouge">K</code> → Trust predictions more</li>
  <li>High Q → High <code class="language-plaintext highlighter-rouge">P</code> (high uncertainty) → High <code class="language-plaintext highlighter-rouge">K</code> → Trust measurements more</li>
</ul>

<p>In a dynamic system:</p>
<ul>
  <li>Predictions are less reliable (the state changed)</li>
  <li>Measurements carry more information</li>
  <li>Higher <code class="language-plaintext highlighter-rouge">K</code> is appropriate - we need to listen to the data</li>
</ul>

<p><strong>Observed Behavior in Leak-Then-Trap Mode</strong> (see Figure 7, middle panel):</p>
<ul>
  <li>Steps 0-100: <code class="language-plaintext highlighter-rouge">K</code> decreases rapidly from ~0.9 to ~0.15 as the filter learns the system</li>
  <li>Steps 100-250: <code class="language-plaintext highlighter-rouge">K</code> stabilizes around ~0.15 (normal mode, fluctuating total)</li>
  <li>Steps 250+: <code class="language-plaintext highlighter-rouge">K</code> remains elevated at ~0.15-0.2 (trap mode, declining total)</li>
</ul>

<p>The key insight: With <code class="language-plaintext highlighter-rouge">Q = 10.0</code>, the Kalman gain never drops too low. Even after 500 steps, <code class="language-plaintext highlighter-rouge">K ≈ 0.15-0.2</code>, meaning the filter still gives 15-20% weight to new measurements. This responsiveness is exactly what enables tracking the declining total.</p>

<p>Compare this to static systems where <code class="language-plaintext highlighter-rouge">Q = 0.1</code> drives <code class="language-plaintext highlighter-rouge">K → 0.01</code> or lower, making the filter “lock in” to its estimate and resist change. The 100× increase in process noise creates a 10-20× higher steady-state Kalman gain, which is the mechanism enabling adaptation.</p>

<p><img src="/assets/images/blog_figure_7_leak_then_trap.png" alt="Leak-Then-Trap Demonstration" />
<em>Figure 7: Leak-then-trap mode demonstration showing adaptive behavior. <strong>Top panel</strong>: Kalman filter successfully tracks the declining observed total after trap activation at step 150. <strong>Middle panel</strong>: Kalman gain K evolution - decreases during learning phase, then stabilizes at a higher value (~0.15-0.2) during trap mode to maintain responsiveness to changing dynamics. The higher K after step 150 shows the filter trusting measurements more when tracking a moving target. <strong>Bottom panel</strong>: Item distribution showing conservation - as items leak to shelf #0 (orange), the observed total (green) decreases while system total (black line) remains constant at 300.</em></p>

<h2 id="key-insights-and-lessons">Key Insights and Lessons</h2>

<p>Working through this inventory problem teaches several profound lessons about Kalman filters:</p>

<h3 id="1-we-dont-need-perfect-information">1. We Don’t Need Perfect Information</h3>

<p>The filter estimates the total accurately despite:</p>
<ul>
  <li>Never observing 5% of the system (shelf #0)</li>
  <li>Observations being 1-19 timesteps stale</li>
  <li>Items constantly moving</li>
</ul>

<p>Lesson: Kalman filters excel at extracting signal from partial, noisy, delayed data.</p>

<h3 id="2-uncertainty-guides-estimation">2. Uncertainty Guides Estimation</h3>

<p>The staleness-based measurement noise <code class="language-plaintext highlighter-rouge">R</code> is crucial:</p>
<ul>
  <li>Fresh observations get high weight (low <code class="language-plaintext highlighter-rouge">R</code>)</li>
  <li>Stale observations get low weight (high <code class="language-plaintext highlighter-rouge">R</code>)</li>
  <li>The filter automatically balances sources</li>
</ul>

<p>Lesson: Quantifying uncertainty is as important as the measurements themselves.</p>

<h3 id="3-models-dont-need-to-be-perfect">3. Models Don’t Need to Be Perfect</h3>

<p>Our process noise <code class="language-plaintext highlighter-rouge">Q = 0.1</code> is somewhat arbitrary, yet the filter works:</p>
<ul>
  <li>Too small <code class="language-plaintext highlighter-rouge">Q</code> → filter is slow to adapt</li>
  <li>Too large <code class="language-plaintext highlighter-rouge">Q</code> → filter is jittery and oversensitive</li>
  <li>Moderate <code class="language-plaintext highlighter-rouge">Q</code> → good balance</li>
</ul>

<p>Lesson: Kalman filters are robust to model imperfections. Tuning matters, but we don’t need exact models.</p>

<h3 id="4-conservation-constraints-help">4. Conservation Constraints Help</h3>

<p>The assumption that total items is constant (state transition = identity) is a strong prior:</p>
<ul>
  <li>It rules out impossible estimates (e.g., total suddenly doubling)</li>
  <li>It focuses uncertainty on distribution, not total count</li>
  <li>It makes the 1D state sufficient</li>
</ul>

<p>Lesson: Incorporating domain knowledge (like conservation laws) dramatically improves estimation.</p>

<h3 id="5-convergence-takes-time">5. Convergence Takes Time</h3>

<p>The filter needs 100-200 observations to converge well:</p>
<ul>
  <li>Each observation updates the estimate</li>
  <li>Uncertainty decreases with each update</li>
  <li>The round-robin pattern determines convergence rate</li>
</ul>

<p>Lesson: Kalman filters are sequential - they need time to “learn” the system.</p>

<h3 id="6-kalman-gain-is-adaptive">6. Kalman Gain is Adaptive</h3>

<p>Early steps: <code class="language-plaintext highlighter-rouge">K ≈ 0.8-1.0</code> (trust measurements)
Later steps: <code class="language-plaintext highlighter-rouge">K ≈ 0.1-0.3</code> (trust predictions)</p>

<p>Lesson: The filter automatically adapts its behavior based on accumulated knowledge.</p>

<h3 id="7-partial-observability-is-manageable">7. Partial Observability is Manageable</h3>

<p>Shelf #0 is never observed, yet:</p>
<ul>
  <li>The filter estimates the total accurately</li>
  <li>The conservation constraint carries the information</li>
  <li>Inference bridges the gap in direct observation</li>
</ul>

<p>Lesson: Indirect inference can be as powerful as direct measurement when combined with good models.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>The Kalman filter has been around since 1960, which in computer science terms makes it ancient. But it’s still the default solution for fusing noisy measurements because the math actually works—provably optimal under the right conditions, surprisingly robust when those conditions don’t hold.</p>

<p>The inventory simulator demonstrates the core ideas without requiring you to track 10×10 covariance matrices. Partial observability, dynamic systems, measurement uncertainty—it’s all there, just scaled down to something you can reason about. Same principles apply whether you’re tracking items on shelves or estimating spacecraft position.</p>

<h3 id="try-it-yourself">Try It Yourself</h3>

<p>The complete code is available at the repository. Install it, run the examples, and experiment:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/dlfelps/kalman_demo
<span class="nb">cd </span>kalman_demo
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[viz]"</span>
python main.py
</code></pre></div></div>

<p>You can watch the filter converge, plot the uncertainty, tune the parameters, and extend to 2D state. The best way to understand Kalman filters is to see them work.</p>

<p><strong>Generate Blog Figures:</strong></p>

<p>To recreate all figures shown in this blog post:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python generate_/assets/images/blog_figures.py
</code></pre></div></div>

<p>This generates all the figures you saw above—convergence plots, Kalman gain evolution, error analysis, and so on.</p>

<hr />

<h2 id="further-reading">Further Reading</h2>

<p>If you want to go deeper, the original Kalman (1960) paper is surprisingly readable. The Welch &amp; Bishop tutorial from UNC is my go-to reference for implementation details. For interactive learning, check out Roger Labbe’s <a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Kalman and Bayesian Filters in Python</a>—it’s free and excellent. The <a href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/">visual explanation at bzarg.com</a> is also worth a look.</p>

<p>Once you outgrow linear systems, you’ll want Extended Kalman Filters (EKF) for mildly nonlinear problems or Unscented Kalman Filters (UKF) when things get really nonlinear.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="kalman-filter" /><category term="inventory" /><summary type="html"><![CDATA[This post presents an inventory tracking simulation that demonstrates how Kalman filters optimally estimate total inventory across partially-observable shelves, especially when the system dynamics change over time as items leak to unobserved locations. Understanding Kalman Filters Through Inventory Management]]></summary></entry><entry><title type="html">Experiment 26</title><link href="https://dlfelps.github.io/2025/12/02/commuter-patterns.html" rel="alternate" type="text/html" title="Experiment 26" /><published>2025-12-02T00:00:00+00:00</published><updated>2025-12-02T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/12/02/commuter-patterns</id><content type="html" xml:base="https://dlfelps.github.io/2025/12/02/commuter-patterns.html"><![CDATA[<p>This post analyzes urban mobility patterns to identify critical mobility hubs, network resilience, and neighborhood structures within cities by applying centrality algorithms and community detection on graph networks.</p>

<h1 id="urban-mobility-networks-uncovering-commuter-patterns-in-cities">Urban Mobility Networks: Uncovering Commuter Patterns in Cities</h1>

<p>Cities are defined by movement. Every morning, millions of people leave home to reach work. Every evening, they return. This migration shapes everything: traffic congestion, air quality, real estate values, employment opportunities, and social equity.</p>

<p>Yet despite its importance, we rarely see commuting patterns clearly. We experience them—sitting in traffic, missing a meeting, choosing where to live—but we don’t <em>see</em> them as a system. Where exactly do commuters come from? Which corridors carry the heaviest flows? Which neighborhoods are most dependent on distant employment centers? Which areas could benefit most from local job creation?</p>

<p>These questions matter for city planning, transportation policy, and equity. But answering them requires data: fine-grained observations of where people are and where they’re going, aggregated at a scale that reveals patterns without violating privacy.</p>

<p>That’s where the LYMOB dataset comes in.</p>

<h2 id="the-lymob-dataset-aggregated-mobility-traces">The LYMOB Dataset: Aggregated Mobility Traces</h2>

<p>The LYMOB (LYsimob Mobility) dataset captures movement patterns derived from mobile device location traces and cellular network data. Instead of tracking individuals, it aggregates patterns at a population level, creating a mathematical portrait of how cities function.</p>

<p>The data represents each city as a 200×200 grid of cells, where each cell is a geographic region. For each cell pair and time period, we know how many people moved from one cell to another, broken down by hour of day and day type (weekday vs. weekend).</p>

<p>What makes this valuable is the granularity: we know not just <em>that</em> people commute, but <em>when</em> they commute (rush hour vs. afternoon), <em>where from</em> and <em>where to</em>, and <em>how many</em> make each trip. For urban scientists, this is like having X-ray vision into the city’s circulatory system.</p>

<h2 id="building-the-commuter-graph-from-raw-movement-to-network-structure">Building the Commuter Graph: From Raw Movement to Network Structure</h2>

<p>Creating a commuter transit network from raw mobility data requires a clever insight: commuting is fundamentally asymmetric in time, but infrastructure is fundamentally shared.</p>

<p>In the morning, people flow from residential areas into business districts. In the evening, they flow back out. But these don’t use different roads—they use the same corridors in opposite directions. A highway from suburb to the central business district carries rush-hour traffic inbound in the morning and outbound in the evening.</p>

<p>To identify these true commute corridors, we exploited this asymmetry using a method that converts directional flow into a signed graph structure.</p>

<h3 id="step-1-filtering-for-peak-commuting-hours">Step 1: Filtering for Peak Commuting Hours</h3>

<p>I started with hundreds of millions of movement records. But not all movement is commuting. A person buying lunch across town isn’t a commuter; they’re a shopper. To isolate the pure signal of work-related commuting, I filtered specifically for:</p>

<ul>
  <li><strong>Morning commute</strong>: 7 AM to 11 AM (peak inbound flow to employment centers)</li>
  <li><strong>Afternoon commute</strong>: 3 PM to 7 PM (peak outbound flow from employment centers)</li>
  <li><strong>Weekdays only</strong>: Ignoring weekends when commute patterns change dramatically</li>
</ul>

<p>This filtering removed the noise of shopping trips, tourist movements, and weekend activity, leaving me with the core commuting signal.</p>

<h3 id="step-2-converting-directional-flows-to-signed-values">Step 2: Converting Directional Flows to Signed Values</h3>

<p>With filtered commute data in hand, I built a directed graph for the <strong>morning commute</strong>:</p>

<p>For morning hours (7-11 AM), if 50 people traveled from cell A to cell B, I recorded a directed edge A → B with weight 50. This captures the direction and magnitude of morning flow.</p>

<p>But then I performed a crucial transformation: <strong>I converted this directed graph into an undirected graph with signed (positive and negative) edge weights</strong>.</p>

<p>For any pair of nodes (A, B), I ask: which direction did the morning flow go? If the net flow during morning was A → B, I record it as edge (A, B) with weight <strong>+50</strong>. If the flow went the opposite direction B → A, I record it as edge (A, B) with weight <strong>-50</strong>. This converts directionality into a sign while preserving magnitude.</p>

<p>The result is an <strong>undirected graph where edge weights can be positive or negative</strong>, with the sign indicating which direction the morning commute flows.</p>

<p>I repeated this exact process for the <strong>afternoon commute</strong> (3-7 PM), creating a separate undirected signed graph with edge weights indicating afternoon flow direction.</p>

<h3 id="step-3-using-multiplication-to-identify-true-bidirectional-corridors">Step 3: Using Multiplication to Identify True Bidirectional Corridors</h3>

<p>Now comes the elegant part. I multiply the morning and afternoon edge weights together, <strong>keeping only edges whose product is negative</strong>.</p>

<p>Here’s why this works:</p>

<ul>
  <li>
    <p><strong>If a corridor has morning flow A → B and afternoon flow B → A (opposite directions):</strong> Morning produces +50, afternoon produces -30. Multiply: (+50) × (-30) = <strong>-1500</strong>. Keep it. ✓</p>
  </li>
  <li>
    <p><strong>If a corridor has morning flow A → B and afternoon flow A → B (same direction):</strong> Morning produces +50, afternoon produces +30. Multiply: (+50) × (+30) = <strong>+1500</strong>. Discard it. ✗ This isn’t a bidirectional commute corridor.</p>
  </li>
  <li>
    <p><strong>If a corridor has morning flow but no afternoon flow:</strong> Morning produces ±50, afternoon produces 0. Multiply: (±50) × 0 = <strong>0</strong>. Discard it. ✗ Not a true corridor.</p>
  </li>
</ul>

<p>This multiplication filter is mathematically elegant: <strong>it selects exactly the corridors that carry flow in opposite directions at different times of day</strong>. Those are the true commute corridors—the ones connecting residential areas (source morning, destination afternoon) with employment centers (destination morning, source afternoon).</p>

<h3 id="step-4-creating-the-final-commuter-graph">Step 4: Creating the Final Commuter Graph</h3>

<p>For each corridor that survived the multiplication filter (negative product), I create the final edge weight as <strong>the absolute value of the product</strong>. This weight represents the magnitude of bidirectional flow: how much traffic flows through this corridor in both directions combined.</p>

<p>A corridor with morning flow of 50 and afternoon flow of 30 in opposite directions becomes a final weight of 50 × 30 = 1500. A corridor with morning flow of 200 and afternoon flow of 100 in opposite directions becomes 200 × 100 = 20,000. The multiplication naturally weights corridors more heavily when they have balanced, substantial flows in both directions.</p>

<p>The resulting <strong>weighted undirected graph</strong> captured the true commute network: 23,090 nodes (geographic cells) and 683,461 edges (commute corridors) representing approximately 1.6 million total commute trips.</p>

<h2 id="network-algorithms-reading-the-citys-hidden-structure">Network Algorithms: Reading the City’s Hidden Structure</h2>

<p>With a mathematical model of the city’s commuting system in hand, I applied network science algorithms to extract deeper insights. Each algorithm answers different questions about urban organization.</p>

<h3 id="centrality-identifying-critical-hubs">Centrality: Identifying Critical Hubs</h3>

<p><img src="/assets/images/top_nodes_comparison.png" alt="Centrality" /></p>

<p>The most intuitive question is: <em>Which locations are most important?</em></p>

<p>I computed this three different ways, each revealing different aspects of importance.</p>

<p><strong>Degree Centrality</strong> simply counts traffic volume. It answers: “Which node has the most commute activity?” The overwhelming winner is node (77, 135)—Nagoya’s the central business district—with over 21,500 units of commute flow. This isn’t surprising; it’s the magnet pulling workers from across the region.</p>

<p><strong>Eigenvector Centrality</strong> is more subtle. It recognizes that not all connections are equal. A node is important if it connects to <em>other important nodes</em>. Imagine two transit hubs with equal traffic volume: one connects to peripheral neighborhoods, the other connects to other major business districts. The second one is arguably more important to the overall system because it’s in the backbone of the network.</p>

<p>For Nagoya, eigenvector centrality again ranked node (77, 135) at the top with a score of 0.482, validating its role as the central hub. But this algorithm also revealed secondary hubs—places that are important because they connect to other important places, not just because they have high traffic.</p>

<p><strong>PageRank</strong> (yes, the algorithm that powers Google) works by thinking about flow as a process: imagine commuters randomly following corridors through the network. Which nodes would they visit most often? Nodes that receive flow from high-traffic sources get higher PageRank. This reveals nodes that are important within the commute flow process itself, not just structurally central.</p>

<p>Together, these three measures paint a picture: the city has a clear hierarchy. The the central business district is dominant, secondary nodes support the main hub, and the periphery consists of neighborhoods that feed the center.</p>

<p>Most nodes have low scores, but a few have extremely high scores—the signature of a hub-and-spoke urban structure. The single the central business district node dominates all three measures, validating its role as the primary employment magnet.</p>

<h3 id="community-detection-discovering-natural-neighborhoods">Community Detection: Discovering Natural Neighborhoods</h3>

<p><img src="/assets/images/communities.PNG" alt="Communities" /></p>

<p>The next question is: <em>Does the network have structure?</em> Are there natural groupings of neighborhoods that commute internally before connecting to broader networks?</p>

<p>I used the Louvain algorithm, one of the most effective community detection methods. It works by repeatedly asking: “If I move this node from its current group to a neighboring group, does the network become more internally cohesive?” Over many iterations, neighborhoods naturally sort into communities.</p>

<p>The algorithm discovered <strong>57 communities</strong> in the commute network. But here’s the striking part: the top 10 communities contain 94.9% of all commuters. The remaining 47 communities are tiny—often just 2-6 isolated nodes on the periphery.</p>

<p>The figure above shows these major commuting zones, each colored distinctly. Looking at this map, you can immediately see where the area naturally divides into semi-independent commute sheds. This tells us that despite being one integrated metropolitan area, the city actually functions as a collection of 10 semi-independent zones. Someone in Community 1 is far more likely to work within Community 1 than to commute to Community 2. This finding has real implications: each zone could be partially self-sufficient in employment, suggesting where to invest in local job creation versus regional transit.</p>

<h3 id="corridor-analysis-finding-the-bottlenecks">Corridor Analysis: Finding the Bottlenecks</h3>

<p><img src="/assets/images/long_distance_corridors_log_thickness.png" alt="Corridors" /></p>

<p>If communities tell us about regional structure, corridor analysis tells us about the critical links.</p>

<p>I analyzed the top 100 long-distance commute corridors (routes spanning at least 10 cells). The findings revealed a classic pattern: <strong>power-law distribution</strong>.</p>

<p>The single busiest long-distance corridor, from neighborhood (60,161) to the the central business district at (77,135), carries 442 commuters. The second busiest carries 199. The tenth busiest carries 92. By the time you get to the 20th corridor, you’re down to 83 commuters.</p>

<p>Why does this matter? Infrastructure planning. If you’re a city planner with a fixed budget for improving commute corridors, you should focus on those top 68, not spread resources across all 683,461 corridors. Every dollar spent improving the (60,161) → (77,135) corridor benefits 442 people daily. Spending it on a low-flow corridor might benefit only a handful.</p>

<p>The longest corridor I identified spans 53 grid cells, connecting a distant residential area directly to the the central business district. This is a classic pattern in sprawling cities: as residential areas expand outward, some commuters accept very long commutes to reach employment centers rather than moving closer. These long-distance corridors deserve special attention because they’re likely to be the most congested relative to their capacity.</p>

<h3 id="connectivity-and-resilience-is-the-network-robust">Connectivity and Resilience: Is the Network Robust?</h3>

<p>The final algorithmic question concerns robustness: <em>If a major corridor or hub fails, can commuters find alternative routes?</em></p>

<p>I measured this using clustering coefficients. A clustering coefficient measures how many “triangles” exist in the network—three nodes where A connects to B, B connects to C, and A also connects to C. High clustering means lots of alternative routes; low clustering means corridors are more like highways with few off-ramps.</p>

<p>Nagoya’s clustering coefficient is very low (0.000446). This means the network is highly hierarchical and hub-dependent. There aren’t many alternative routes. If you disrupt the the central business district or major corridors leading to it, many commuters have no alternative path.</p>

<p>This is actually typical of transportation networks. They’re not meant to be highly redundant (that would be inefficient and expensive). But it does suggest vulnerabilities. A transit disruption in one of the major corridors could strand thousands of commuters with no obvious alternative.</p>

<h2 id="conclusion-reading-the-citys-structure">Conclusion: Reading the City’s Structure</h2>

<p>Every city has underlying structure—concentrated employment, dispersed housing, funnel-like commute flows. But most cities don’t have fine-grained data making this structure visible.</p>

<p>The LYMOB dataset, combined with network algorithms, reveals what residents experience but can’t quantify. It transforms millions of individual decisions (where to work, where to live, how to get there) into a coherent mathematical structure that shows how cities actually function.</p>

<p>What’s remarkable is how well the mathematics maps to human reality. The hubs we identified match known employment centers. The communities we detected match actual geographic and social boundaries. The power-law distribution matches what traffic engineers observe every day.</p>

<p>By building a mathematical model of commute flows and applying network algorithms, we can answer questions that matter: Which areas are most critical? Where are bottlenecks? Which corridors would benefit most from capacity improvements? Which neighborhoods could absorb more employment to reduce commute pressure?</p>

<p>The tools exist. The data exists. What remains is applying these insights to create more equitable, efficient, and resilient urban systems.</p>

<h2 id="appendix-geographic-discovery---how-we-identified-nagoya">Appendix: Geographic Discovery - How We Identified Nagoya</h2>

<p><img src="/assets/images/nagoya.PNG" alt="Nagoya" />
(left) cityA in the LYMOB dataset (middle) ORNL Landscan Population layer (right) Google Maps of Nagoya region in Japan.</p>

<p>The LYMOB dataset does not disclose that the data represents the Nagoya region. The dataset we looked at is simply called “cityA” - this was done intentionally to increase privacy in the data.</p>

<p>During initial exploration of the data, I noticed that the flow of the cityA data had a rather unique shape. Particularly it featured a double bay and a circular gap, which I assumed was a river feeding the bay. I cross-referenced this pattern against Japan’s coastline and determined that the area was in fact Nagoya, but it was rotated. When I mapped the coordinate system as <strong>X = Longitude, Y = Latitude</strong> , the grid coordinates aligned perfectly with the actual layout on a real map.</p>

<p>This discovery is the first that I could find that uncovered a true location in the LYMOB dataset.</p>

<p><strong>Network Summary:</strong></p>
<ul>
  <li><strong>Nodes:</strong> 23,090 geographic cells</li>
  <li><strong>Edges:</strong> 683,461 commute corridors</li>
  <li><strong>Total trips:</strong> 1.6 million bidirectional commutes</li>
  <li><strong>Connected components:</strong> 46 (largest contains 99.5% of nodes)</li>
  <li><strong>Detected communities:</strong> 57 (top 10 contain 94.9% of commuters)</li>
</ul>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="graph-learning" /><category term="graphs" /><category term="urban-mobility" /><category term="mobility" /><category term="network-analysis" /><category term="networks" /><category term="algorithms" /><summary type="html"><![CDATA[This post analyzes urban mobility patterns to identify critical mobility hubs, network resilience, and neighborhood structures within cities by applying centrality algorithms and community detection on graph networks.]]></summary></entry><entry><title type="html">Experiment 25</title><link href="https://dlfelps.github.io/2025/12/01/sparse-rl.html" rel="alternate" type="text/html" title="Experiment 25" /><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/12/01/sparse-rl</id><content type="html" xml:base="https://dlfelps.github.io/2025/12/01/sparse-rl.html"><![CDATA[<p>This post demonstrates that reinforcement learning agents can be trained with sparse reward signals as effectively as carefully tuned dense rewards.</p>

<h1 id="optimizing-last-mile-delivery-with-reinforcement-learning-from-dense-to-sparse-rewards">Optimizing Last-Mile Delivery with Reinforcement Learning: From Dense to Sparse Rewards</h1>

<h2 id="introduction-the-problem-of-last-mile-delivery">Introduction: The Problem of Last-Mile Delivery</h2>

<p>Last-mile delivery—the final leg of a package’s journey from distribution center to customer—represents one of the most expensive and operationally challenging aspects of modern logistics. According to industry research, last-mile costs can account for 50-60% of total shipping expenses, and with the explosive growth of e-commerce, the problem has only become more acute.</p>

<p>The core challenge is deceptively simple to state: Given a fleet of vehicles with limited capacity, a stream of daily package arrivals with varying priorities and delivery deadlines, and geographic constraints on which truck can serve which zone, how do you decide <strong>which packages to deliver today and which to defer</strong> to maximize operational value while meeting customer expectations?</p>

<p>This is a constrained optimization problem that sits at the intersection of:</p>
<ul>
  <li><strong>Vehicle Routing Problems (VRP)</strong>: Classical logistics optimization</li>
  <li><strong>Knapsack Problems</strong>: Capacity-constrained selection</li>
  <li><strong>Temporal Scheduling</strong>: Deadline-aware decision making</li>
  <li><strong>Reinforcement Learning</strong>: Learning optimal policies from experience</li>
</ul>

<p>Traditional approaches to logistics optimization have relied on integer linear programming, metaheuristics (genetic algorithms, simulated annealing), and handcrafted rule-based systems. But what if we could train an agent to learn better strategies through experience? And what if we could do it <strong>without designing a complex reward function</strong>?</p>

<p>That’s the premise of this project, and the surprising answer lies in <strong>sparse reward learning</strong>.</p>

<hr />

<p>The Simulation Model</p>

<p>We model the delivery optimization problem as a <strong>constrained capacity problem with 10 trucks, 10 zones, and 30 daily package arrivals</strong>:</p>

<p><strong>Infrastructure:</strong></p>
<ul>
  <li><strong>Warehouse</strong>: Central depot (location 0)</li>
  <li><strong>Zones</strong>: 10 geographic zones, each with 4 delivery addresses (40 total addresses)</li>
  <li><strong>Trucks</strong>: 10 trucks, one assigned per zone
    <ul>
      <li>Capacity: 2 packages per truck per day</li>
      <li>Total daily fleet capacity: 20 packages</li>
    </ul>
  </li>
</ul>

<p><strong>Daily Arrivals:</strong></p>
<ul>
  <li><strong>Volume</strong>: 30 packages per day</li>
  <li><strong>Shortage</strong>: 33% capacity shortage (must defer 10 packages)</li>
  <li><strong>Priority</strong>: Bimodal distribution
    <ul>
      <li>40% low-priority (1-3)</li>
      <li>40% high-priority (8-10)</li>
      <li>20% medium-priority (4-7)</li>
    </ul>
  </li>
  <li><strong>Deadlines</strong>: 1-7 days from arrival</li>
  <li><strong>Destinations</strong>: Uniformly distributed across zones</li>
</ul>

<hr />

<p>The Core Decision Problem</p>

<p>Each day, each truck independently faces the decision:</p>
<blockquote>
  <p><strong>“Of my available packages (new arrivals + deferred backlog), which 2 should I deliver today?”</strong></p>
</blockquote>

<p>This is non-trivial because:</p>
<ol>
  <li><strong>Capacity is limited</strong>: Can’t deliver everything</li>
  <li><strong>Deadlines create urgency</strong>: Some packages expire soon</li>
  <li><strong>Priorities are heterogeneous</strong>: High-priority packages are worth more</li>
  <li><strong>Deferrals have costs</strong>: Deferred packages tie up future capacity</li>
  <li><strong>Information is incomplete</strong>: Don’t know future arrivals</li>
</ol>

<h2 id="algorithmic-solutions-from-dense-to-sparse-rewards">Algorithmic Solutions: From Dense to Sparse Rewards</h2>

<h3 id="1-greedy-baseline">1. Greedy Baseline</h3>

<p><strong>Algorithm</strong>: Deliver all available packages (up to capacity limit)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For each truck:
  available_packages = deferred + new_arrivals
  deliver = sort_by_priority(available_packages)[:capacity]
</code></pre></div></div>

<p><strong>Characteristics:</strong></p>
<ul>
  <li>Deterministic</li>
  <li>No learning required</li>
  <li>Simple to implement</li>
  <li>Treats all packages equally (ignores deadline pressure)</li>
</ul>

<p><strong>Performance Baseline</strong>:</p>
<ul>
  <li>Average daily reward: 805.7</li>
  <li>On-time delivery: 75.6%</li>
  <li>No variance (deterministic)</li>
</ul>

<h3 id="2-dense-reward-pposac-traditional-rl-approach">2. Dense Reward PPO/SAC (Traditional RL Approach)</h3>

<p><strong>Reward Function</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total Reward = Σ(delivered_package_value) - Σ(missed_deadline_penalty) - distance_cost

Where:
  delivered_package_value = 100 × (priority / 10)
  missed_deadline_penalty = -50 × (priority / 10)
  distance_cost = -0.01 × kilometers_traveled
</code></pre></div></div>

<p><strong>Key Characteristics</strong>:</p>
<ul>
  <li>Complex reward engineering required</li>
  <li>Hyperparameters: 100 (delivery weight), -50 (miss penalty), -0.01 (distance)</li>
  <li>Requires extensive domain knowledge to set correctly</li>
  <li>Dense signal ranges from -500 to +1000 per decision</li>
  <li>Scaling factors need careful tuning</li>
</ul>

<h3 id="3-sparse-reward-pposac-novel-approach">3. Sparse Reward PPO/SAC (Novel Approach)</h3>

<p><strong>Core Insight</strong>: What if we don’t need a complex reward function at all? What if agents can learn from <strong>comparative signals only</strong>?</p>

<p><strong>Sparse Reward Signal</strong>:</p>

<p>Instead of <code class="language-plaintext highlighter-rouge">-500 to +1000</code>, agents receive:</p>
<ul>
  <li><strong>+1</strong>: When policy outperforms what the greedy baseline would have done</li>
  <li><strong>-1</strong>: When policy underperforms the baseline</li>
  <li><strong>0</strong>: When policy matches the baseline</li>
</ul>

<p><strong>Why This Matters</strong>:</p>
<ul>
  <li>No hyperparameter tuning needed (just +1 and -1)</li>
  <li>No domain knowledge required to scale rewards</li>
  <li>Agents learn from pure comparison: “Am I better or worse?”</li>
  <li>Signal is invariant to absolute reward magnitude</li>
  <li><strong>Simpler, more robust, more generalizable</strong></li>
</ul>

<p><strong>Architecture Unchanged</strong>:</p>
<ul>
  <li>Same zone-agnostic shared policy</li>
  <li>Same observations: priority, deadline_urgency, distance, truck_load</li>
  <li>Same action space: continuous confidence scores for packages</li>
  <li>Same training (PPO and SAC) and duration (500k timesteps)</li>
</ul>

<p><strong>The only difference</strong>: The reward signal during training</p>

<hr />

<h2 id="the-surprising-results-sparse-wins-over-dense">The Surprising Results: Sparse Wins Over Dense</h2>

<p>We evaluated all agents (Greedy, Dense PPO, Dense SAC, Sparse PPO, Sparse SAC) using a <strong>relative value metric</strong>: How much better/worse than the baseline?</p>

<table>
  <thead>
    <tr>
      <th>Agent</th>
      <th>Daily Advantage</th>
      <th>Win Rate</th>
      <th>Max Advantage</th>
      <th>Std Dev</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Sparse PPO</strong> 🏆</td>
      <td><strong>+464.8</strong></td>
      <td>98.6%</td>
      <td>+1020.2</td>
      <td>200.7</td>
    </tr>
    <tr>
      <td><strong>Dense PPO</strong></td>
      <td>+461.5</td>
      <td>99.6%</td>
      <td>+980.0</td>
      <td>192.5</td>
    </tr>
    <tr>
      <td>Dense SAC</td>
      <td>+399.7</td>
      <td>96.8%</td>
      <td>+999.5</td>
      <td>220.0</td>
    </tr>
    <tr>
      <td>Sparse SAC</td>
      <td>+362.4</td>
      <td>96.6%</td>
      <td>+860.1</td>
      <td>198.8</td>
    </tr>
    <tr>
      <td><strong>Greedy Baseline</strong></td>
      <td>0 (reference)</td>
      <td>50%</td>
      <td>0</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/images/01_absolute_rewards.png" alt="Absolute Rewards" /></p>

<p>Both PPO agents significantly outperform SAC and the baseline. The Sparse PPO’s slight edge over Dense PPO (127,054 vs 126,720) is noteworthy because it achieves this with a 1000x simpler reward signal.</p>

<hr />

<p><img src="/assets/images/02_relative_values.png" alt="Relative Values" /></p>

<p>This is the most important metric: How much better/worse than the greedy baseline?</p>

<p>The heights are nearly identical for the PPO variants, showing that sparse and dense rewards achieve essentially the same end result. The slight Sparse PPO edge (+3.3 points) is statistically insignificant but philosophically important: <strong>simpler signals achieved better results</strong>.</p>

<p>The SAC agents’ lower performance suggests that SAC’s off-policy nature may require denser reward signals for effective learning, or that our SAC hyperparameters weren’t as well-tuned as PPO’s.</p>

<hr />

<p><img src="/assets/images/03_win_rates.png" alt="Win Rates" /></p>

<p>Win rate = percentage of days the agent beats the greedy baseline</p>

<p><strong>Interpretation</strong>:</p>
<ul>
  <li>All agents are highly reliable, beating the baseline &gt;96% of the time</li>
  <li>Dense PPO has a slight edge in consistency (99.6% vs 98.6%)</li>
  <li>But this comes at the cost of complex reward engineering</li>
  <li>The trade-off is acceptable: Sparse PPO’s 98.6% win rate is still excellent</li>
  <li>Only 7 days out of 500 where Sparse PPO underperforms (1.4% failure rate)</li>
</ul>

<hr />

<p><img src="/assets/images/04_relative_evolution.png" alt="Relative Evolution" /></p>

<p>This plot shows how the daily advantage changes throughout a 100-day episode</p>

<p><strong>Interpretation</strong>:</p>
<ul>
  <li>All agents maintain stable performance across the full episode</li>
  <li>No degradation as days progress (agents don’t “tire out”)</li>
  <li>No improvement trend (agents don’t learn within an episode; learning happens across episodes)</li>
  <li>Sparse PPO’s consistency mirrors Dense PPO perfectly</li>
  <li>This stability is crucial for real-world deployment: reliable day-after-day performance</li>
</ul>

<h2 id="why-does-sparse-reward-learning-work">Why Does Sparse Reward Learning Work?</h2>

<p>Agents trained with only <strong>-1, 0, +1</strong> signals matched or exceeded agents trained with <strong>-500 to +1000</strong> dense rewards. This challenges conventional RL wisdom. Why?</p>

<h3 id="hypothesis-1-signal-noise-reduction">Hypothesis 1: Signal Noise Reduction</h3>

<p>Dense rewards encode both:</p>
<ol>
  <li><strong>Direction</strong>: Should I improve or degrade my behavior?</li>
  <li><strong>Magnitude</strong>: By how much?</li>
</ol>

<p>The magnitude component introduces noise:</p>
<ul>
  <li>How much should I weight a +500 reward vs +550?</li>
  <li>Are the scaling factors (100, -50, -0.01) optimal?</li>
  <li>What if I increase scale 2x? The agent relearns everything.</li>
</ul>

<p>Sparse signals provide only <strong>direction</strong>, removing scaling noise:</p>
<ul>
  <li>Better than baseline? → +1 (learn this policy)</li>
  <li>Worse? → -1 (unlearn this policy)</li>
  <li>Same? → 0 (neutral)</li>
</ul>

<p>This <strong>cleaner signal</strong> might be more learnable.</p>

<h3 id="hypothesis-2-relative-framebench-built-in">Hypothesis 2: Relative Framebench Built-In</h3>

<p>Dense rewards are <strong>absolute</strong>: “This decision is worth 547 points.”</p>

<p>Sparse rewards are <strong>relative</strong>: “This decision beats the baseline.”</p>

<p>Relative framebench is more robust:</p>
<ul>
  <li>Generalizes across different problem scales</li>
  <li>Adapts if baseline changes (e.g., competitor improves)</li>
  <li>Built-in regularization against extreme behaviors</li>
  <li>Agents learn what matters: being <strong>better, not just good</strong></li>
</ul>

<h3 id="hypothesis-3-simpler-loss-landscape">Hypothesis 3: Simpler Loss Landscape</h3>

<p>Dense reward functions create complex, multi-objective loss surfaces:</p>
<ul>
  <li>Deliver high-priority: +100 × priority/10</li>
  <li>Avoid misses: -50 × priority/10</li>
  <li>Minimize distance: -0.01 × km</li>
</ul>

<p>Three conflicting objectives, each with scaling factors.</p>

<p>Sparse signals create simpler landscapes:</p>
<ul>
  <li>Single objective: maximize probability of beating baseline</li>
  <li>Binary feedback (better/worse)</li>
  <li>Cleaner gradient flow</li>
</ul>

<p>Simpler landscapes = faster convergence, better local optima.</p>

<h2 id="real-world-advantages-of-sparse-reward-learning">Real-World Advantages of Sparse Reward Learning</h2>

<p>In academic settings, we can carefully design reward functions. In the real world, this is <strong>hard</strong>:</p>

<ol>
  <li><strong>Unknown Objectives</strong>: What’s the true business objective?
    <ul>
      <li>Maximize revenue? Minimize costs? Customer satisfaction? All of the above?</li>
      <li>Often these conflict</li>
    </ul>
  </li>
  <li><strong>Scaling Uncertainty</strong>: How to weight different objectives?
    <ul>
      <li>Is on-time delivery worth 2x cost savings or 10x?</li>
      <li>How much does customer satisfaction matter vs revenue?</li>
      <li>These weights differ by market, season, customer segment</li>
    </ul>
  </li>
  <li><strong>Lack of Domain Knowledge</strong>: What if you’re optimizing something new?
    <ul>
      <li>Traditional logistics expertise doesn’t transfer perfectly</li>
      <li>New market = new constraints</li>
      <li>Hand-tuned weights from one domain fail in another</li>
    </ul>
  </li>
  <li><strong>Hyperparameter Sensitivity</strong>: Dense rewards require constant retuning
    <ul>
      <li>Market conditions change → tune weights again</li>
      <li>Competitor behavior changes → tune again</li>
      <li>Seasonal variations → more tuning</li>
      <li><strong>No end to the tuning work</strong></li>
    </ul>
  </li>
</ol>

<p>With sparse rewards, you <strong>only need one thing</strong>: A baseline policy</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define baseline (e.g., greedy heuristic, existing system, competitor)
</span><span class="n">baseline_action</span> <span class="o">=</span> <span class="nf">get_greedy_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="n">baseline_reward</span> <span class="o">=</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">baseline_action</span><span class="p">)</span>

<span class="c1"># Define learned policy
</span><span class="n">learned_action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="n">learned_reward</span> <span class="o">=</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">learned_action</span><span class="p">)</span>

<span class="c1"># That's it! No hyperparameter tuning needed
</span><span class="k">if</span> <span class="n">learned_reward</span> <span class="o">&gt;</span> <span class="n">baseline_reward</span><span class="p">:</span>
    <span class="n">reward_signal</span> <span class="o">=</span> <span class="o">+</span><span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">reward_signal</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div>

<p><strong>Advantages</strong>:</p>
<ul>
  <li>✅ No reward scaling needed</li>
  <li>✅ Works with any baseline (greedy, heuristic, human expert, previous system)</li>
  <li>✅ Automatically adapts as baseline changes</li>
  <li>✅ Same code works for different objectives (just change baseline)</li>
  <li>✅ Explainable: “We learned a policy better than your existing system”</li>
</ul>

<p>This is <strong>profoundly more practical</strong> for real systems.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This project demonstrates two critical insights:</p>

<p><strong>First</strong>: Reinforcement learning can discover sophisticated logistics optimization strategies. Our shared policy agents beat a greedy baseline by 57-58% on the same problem instances that professional logistics systems solve with static heuristics.</p>

<p><strong>Second, and more surprising</strong>: You might not need a complex reward function to achieve this. A simple comparative signal (+1 if better, -1 if worse) matched or exceeded carefully tuned dense rewards (-500 to +1000).</p>

<p>This challenges a fundamental assumption in RL: <strong>that bigger, richer reward signals are always better</strong>. In practice, simpler signals that provide clear direction (“beat the baseline”) might be more learnable and generalizable than complex signals with precise magnitudes.</p>

<p>The counter-intuitive result—that sparse PPO (+464.8 advantage) slightly outperformed dense PPO (+461.5)—suggests that <strong>how you train matters more than what you train with</strong>. A cleaner learning signal, even if simpler, can outweigh richer but more complex information.</p>

<p>For practitioners building real-world RL systems: <strong>Stop optimizing your reward function. Start comparing against your baseline.</strong> It’s simpler, more practical, and achieves the same (or better) results.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Optimization problems in logistics have been solved for decades using mathematical programming. Traditional RL adds learning capability. But <strong>sparse reward learning adds something different: simplicity and adaptability</strong>.</p>

<p>A traditional solver finds the optimal solution given a model. But if your model is wrong or incomplete, you’ve optimized the wrong thing.</p>

<p>A dense reward agent learns a policy that maximizes your hand-tuned objective. But if that objective changes (market shifts, new constraint discovered), the training loses relevance.</p>

<p>A sparse reward agent learns one simple thing: <strong>How to beat what you already have.</strong> This is more robust, more adaptable, and more practical.</p>

<p>The next generation of logistics software—and optimization software broadly—won’t just optimize given constraints. It will learn baseline-relative improvement through sparse, adaptive signals.</p>

<p>And sometimes, the simpler signal is the stronger one.</p>

<p>Code: Available in this <a href="https://github.com/dlfelps/sparse-rl">repository</a></p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="reinforcement-learning" /><category term="rl" /><category term="ppo" /><category term="sparse" /><category term="soft-actor-critic" /><category term="optimization" /><category term="policy" /><category term="last-mile" /><summary type="html"><![CDATA[This post demonstrates that reinforcement learning agents can be trained with sparse reward signals as effectively as carefully tuned dense rewards.]]></summary></entry><entry><title type="html">Experiment 24</title><link href="https://dlfelps.github.io/2025/11/01/langextract.html" rel="alternate" type="text/html" title="Experiment 24" /><published>2025-11-01T00:00:00+00:00</published><updated>2025-11-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/11/01/langextract</id><content type="html" xml:base="https://dlfelps.github.io/2025/11/01/langextract.html"><![CDATA[<p>The blog post demonstrates that few-shot LLMs can match 96% of fine-tuned model accuracy on scene graph extraction with 103x faster inference.</p>

<h1 id="comparing-scene-graph-extraction-methods-from-fine-tuned-t5-to-few-shot-llms">Comparing Scene Graph Extraction Methods: From Fine-tuned T5 to Few-Shot LLMs</h1>

<h2 id="introduction">Introduction</h2>

<h3 id="what-is-scene-graph-parsing">What is Scene Graph Parsing?</h3>

<p>In computer vision and natural language processing, <strong>scene graph parsing</strong> is the task of extracting structured representations from image captions or visual scenes. A scene graph breaks down a description into three key components:</p>

<ol>
  <li><strong>Entities</strong>: The objects, people, or things in the scene (e.g., “dog”, “bench”, “person”)</li>
  <li><strong>Attributes</strong>: Properties describing entities (e.g., “brown dog”, “wooden bench”)</li>
  <li><strong>Relationships</strong>: Interactions and spatial relationships between entities (e.g., “dog sits on bench”, “person next to tree”)</li>
</ol>

<p>For example, given the caption:</p>
<blockquote>
  <p>“A brown dog sitting on a wooden bench in a park”</p>
</blockquote>

<p>A scene graph would extract:</p>
<ul>
  <li><strong>Entities</strong>: dog, bench, park</li>
  <li><strong>Attributes</strong>: (dog, brown), (dog, sitting), (bench, wooden)</li>
  <li><strong>Relationships</strong>: (dog, sit on, bench), (bench, in, park)</li>
</ul>

<h3 id="why-scene-graphs-matter">Why Scene Graphs Matter</h3>

<p>Scene graphs provide structured, machine-readable representations of visual information, enabling:</p>
<ul>
  <li><strong>Visual Question Answering</strong>: Understanding spatial relationships to answer questions like “What is the dog sitting on?”</li>
  <li><strong>Image Retrieval</strong>: Finding images based on specific object relationships</li>
  <li><strong>Robotics</strong>: Helping robots understand and navigate physical environments</li>
  <li><strong>Accessibility</strong>: Generating rich descriptions for visually impaired users</li>
</ul>

<h3 id="the-challenge">The Challenge</h3>

<p>Traditional approaches to scene graph extraction rely on fine-tuning specialized models on domain-specific datasets. While effective, this approach has limitations:</p>
<ul>
  <li>Requires large amounts of labeled training data</li>
  <li>Models are domain-specific and don’t generalize well</li>
  <li>Fine-tuning is computationally expensive</li>
  <li>Slow inference times due to model size</li>
</ul>

<p><strong>Can modern large language models (LLMs) match or exceed fine-tuned models using only few-shot learning?</strong></p>

<p>This blog post documents a series of experiments comparing:</p>
<ol>
  <li>A fine-tuned T5 model (baseline)</li>
  <li>LLM-based extraction using Google’s LangExtract framework with Gemini</li>
  <li>Native Gemini structured output</li>
  <li>Iteratively improved few-shot prompting</li>
</ol>

<p>All experiments use the <strong>FACTUAL Scene Graph dataset</strong>, which contains 50,000+ image captions with ground-truth scene graph annotations.</p>

<hr />

<h2 id="experiment-1-t5-baseline-performance">Experiment 1: T5 Baseline Performance</h2>

<p>Establish baseline performance using a fine-tuned T5 model pre-trained on the FACTUAL dataset.</p>

<ul>
  <li><strong>Model</strong>: <code class="language-plaintext highlighter-rouge">flan-t5-base-VG-factual-sg</code> (220M parameters)</li>
  <li><strong>Test Set</strong>: 100 complex samples (captions with &gt;20 words)</li>
  <li><strong>Evaluation</strong>: Precision, Recall, and F1 for entities, attributes, and relationships</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1 Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Entities</strong></td>
      <td>0.946</td>
      <td>0.884</td>
      <td><strong>0.907</strong></td>
      <td>343</td>
    </tr>
    <tr>
      <td><strong>Attributes</strong></td>
      <td>0.822</td>
      <td>0.770</td>
      <td><strong>0.782</strong></td>
      <td>305</td>
    </tr>
    <tr>
      <td><strong>Relationships</strong></td>
      <td>0.722</td>
      <td>0.635</td>
      <td><strong>0.662</strong></td>
      <td>227</td>
    </tr>
    <tr>
      <td><strong>Macro F1</strong></td>
      <td>-</td>
      <td>-</td>
      <td><strong>0.784</strong></td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p><strong>Inference Speed</strong>: 4.64 seconds per sample</p>

<p>The T5 baseline demonstrates that fine-tuning on domain-specific data yields strong performance, particularly for relationship extraction. However, the 4.6-second inference time and lack of flexibility make it less practical for production use cases requiring fast inference or cross-domain generalization. The high performance on the FACTUAL dataset is to be expected since this model was specifically trained using FACTUAL data. In the following experiments we only provide a handful of examples to the LLM during the prompt to achieve a similar level of performance.</p>

<hr />

<h2 id="experiment-2-langextract-proof-of-concept">Experiment 2: LangExtract Proof of Concept</h2>

<p>Validate whether LangExtract with Gemini can extract scene graphs with reasonable accuracy using few-shot learning (no fine-tuning required).</p>

<ul>
  <li><strong>Framework</strong>: Google LangExtract with Gemini 2.5 Flash</li>
  <li><strong>Test Set</strong>: 30 diverse samples (subset of Experiment 1)</li>
  <li><strong>Format</strong>: Flat Entities (separate classes for entity, attribute, relationship)</li>
  <li><strong>Examples</strong>: 5 few-shot examples demonstrating extraction patterns</li>
  <li><strong>Processing</strong>: Batch processing (all 30 samples in single API call)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1 Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Entities</strong></td>
      <td>0.956</td>
      <td>0.932</td>
      <td><strong>0.944</strong></td>
      <td>114</td>
    </tr>
    <tr>
      <td><strong>Attributes</strong></td>
      <td>0.899</td>
      <td>0.888</td>
      <td><strong>0.893</strong></td>
      <td>99</td>
    </tr>
    <tr>
      <td><strong>Relationships</strong></td>
      <td>0.237</td>
      <td>0.139</td>
      <td><strong>0.174</strong></td>
      <td>72</td>
    </tr>
    <tr>
      <td><strong>Macro F1</strong></td>
      <td>-</td>
      <td>-</td>
      <td><strong>0.670</strong></td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p><strong>Inference Speed</strong>: 0.045 seconds per sample (103x faster than T5)</p>

<p>LangExtract demonstrates the power of few-shot learning: with just 5 examples, it matches or exceeds T5’s performance on entities and attributes while being dramatically faster. However, relationship extraction is significantly weaker, suggesting the few-shot examples need improvement or the task requires more sophisticated prompting.</p>

<hr />

<h2 id="experiment-3-format-optimization">Experiment 3: Format Optimization</h2>

<p>Identify the optimal output format for LangExtract to maximize extraction accuracy.</p>

<ul>
  <li><strong>Framework</strong>: LangExtract + Gemini 2.5 Flash</li>
  <li><strong>Test Set</strong>: 50 diverse samples (subset of Experiment 1)</li>
  <li><strong>Formats Tested</strong>:
    <ol>
      <li><strong>Flat Entities</strong>: Separate classes (entity, attribute, relationship)</li>
      <li><strong>Tuple Format</strong>: Direct FACTUAL format <code class="language-plaintext highlighter-rouge">(subject, predicate, object)</code></li>
      <li><strong>Hierarchical</strong>: Nested objects with properties</li>
      <li><strong>JSON Structured</strong>: Clean JSON with entities/attributes/relationships arrays</li>
    </ol>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Flat Entities</td>
      <td>0.922</td>
      <td>0.865</td>
      <td>0.161</td>
      <td><strong>0.649</strong></td>
    </tr>
    <tr>
      <td>Tuple Format</td>
      <td>0.905</td>
      <td>0.851</td>
      <td>0.145</td>
      <td><strong>0.634</strong></td>
    </tr>
    <tr>
      <td>Hierarchical</td>
      <td>0.898</td>
      <td>0.842</td>
      <td>0.138</td>
      <td><strong>0.626</strong></td>
    </tr>
    <tr>
      <td><strong>JSON Structured</strong></td>
      <td><strong>0.928</strong></td>
      <td><strong>0.878</strong></td>
      <td><strong>0.173</strong></td>
      <td><strong>0.660</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Winner</strong>: JSON Structured (0.660 macro F1)</p>

<p>While JSON Structured emerges as the winner, the relatively small differences between formats (0.626-0.660) suggest that representation format is not the primary bottleneck. The consistent weakness in relationship extraction across all formats points to a deeper issue with the few-shot examples or prompting strategy.</p>

<hr />

<h2 id="experiment-4-backend-comparison-langextract-vs-native-gemini">Experiment 4: Backend Comparison (LangExtract vs Native Gemini)</h2>

<p>Compare LangExtract framework against native Gemini structured output using the winning JSON format.</p>

<ul>
  <li><strong>Approaches</strong>:
    <ol>
      <li><strong>LangExtract</strong>: Framework with 2 few-shot examples</li>
      <li><strong>Native Gemini</strong>: Direct API with <code class="language-plaintext highlighter-rouge">response_schema</code> + same 2 examples</li>
    </ol>
  </li>
  <li><strong>Test Set</strong>: 50 samples (same as Experiment 3)</li>
  <li><strong>Format</strong>: JSON Structured (winner from Experiment 3)</li>
</ul>

<p>After implementing centralized dataset loading to ensure all experiments use identical test data:</p>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
      <th>Speed (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>LangExtract</strong></td>
      <td><strong>0.944</strong></td>
      <td><strong>0.893</strong></td>
      <td>0.174</td>
      <td><strong>0.670</strong></td>
      <td><strong>0.045</strong></td>
    </tr>
    <tr>
      <td>Native Gemini</td>
      <td>0.784</td>
      <td>0.713</td>
      <td><strong>0.392</strong></td>
      <td>0.630</td>
      <td>3.762</td>
    </tr>
  </tbody>
</table>

<p>LangExtract’s framework optimizations for batch processing and few-shot learning make it more effective than raw Gemini API calls for entity and attribute extraction. However, Native Gemini’s structured schema enforcement provides better relationship extraction. The dramatic speed difference (83x) favors LangExtract for production use cases.</p>

<hr />

<h2 id="experiment-4b-targeted-improvement-with-analysis-driven-examples">Experiment 4b: Targeted Improvement with Analysis-Driven Examples</h2>

<p>Improve LangExtract’s relationship extraction by analyzing specific failures and creating targeted few-shot examples.</p>

<p><strong>Step 1: Failure Analysis</strong></p>
<ul>
  <li>Compared detailed results from LangExtract vs Native Gemini</li>
  <li>Identified specific relationships Native Gemini extracted correctly but LangExtract missed</li>
  <li>Found the root cause of failures</li>
</ul>

<p><strong>Key Discovery</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Analysis Results:
- Total failed relationships: 52
- "sit on" predicate failures: 48 (92%)
- "with" predicate failures: 4 (8%)

Root Cause: LangExtract was extracting "sitting on" instead of "sit on"
despite having instructions to use base verb forms.
</code></pre></div></div>

<p><strong>Step 2: Create Targeted Examples</strong>
Added 5 new examples specifically demonstrating “sitting on” → “sit on” normalization:</p>

<ol>
  <li>“A white teddy bear sitting on a green carpeted stair” → uses “sit on”</li>
  <li>“A small child sitting on a wooden chair” → uses “sit on”</li>
  <li>“A dog sitting beside a tree” → uses “beside” (not “sitting beside”)</li>
  <li>“People sitting on benches next to tables” → uses “sit on”</li>
  <li>“A man standing on a ladder” → uses “stand on” (same pattern)</li>
</ol>

<p><strong>Step 3: Re-run Evaluation</strong></p>
<ul>
  <li>Same 50 samples as Experiment 4</li>
  <li>Same LangExtract framework</li>
  <li>Now with 7 total examples (2 original + 5 targeted)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Improved LangExtract</strong></td>
      <td><strong>0.901</strong></td>
      <td><strong>0.900</strong></td>
      <td><strong>0.450</strong></td>
      <td><strong>0.750</strong></td>
    </tr>
    <tr>
      <td>Original LangExtract</td>
      <td>0.944</td>
      <td>0.893</td>
      <td>0.174</td>
      <td>0.670</td>
    </tr>
    <tr>
      <td>Native Gemini</td>
      <td>0.784</td>
      <td>0.713</td>
      <td>0.392</td>
      <td>0.630</td>
    </tr>
    <tr>
      <td>T5 Baseline</td>
      <td>0.907</td>
      <td>0.782</td>
      <td>0.662</td>
      <td>0.784</td>
    </tr>
  </tbody>
</table>

<p><strong>Improvement from Targeted Examples:</strong></p>
<ul>
  <li>Relationships: 0.174 → 0.450 (+159% improvement!)</li>
  <li>Macro F1: 0.670 → 0.750 (+12% improvement)</li>
</ul>

<p>This experiment demonstrates a powerful methodology for improving LLM performance:</p>
<ol>
  <li><strong>Analyze failures</strong>: Don’t just look at aggregate metrics</li>
  <li><strong>Identify patterns</strong>: Find common error modes (92% were “sit on” failures)</li>
  <li><strong>Create targeted examples</strong>: Address specific weaknesses with focused demonstrations</li>
  <li><strong>Iterate</strong>: Measure impact and repeat</li>
</ol>

<p>The result: Improved LangExtract now <strong>outperforms all other approaches</strong> including Native Gemini, while remaining extremely fast. It comes within 4.3% of T5’s performance (0.750 vs 0.784) despite using only 7 examples instead of full fine-tuning.</p>

<hr />

<h2 id="comprehensive-comparison">Comprehensive Comparison</h2>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Test Samples</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
      <th>Speed (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>T5 Baseline</strong></td>
      <td>100</td>
      <td>0.907</td>
      <td>0.782</td>
      <td><strong>0.662</strong></td>
      <td><strong>0.784</strong></td>
      <td>4.641</td>
    </tr>
    <tr>
      <td>LangExtract Original</td>
      <td>50</td>
      <td><strong>0.944</strong></td>
      <td><strong>0.893</strong></td>
      <td>0.174</td>
      <td>0.670</td>
      <td><strong>0.045</strong></td>
    </tr>
    <tr>
      <td>Native Gemini</td>
      <td>50</td>
      <td>0.784</td>
      <td>0.713</td>
      <td>0.392</td>
      <td>0.630</td>
      <td>3.762</td>
    </tr>
    <tr>
      <td><strong>LangExtract Improved</strong></td>
      <td>50</td>
      <td>0.901</td>
      <td><strong>0.900</strong></td>
      <td>0.450</td>
      <td><strong>0.750</strong></td>
      <td><strong>0.045</strong></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="conclusions-and-recommendations">Conclusions and Recommendations</h2>

<ol>
  <li><strong>Few-Shot Learning is Surprisingly Effective</strong>
    <ul>
      <li>With just 7 examples, LangExtract achieved 96% of T5’s performance (0.750 vs 0.784)</li>
      <li>No fine-tuning, no domain-specific training data required</li>
      <li>Dramatically faster inference (103x speedup)</li>
    </ul>
  </li>
  <li><strong>Analysis-Driven Few-Shot Engineering is Powerful</strong>
    <ul>
      <li>Analyzing specific failures revealed 92% of errors came from one pattern</li>
      <li>Creating 5 targeted examples improved relationship F1 by +159%</li>
      <li>This methodology can be repeated iteratively to close the gap with fine-tuned models</li>
    </ul>
  </li>
  <li><strong>Dataset Consistency is Critical</strong>
    <ul>
      <li>Initial experiment 4 results were misleading due to inconsistent test data</li>
      <li>Centralizing dataset loading (dataset_utils.py) ensured fair comparisons</li>
      <li>Lesson: Always use identical test sets when comparing approaches</li>
    </ul>
  </li>
  <li><strong>LLMs Excel at Different Tasks Than Fine-tuned Models</strong>
    <ul>
      <li><strong>LLMs better at</strong>: Entities (0.90+ F1), Attributes (0.90 F1)</li>
      <li><strong>Fine-tuned models better at</strong>: Relationships (0.66 vs 0.45 F1)</li>
      <li>This suggests relationships require more domain-specific knowledge</li>
    </ul>
  </li>
  <li><strong>Speed vs Accuracy Trade-offs</strong>
    <ul>
      <li>T5: Best accuracy, slowest (4.6s/sample)</li>
      <li>Improved LangExtract: Excellent accuracy, extremely fast (0.045s/sample)</li>
      <li>Native Gemini: Worst of both worlds (lower accuracy, slow)</li>
    </ul>
  </li>
</ol>

<h3 id="recommendations-by-use-case">Recommendations by Use Case</h3>

<h4 id="choose-t5-fine-tuned-model-if">Choose <strong>T5 Fine-tuned Model</strong> if:</h4>
<p>✅ You need the absolute best relationship extraction (0.662 F1)</p>

<p>✅ Inference speed is not a constraint</p>

<p>✅ You’re working with FACTUAL-style data</p>

<p>✅ You have computational resources for model loading</p>

<h4 id="choose-improved-langextract-if">Choose <strong>Improved LangExtract</strong> if:</h4>
<p>✅ You need fast inference (22 samples/second)</p>

<p>✅ You want excellent entity/attribute extraction (0.90 F1)</p>

<p>✅ You need flexibility to adapt to new domains (just change examples)</p>

<p>✅ You want good all-around performance without fine-tuning</p>

<p>✅ <strong>Recommended for most production use cases</strong></p>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>This series of experiments demonstrates that <strong>modern LLMs with few-shot learning can approach the performance of fine-tuned models</strong> while offering dramatic speed advantages and flexibility. The key insight: <strong>analysis-driven few-shot engineering</strong> is a powerful technique for iteratively improving LLM performance on structured extraction tasks.</p>

<p>For scene graph extraction specifically:</p>
<ul>
  <li><strong>Improved LangExtract achieved 96% of T5’s performance with 103x faster inference</strong></li>
  <li><strong>Adding just 5 targeted examples improved relationship extraction by +159%</strong></li>
  <li><strong>The gap between few-shot and fine-tuning continues to narrow</strong></li>
</ul>

<p>As LLMs continue to improve and few-shot learning techniques become more sophisticated, we expect the gap to close further. For practitioners, this means:</p>
<ul>
  <li><strong>Start with few-shot LLMs</strong> for their speed and flexibility</li>
  <li><strong>Use fine-tuned models</strong> only when you need the absolute best accuracy</li>
  <li><strong>Invest time in analysis-driven example engineering</strong> rather than collecting labeled data for fine-tuning</li>
</ul>

<p>The future of structured extraction is fast, flexible, and increasingly accurate—powered by few-shot learning with large language models.</p>

<hr />

<h2 id="appendix-reproduction">Appendix: Reproduction</h2>

<p>All experiments are reproducible using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install dependencies</span>
uv add FactualSceneGraph langextract anthropic datasets torch pandas numpy matplotlib seaborn python-dotenv scikit-learn

<span class="c"># Run experiments</span>
uv run experiment_1_t5_baseline.py
uv run experiment_2_langextract_poc_batched.py
uv run experiment_3_format_optimization_batched.py
uv run experiment_4_backend_comparison.py
uv run experiment_4b_improved_langextract.py

<span class="c"># Compare all results</span>
uv run compare_all_experiments.py
</code></pre></div></div>

<p>Dataset: <a href="https://huggingface.co/datasets/lizhuang144/FACTUAL_Scene_Graph">FACTUAL Scene Graph Dataset</a></p>

<p>Code: Available in this <a href="https://github.com/dlfelps/semantic-extraction">repository</a></p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="semantic" /><category term="scene-graph" /><category term="captioning" /><category term="langextract" /><category term="t5" /><category term="few-shot" /><category term="llm" /><category term="gemini" /><summary type="html"><![CDATA[The blog post demonstrates that few-shot LLMs can match 96% of fine-tuned model accuracy on scene graph extraction with 103x faster inference.]]></summary></entry><entry><title type="html">Experiment 23</title><link href="https://dlfelps.github.io/2025/10/01/devit.html" rel="alternate" type="text/html" title="Experiment 23" /><published>2025-10-01T00:00:00+00:00</published><updated>2025-10-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/10/01/devit</id><content type="html" xml:base="https://dlfelps.github.io/2025/10/01/devit.html"><![CDATA[<p>The blog post describes a fork of the DE-ViT algorithm that adapts it for few-shot object detection on satellite imagery by using DINOv3 vision transformers pretrained on the SAT-493M dataset, with a target application of detecting objects in the xView dataset.</p>

<h1 id="zero-shot-object-detection-for-overhead-imagery-a-de-vit-adaptation">Zero-Shot Object Detection for Overhead Imagery: A DE-ViT Adaptation</h1>

<h2 id="the-de-vit-approach">The DE-ViT Approach</h2>

<p>DE-ViT (Detection Transformer with Vision Transformers) represents a significant advancement in few-shot object detection, leveraging the powerful feature representations learned by vision transformers. The approach combines a pretrained vision transformer backbone with a region propagation network to enable detection with minimal training examples. The core innovation lies in how DE-ViT propagates information between support (example) images and query (target) images through a sophisticated attention mechanism, allowing the model to generalize to novel object categories with limited data.</p>

<p>The original DE-ViT architecture employs a subspace projection mechanism to align features from the vision transformer backbone, followed by region proposal generation and a region propagation network that refines detections based on support set prototypes. For a comprehensive understanding of the theoretical foundations, architectural details, and empirical validations, readers are directed to the <a href="https://arxiv.org/abs/2309.12969">original research paper</a>.</p>

<p><img src="/assets/images/main-rpropnet.jpg" alt="devit overview" />
<em>Overview of the DE-ViT method</em></p>

<h2 id="overhead-imagery-unique-challenges-and-the-xview-dataset">Overhead Imagery: Unique Challenges and the xView Dataset</h2>

<p>Overhead imagery presents distinct challenges that differentiate it from natural image domains. The most critical challenge is <strong>rotation invariance</strong>: objects in satellite and aerial imagery can appear at arbitrary orientations, unlike natural images where vehicles are typically upright and buildings follow gravity-aligned perspectives. A car photographed from above may appear at any angle from 0 to 360 degrees, and detection systems must recognize it regardless of orientation.</p>

<p>The <strong>xView dataset</strong> has become a cornerstone benchmark for evaluating object detection algorithms in overhead imagery. Released in 2018, xView contains over 1 million object instances across 60 categories in high-resolution satellite imagery, spanning diverse geographic regions and imaging conditions. The dataset was specifically designed to challenge computer vision systems with the complexities of overhead imagery: dense object clustering, extreme scale variation (objects ranging from small vehicles to large buildings), and the aforementioned rotation problem.</p>

<p>Historically, xView has driven innovation in several areas: anchor-free detection methods to handle arbitrary orientations, multi-scale feature pyramids to address the extreme scale variation, and attention mechanisms to manage dense scenes. However, most successful approaches have required extensive training on large labeled datasets, limiting their applicability in scenarios where labeled data is scarce or when adapting to new object categories.</p>

<p><img src="/assets/images/dataset-cover.jpeg" alt="sample xview image" />
<em>Example image from xView with detections</em></p>

<h2 id="novel-contributions-toward-zero-shot-detection">Novel Contributions: Toward Zero-Shot Detection</h2>

<p>This project introduces several key modifications to the original DE-ViT framework, fundamentally shifting from a few-shot to a <strong>zero-shot detection paradigm</strong> optimized for overhead imagery.</p>

<h3 id="1-removal-of-the-region-propagation-network">1. Removal of the Region Propagation Network</h3>

<p>The most significant architectural change is the <strong>elimination of the region propagation component</strong>. While the original DE-ViT uses this network to refine detections through learned interactions between support and query features, this requires training on a base dataset. By removing this component, we transition to a true zero-shot scenario where no training is required. Instead, the system relies entirely on the rich feature representations from the DINOv3 vision transformer, which has been pretrained on 493 million satellite images (SAT-493M dataset). This pretrained knowledge serves as the foundation for detection without any task-specific fine-tuning.</p>

<h3 id="2-differentiated-prototype-generation">2. Differentiated Prototype Generation</h3>

<p>The approach to <strong>prototype generation</strong> differs fundamentally between base and novel categories:</p>

<p><strong>Base Data Prototypes</strong>: For established object categories in the base dataset, prototypes are generated using standard feature extraction and pooling techniques. A large number of examples from each base class is sampled from the training data. Once the DINOv3 features are extracted and pooled, a the KMeans clustering algorithm is used to capture the centers that provide coverage for 75% of the examples. These centers become the prototypes for that base class.</p>

<p><strong>Novel Data Prototypes with Rotation Augmentation</strong>: For novel categories where rotation invariance cannot be assumed, the system employs a <strong>rotation augmentation strategy</strong>. For each image in the few-shot set (e.g. 5), 23 rotations (at 15 degree increments) are created. Next DINOv3 features are calculated and pooled from each of the rotated images. These serve as the novel class prototypes. NOTE: the original orientation of the object is not needed, but at the end of this process it we are guaranteed to have full 360 degree rotation examples.</p>

<h3 id="3-support-vector-machine-ensemble-for-confidence-scoring">3. Support Vector Machine Ensemble for Confidence Scoring</h3>

<p>The final novel component is the integration of a <strong>Support Vector Machine (SVM) ensemble</strong> as a confidence gating mechanism. First and SVM is trained to maximally separate the novel classes, base classes, and background. Given a new image, it is rotated using the same scheme from the novel data prototype yielding 24 total images covering all angles. Finally these 24 samples are classified by the SVM and assigned a confidence corresponding to the highest percentage of class agreement (e.g. if 12 of the 24 examples are assigned to background class then it is classified as background with 50% confidence).</p>

<p>This ensemble approach provides several benefits: it captures non-linear decision boundaries that simple cosine similarity cannot, it reduces false positives by requiring consensus across multiple orientations of the target, and it produces calibrated confidence scores that better reflect true match probability. The SVM ensemble acts as a final filter, ensuring that only high-confidence detections propagate to the final output.</p>

<h2 id="experiment">Experiment</h2>

<p>I did not perform a formal experiment, but here is a rough description of the anecdotal test:</p>
<ul>
  <li>2 prototype base classes that cover +70% of xView’s dataset (building &amp; bus)</li>
  <li>1 background base class</li>
  <li>1 novel class for testing (excavator)</li>
  <li>5 full view satellite images with known excavators present</li>
</ul>

<h2 id="results">Results</h2>

<p>The results were <em>dissapointing</em>. Despite the favorable results published in the <a href="https://arxiv.org/abs/2309.12969">paper</a>, the author admits on his <a href="https://github.com/mlzxy/devit/tree/main">Github site</a> that:</p>

<blockquote>
  <p>[DeVIT] tends to detect objects that do not belong to the prototypes, especially for retailed products that are not presented in the training data. For example, if you have “can”, “bottle”, and “toy” in the scene, but you only have “can” and “bottle” in the class prototypes. The ideal performance is to mark “toy” as background, but DE-ViT tends to detect “toy” as either “can” or “bottle”.</p>
</blockquote>

<p>I noticed a similar trend. While the approach is promising for tapping into the power of foundation vision models for few-shot learning, it fails to provide good performance at scale for satellite imagery. I didn’t bother calculating any formal statistics, but you can see for yourself the quality of the classifications below:</p>

<p><a href="/assets/images/31_detections.png"><img src="/assets/images/31_detections_thumb.jpg" alt="Click for full size" /></a></p>

<p><em>(Click to download full resolution) The green boxes are the novel detections (excavators) and blue boxes are classified as one of the base classes. NOTE: the few-shot adaptation did not affect the region proposal network, which was taken from a Faster RCNN model trained on xView, so the bounding box proposals themselves are not part of this evaluation. If you review the contents of the green boxes, however, you will find a low classification accuracy for the excavator class.</em></p>

<h2 id="conclusion">Conclusion</h2>

<p>This project demonstrates some potential improvements for zero-shot object detection in overhead imagery. Given the results, I cannot recommend the approach without additional changes.</p>

<p>Code for this project can be found at <a href="https://github.com/dlfelps/devit-xview">https://github.com/dlfelps/devit-xview</a>.</p>

<h2 id="claude-protips">CLAUDE PROTIPS</h2>

<p>In the last few blog posts I provided examples where CLAUDE needed extra guidance. I thought I would do something different in this post.</p>

<ol>
  <li>PROTIP #1: When working in complex code bases (such as a research level machine learning project), work in small measured steps.</li>
  <li>PROTIP #2: After using CLAUDE to implement a new feature (or in this case replace an existing module with something else), ask CLAUDE to write a simple script to demonstrate/test the new feature. Optional: treat this as a temporary test - you don’t have to add it to git.</li>
  <li>PROTIP #3: If there is a problem that “feels small” and I think I have a good idea how to fix it, I just give CLAUDE the error and CLAUDE will usually fix it.</li>
  <li>PROTIP #4: If there is a problem that “feels big” and I don’t understand it or know how to fix it, I ask CLAUDE to write a debugging script to log values up to the point at which the error occurs. Then I review the output. Next I ask CLAUDE to fix the problem and then run the debugging script again. If all goes well and values make sense I rerun the original code.</li>
</ol>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="few-shot" /><category term="dino" /><category term="devit" /><summary type="html"><![CDATA[The blog post describes a fork of the DE-ViT algorithm that adapts it for few-shot object detection on satellite imagery by using DINOv3 vision transformers pretrained on the SAT-493M dataset, with a target application of detecting objects in the xView dataset.]]></summary></entry><entry><title type="html">Experiment 22</title><link href="https://dlfelps.github.io/2025/09/01/cobad.html" rel="alternate" type="text/html" title="Experiment 22" /><published>2025-09-01T00:00:00+00:00</published><updated>2025-09-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/09/01/cobad</id><content type="html" xml:base="https://dlfelps.github.io/2025/09/01/cobad.html"><![CDATA[<p>The blog post introduces CoBAD (Collective Behavior Anomaly Detection), a deep learning approach that detects anomalous group behaviors in human mobility data by analyzing stay points and collective patterns rather than individual trajectories.</p>

<h2 id="introduction-beyond-individual-anomalies">Introduction: Beyond Individual Anomalies</h2>

<p>Traditional anomaly detection in human mobility has predominantly focused on identifying individual outliers—detecting when a single person deviates from their typical movement patterns. While effective for personal applications like location privacy or individual behavior analysis, this individual-centric approach misses a critical dimension of human behavior: <strong>collective dynamics</strong>.</p>

<h3 id="the-paradigm-shift-from-individual-to-collective">The Paradigm Shift: From Individual to Collective</h3>

<p>Individual anomaly detection, as established by foundational works like Hawkins (1980) <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> and more recently applied to mobility by Zheng et al. (2008) <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, typically examines single trajectories against learned normal patterns. These approaches excel at detecting when <em>one person</em> visits an unusual location or travels at an atypical time.</p>

<p>However, human behavior is inherently social. Consider these scenarios that individual anomaly detection would miss:</p>

<ul>
  <li><strong>Flash mob gatherings</strong>: Each individual trajectory might appear normal, but the collective convergence at a specific location and time is highly anomalous</li>
  <li><strong>Emergency evacuations</strong>: Individual movements may follow normal route patterns, but the simultaneous mass exodus represents a collective anomaly</li>
  <li><strong>Coordinated activities</strong>: Groups meeting at unusual times or locations that would be undetectable when analyzing individual patterns in isolation</li>
</ul>

<p>This limitation led researchers to explore <strong>collective anomaly detection</strong>—identifying when groups of entities exhibit anomalous behavior as a cohesive unit, even if individual behaviors appear normal.</p>

<h3 id="theoretical-foundations">Theoretical Foundations</h3>

<p>The concept of collective anomalies was formalized by Chandola et al. (2009) <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, who distinguished between:</p>

<ul>
  <li><strong>Point anomalies</strong>: Individual data points that deviate from normal patterns</li>
  <li><strong>Contextual anomalies</strong>: Data points that are anomalous in specific contexts but normal otherwise</li>
  <li><strong>Collective anomalies</strong>: Collections of data points that are anomalous when considered together</li>
</ul>

<p>In mobility analysis, collective anomaly detection has gained traction through works like:</p>

<ul>
  <li><strong>Ge et al. (2010)</strong> <sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>: Early work on detecting abnormal crowd behaviors in video surveillance</li>
  <li><strong>Liu et al. (2013)</strong> <sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>: Collective outlier detection in spatial data using density-based approaches</li>
  <li><strong>Araujo et al. (2014)</strong> <sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>: Group anomaly detection in social networks and mobility patterns</li>
</ul>

<p>However, existing collective anomaly detection methods face significant challenges when applied to large-scale human mobility data:</p>

<ol>
  <li><strong>Scalability</strong>: Many algorithms struggle with the volume and dimensionality of modern mobility datasets</li>
  <li><strong>Temporal dynamics</strong>: Most approaches fail to capture the evolving nature of collective behaviors over time</li>
  <li><strong>Spatial complexity</strong>: Urban mobility involves complex spatial relationships that simple clustering approaches miss</li>
  <li><strong>Ground truth</strong>: Validating collective anomalies is inherently more challenging than individual anomalies</li>
</ol>

<hr />

<h2 id="cobad-a-novel-approach-to-collective-behavior-anomaly-detection">CoBAD: A Novel Approach to Collective Behavior Anomaly Detection</h2>

<p><strong>CoBAD (Collective Behavior Anomaly Detection)</strong> addresses these challenges through a fundamentally different approach that focuses on <strong>stay point events</strong> and <strong>collective behavior modeling</strong> using deep learning techniques.</p>

<h3 id="core-innovation-stay-point-centric-analysis">Core Innovation: Stay Point-Centric Analysis</h3>

<p>Unlike traditional trajectory-based approaches that analyze continuous movement paths, CoBAD operates on <strong>stay points</strong>—locations where individuals or groups remain for significant durations. This paradigm shift offers several advantages:</p>

<ol>
  <li><strong>Semantic relevance</strong>: Stay points correspond to meaningful activities (meetings, events, gatherings)</li>
  <li><strong>Noise reduction</strong>: Filters out transitional movements that may not represent intentional collective behavior</li>
  <li><strong>Computational efficiency</strong>: Reduces data dimensionality while preserving behavioral significance</li>
  <li><strong>Collective focus</strong>: Natural aggregation point for identifying group behaviors</li>
</ol>

<h3 id="algorithm-architecture">Algorithm Architecture</h3>

<p>CoBAD employs a multi-stage pipeline that transforms raw trajectory data into collective behavior patterns:</p>

<h4 id="stage-1-stay-point-extraction">Stage 1: Stay Point Extraction</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Raw trajectory data → Stay point events
Features: (user_id, location, duration, temporal_context, weekend_flag)
</code></pre></div></div>

<p>The system converts GPS trajectories into discrete stay events, capturing not just <em>where</em> people are, but <em>how long</em> they stay and <em>when</em> these stays occur.</p>

<h4 id="stage-2-spatial-temporal-clustering">Stage 2: Spatial-Temporal Clustering</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Stay points → Collective behaviors via DBSCAN
Spatial proximity + Temporal alignment → Group identification
</code></pre></div></div>

<p>Using density-based clustering (DBSCAN), CoBAD identifies groups of people who are:</p>
<ul>
  <li><strong>Spatially co-located</strong> (within configurable distance threshold)</li>
  <li><strong>Temporally synchronized</strong> (occurring in the same time window)</li>
  <li><strong>Sufficiently numerous</strong> (meeting minimum collective size threshold)</li>
</ul>

<h4 id="stage-3-collective-feature-engineering">Stage 3: Collective Feature Engineering</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Group clusters → 8-dimensional feature vectors
Features: [time_window, center_x, center_y, spread_x, spread_y, 
          relative_size, weekend_ratio, avg_stay_duration]
</code></pre></div></div>

<p>Each collective behavior is characterized by an 8-dimensional feature vector capturing:</p>
<ul>
  <li><strong>Temporal context</strong>: When the collective behavior occurs</li>
  <li><strong>Spatial characteristics</strong>: Location and spatial spread of the group</li>
  <li><strong>Group dynamics</strong>: Size and composition metrics</li>
  <li><strong>Activity patterns</strong>: Duration and weekend/weekday distinctions</li>
</ul>

<h4 id="stage-4-paper-accurate-cobad-architecture">Stage 4: Paper-Accurate CoBAD Architecture</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Individual trajectories → Cross-time attention → Graph transformer → Anomaly scoring
Features: [norm_x, norm_y, is_weekend, start_time, duration, placeholder] (6D)
</code></pre></div></div>

<p>The paper-accurate CoBAD implementation employs a sophisticated neural architecture:</p>
<ol>
  <li><strong>Cross-time attention</strong>: Captures temporal dependencies within individual trajectory sequences</li>
  <li><strong>Graph transformer</strong>: Models collective behaviors through cross-people attention mechanisms</li>
  <li><strong>Dual reconstruction</strong>: Reconstructs both event attributes and co-occurrence link patterns</li>
  <li><strong>Masked pretraining</strong>: Uses BERT-style masking for robust pattern learning</li>
  <li><strong>Multi-component scoring</strong>: Combines learned patterns, event reconstruction, and link reconstruction errors</li>
</ol>

<h3 id="what-makes-cobad-unique">What Makes CoBAD Unique</h3>

<h4 id="1-multi-resolution-anomaly-analysis">1. <strong>Multi-Resolution Anomaly Analysis</strong></h4>
<p>The paper-accurate CoBAD provides detailed anomaly interpretation through <strong>eight trajectory-level subscore types</strong>:</p>

<ul>
  <li><strong>Spatial dispersion</strong>: Unusual location variance within trajectories</li>
  <li><strong>Temporal pattern</strong>: Rare timing patterns (weekend vs weekday, start times)</li>
  <li><strong>Duration anomaly</strong>: Atypical stay duration patterns</li>
  <li><strong>Mobility anomaly</strong>: Unusual movement patterns and distances</li>
  <li><strong>Length anomaly</strong>: Trajectory complexity outliers</li>
  <li><strong>Event reconstruction</strong>: Neural network event attribute reconstruction errors</li>
  <li><strong>Link reconstruction</strong>: Co-occurrence pattern reconstruction errors</li>
  <li><strong>Pattern learned</strong>: Learned anomaly patterns from the neural scorer</li>
</ul>

<p><img src="/assets/images/cobad_radar.jpeg" alt="Radar chart" />
<em>Subscore radar chart showing breakdown of top anomalies</em></p>

<h4 id="2-dynamic-threshold-adaptation">2. <strong>Dynamic Threshold Adaptation</strong></h4>
<p>CoBAD implements multiple robust threshold adjustment methods to handle dataset distribution shifts:</p>

<ul>
  <li><strong>Percentile-based</strong>: Maintains target anomaly rates (default 5%) across datasets</li>
  <li><strong>Statistical outlier</strong>: Mean + 2σ approach for statistical anomaly boundaries</li>
  <li><strong>IQR-based</strong>: Q3 + 1.5×IQR for robust outlier detection</li>
  <li><strong>Adaptive</strong>: Automatically uses median of all three methods and detects distribution shifts</li>
  <li><strong>Test-time adaptation</strong>: Recalculates thresholds when significant distribution shifts are detected</li>
</ul>

<h4 id="3-trajectory-level-pattern-analysis">3. <strong>Trajectory-Level Pattern Analysis</strong></h4>
<p>Each detected anomaly includes detailed pattern analysis explaining the anomalous behavior:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example trajectory anomaly breakdown:
🚨 RANK 1 - TRAJECTORY #4521:
📊 Anomaly Score: 42.3847
📏 Trajectory Length: 18 points
🗺️  Spatial Range: X[0.234, 0.891], Y[0.123, 0.567]  
📅 Weekend Activity: 85.2%
⏱️  Average Duration: 0.82

🔍 Sub-score Breakdown:
   spatial_dispersion  :  8.2341 (98.2nd percentile)
   temporal_pattern   :  6.7834 (94.1st percentile)
   mobility_anomaly    :  5.9123 (91.5th percentile)

🎯 Key anomaly drivers: Extreme spatial dispersion, High temporal pattern
</code></pre></div></div>

<p>This detailed analysis enables analysts to understand <em>why</em> specific trajectories are flagged as anomalous and what makes them unusual.</p>

<h4 id="4-embedding-space-analysis-and-clustering">4. <strong>Embedding Space Analysis and Clustering</strong></h4>
<p>The paper-accurate CoBAD performs sophisticated embedding space analysis:</p>
<ul>
  <li><strong>Dimensionality reduction</strong>: PCA and t-SNE visualization of trajectory embeddings</li>
  <li><strong>Trajectory clustering</strong>: K-means and DBSCAN clustering of learned embeddings</li>
  <li><strong>Cluster characterization</strong>: Analysis of anomaly rates and patterns per cluster</li>
  <li><strong>Cross-cluster anomaly analysis</strong>: Understanding how different trajectory types produce different anomaly patterns</li>
</ul>

<p><img src="/assets/images/cobad_tsne.jpeg" alt="t-SNE chart" />
<em>t-SNE visualization showing trajectory clusters colored by anomaly scores</em></p>

<h3 id="scalability-and-performance">Scalability and Performance</h3>

<p>The paper-accurate CoBAD is designed for large-scale trajectory datasets with CPU-friendly optimizations:</p>

<ul>
  <li><strong>Mini-batch processing</strong>: Configurable batch sizes (default 32) for memory-constrained systems</li>
  <li><strong>Lazy loading</strong>: Efficient data streaming without loading entire datasets into memory</li>
  <li><strong>Batched inference</strong>: Both training and inference use batched processing to prevent memory overflow</li>
  <li><strong>Dynamic batching</strong>: Automatic progress reporting and memory cleanup during processing</li>
  <li><strong>Checkpointing</strong>: Automatic model saving with best model tracking and early stopping</li>
  <li><strong>Validation splitting</strong>: Built-in train/validation splits with adaptive threshold setting</li>
</ul>

<hr />

<h2 id="case-study-anomaly-detection-in-urban-mobility">Case Study: Anomaly Detection in Urban Mobility</h2>

<p>To demonstrate CoBAD’s capabilities, we present results from analyzing the <strong>LYMob-4Cities dataset</strong> <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>—a comprehensive mobility dataset containing trajectory data from four Japanese metropolitan areas over a 75-day period.</p>

<h3 id="dataset-characteristics">Dataset Characteristics</h3>
<ul>
  <li><strong>Scale</strong>: 151,000+ individuals across 4 cities</li>
  <li><strong>Temporal coverage</strong>: 75 days with 30-minute intervals</li>
  <li><strong>Spatial resolution</strong>: 500m × 500m grid cells (200×200 grid)</li>
  <li><strong>Total records</strong>: 111+ million location records</li>
</ul>

<h3 id="key-findings">Key Findings</h3>

<p><img src="/assets/images/cobad_temporal.jpeg" alt="temporal chart" />
<em>Interactive dashboard showing temporal distribution of anomalies</em></p>

<h4 id="anomaly-detection-results">Anomaly Detection Results</h4>
<p>CoBAD successfully identified <strong>20 collective anomalies</strong> from <strong>393 collective behaviors</strong> (5.09% anomaly rate), revealing several interesting patterns:</p>

<p><strong>Top Anomaly Categories:</strong></p>
<ol>
  <li><strong>Duration anomalies</strong> (mean score: 2.11 ± 3.40): Groups with extremely long or short stay durations</li>
  <li><strong>Temporal anomalies</strong> (mean score: 0.94 ± 0.01): Collective activities at unusual hours</li>
  <li><strong>Spatial anomalies</strong> (mean score: 0.88 ± 0.19): Groups gathering in rare locations</li>
</ol>

<p><img src="/assets/images/cobad_spatial.jpeg" alt="spatial chart" />
<em>Plot showing spatial distribution of detected anomalies</em></p>

<h4 id="detailed-anomaly-analysis">Detailed Anomaly Analysis</h4>

<p><strong>Rank 1 Anomaly</strong> - Score: 29.27</p>
<ul>
  <li><strong>Location</strong>: (0.023, 0.550) - Peripheral urban area</li>
  <li><strong>Time</strong>: 5:00 AM - Off-peak collective activity</li>
  <li><strong>Duration</strong>: 0.84 time units - Extended stay duration</li>
  <li><strong>Primary contributors</strong>:
    <ul>
      <li>Stay duration (16.68) - Extremely long collective gathering</li>
      <li>Temporal pattern (11.71) - Very unusual timing</li>
      <li>Spatial spread (10.71) - Unusual spatial arrangement</li>
    </ul>
  </li>
</ul>

<p><strong>Rank 2 Anomaly</strong> - Score: 5.41</p>
<ul>
  <li><strong>Location</strong>: (0.458, 0.041) - Urban edge location</li>
  <li><strong>Time</strong>: 2:00 PM - Moderate temporal anomaly</li>
  <li><strong>Primary contributors</strong>:
    <ul>
      <li>Spatial location (6.75) - Rare meeting location</li>
      <li>Stay duration (6.59) - Unusual duration pattern</li>
      <li>Weekend activity (5.71) - Unexpected weekday pattern</li>
    </ul>
  </li>
</ul>

<h4 id="link-analysis-results">Link Analysis Results</h4>
<p>The network analysis revealed:</p>
<ul>
  <li><strong>8 anomaly communities</strong>: Distinct groups of related anomalous behaviors</li>
  <li><strong>30 connections</strong>: Links between similar anomalous patterns</li>
  <li><strong>Largest community</strong>: 6 related anomalies suggesting coordinated activities</li>
</ul>

<p><img src="/assets/images/cobad_network.jpeg" alt="cluster chart" />
<em>Network graph showing anomaly relationships and communities</em></p>

<h3 id="feature-attribution-insights">Feature Attribution Insights</h3>

<p>Across all detected anomalies, the most significant contributing features were:</p>
<ol>
  <li><strong>Average stay duration</strong> (2.36): Unusual activity lengths</li>
  <li><strong>Weekend ratio</strong> (3.17): Unexpected temporal patterns</li>
  <li><strong>Spatial coordinates</strong> (2.54-3.55): Rare location choices</li>
  <li><strong>Temporal windows</strong> (2.83): Off-peak collective activities</li>
</ol>

<p>These patterns suggest that anomalous collective behaviors are primarily characterized by <strong>temporal unusualness</strong> (when and how long) rather than just spatial rareness.</p>

<hr />

<h2 id="implementation-and-reproducibility">Implementation and Reproducibility</h2>

<p>CoBAD is implemented in Python using modern machine learning frameworks and is designed for reproducibility and extensibility:</p>

<h3 id="technical-stack">Technical Stack</h3>
<ul>
  <li><strong>PyTorch</strong>: Deep learning framework for transformer-based architecture</li>
  <li><strong>Scikit-learn</strong>: Dimensionality reduction, clustering, and preprocessing utilities</li>
  <li><strong>Plotly</strong>: Interactive visualization dashboards with trajectory analysis</li>
  <li><strong>NumPy/Pandas</strong>: Efficient data processing and manipulation</li>
  <li><strong>tqdm</strong>: Progress tracking for long-running trajectory analysis</li>
</ul>

<h3 id="key-outputs">Key Outputs</h3>
<p>The CoBAD analysis pipeline generates comprehensive outputs for further investigation:</p>

<h4 id="1-interactive-visualizations">1. Interactive Visualizations</h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">trajectory_overview_dashboard.html</code></strong>: 6-panel comprehensive analysis with score distributions, subscore correlations, temporal patterns, and spatial distributions</li>
  <li><strong><code class="language-plaintext highlighter-rouge">trajectory_embedding_analysis.html</code></strong>: t-SNE, PCA, and clustering visualizations of trajectory embeddings</li>
  <li>Real-time filtering and zooming capabilities</li>
  <li>Hover tooltips with detailed trajectory information</li>
</ul>

<h4 id="2-detailed-text-reports">2. Detailed Text Reports</h4>
<ul>
  <li>Console output with comprehensive anomaly reports including:
    <ul>
      <li>Top-N trajectory anomalies with detailed breakdowns</li>
      <li>Subscore analysis and percentile rankings</li>
      <li>Pattern analysis with human-readable explanations</li>
      <li>Cluster analysis with anomaly rates per cluster</li>
    </ul>
  </li>
</ul>

<h4 id="3-numerical-data-exports">3. Numerical Data Exports</h4>
<ul>
  <li><strong>Anomaly scores</strong>: <code class="language-plaintext highlighter-rouge">anomaly_scores.npy</code></li>
  <li><strong>Binary classifications</strong>: <code class="language-plaintext highlighter-rouge">anomaly_labels.npy</code></li>
  <li><strong>Pattern scores</strong>: <code class="language-plaintext highlighter-rouge">pattern_scores.npy</code></li>
  <li><strong>Reconstruction errors</strong>: <code class="language-plaintext highlighter-rouge">event_recon_errors.npy</code>, <code class="language-plaintext highlighter-rouge">link_recon_errors.npy</code></li>
  <li><strong>Embeddings</strong>: <code class="language-plaintext highlighter-rouge">embeddings.npy</code></li>
  <li><strong>Subscore data</strong>: <code class="language-plaintext highlighter-rouge">subscore_*.npy</code> files for each subscore type</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>CoBAD represents a significant advancement in collective anomaly detection for human mobility data. By shifting focus from individual trajectories to collective behaviors and employing deep learning techniques for pattern recognition, CoBAD enables the detection of complex group anomalies that traditional methods miss.</p>

<p>The approach’s strength lies in its comprehensive analysis framework—providing not just binary anomaly classifications, but detailed explanations, feature attributions, and relationship networks that enable deeper understanding of anomalous collective behaviors.</p>

<p>As urban populations grow and mobility data becomes increasingly available, tools like CoBAD become essential for understanding and responding to collective human behavior patterns in smart city environments.</p>

<hr />

<p><em>Want to explore the code? Check out the <a href="https://github.com/dlfelps/cobad">GitHub repository</a> for complete implementation details, documentation, and examples.</em></p>

<h2 id="claude-gotchas">Claude Gotchas</h2>

<ol>
  <li>ISSUE - Claude initially implemented an autoencoder-based version of the CoBAD algorithm. Although it trained, and produced results, it did not implement the approach as described in the paper. <strong>RESOLUTION - I described the correct implementation using text from the CoBAD paper. Claude then made a faithful reimplementation of the attention-based model.</strong></li>
  <li>ISSUE - Claude produced an optimized, but incorrect version of my data-loader. I implemented the original data-loader manually, however when I scaled to +1M point CSV files my implementation was too slow. I asked Claude to optimize the loading function for performance. It produced a fast loading function, but it did not produce equivalent output to my function. <strong>RESOLUTION -  I manually inspected the output of the new loading function and noticed that the output was incorrect. I then asked Claude to produce a unit-test to verify the equivalence of the new function with my function. It did and then when it failed the test it fixed the optimized function.</strong></li>
  <li>ISSUE - Claude suggested a naive way to split the training/test data that was not optimal for the approach. When questioned it was too agreeable. <strong>RESOLUTION - I asked Claude about sorting the data before performing a split. It suggested the best way was to sort by time. I replied that that wouldn’t test the approach against unknown users and that withholding never-before seen users would produce a stronger test case. It agreed and implemented the latter.</strong></li>
  <li>ISSUE - Claude littered the code with import statements throughout the code. <strong>RESOLUTION - It is preferred style to have all import statements at the top of the file. Although it works to import code in the middle of a function it looks quite messy and it also imports the code each time the function is called. I asked Claude to move all import statements to the top and it did.</strong></li>
</ol>

<h2 id="future-directions-and-applications">Future Directions and Applications</h2>

<h3 id="potential-applications">Potential Applications</h3>

<p><strong>Urban Planning</strong>: Identifying unusual gathering patterns that may indicate:</p>
<ul>
  <li>Infrastructure bottlenecks requiring attention</li>
  <li>Emergency situation responses</li>
  <li>Public event impact assessment</li>
</ul>

<p><strong>Public Safety</strong>: Detecting coordinated activities that warrant investigation:</p>
<ul>
  <li>Unusual crowd formations</li>
  <li>Synchronized movement patterns</li>
  <li>Off-hours collective activities</li>
</ul>

<p><strong>Transportation Analysis</strong>: Understanding collective mobility patterns:</p>
<ul>
  <li>Mass transit disruption impacts</li>
  <li>Event-driven mobility changes</li>
  <li>Seasonal or periodic collective behavior variations</li>
</ul>

<h3 id="technical-enhancements">Technical Enhancements</h3>

<p><strong>Streaming Anomaly Detection</strong>: Extending CoBAD for real-time analysis of incoming mobility streams with concept drift adaptation.</p>

<p><strong>Multi-Modal Integration</strong>: Incorporating additional data sources (social media, weather, events) to improve context understanding.</p>

<p><strong>Hierarchical Anomaly Detection</strong>: Developing multi-scale approaches that detect anomalies at different spatial and temporal resolutions simultaneously.</p>

<p><strong>Causal Anomaly Analysis</strong>: Moving beyond correlation to understand causal relationships between anomalous collective behaviors and external factors.</p>

<h2 id="references">References</h2>

<hr />

<p><em>This research contributes to the growing field of collective behavior analysis in urban computing and demonstrates the potential of deep learning approaches for understanding complex mobility patterns in smart city applications.</em></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Hawkins, D. M. (1980). <em>Identification of Outliers</em>. Chapman and Hall. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Zheng, Y., Zhang, L., Xie, X., &amp; Ma, W. Y. (2009). Mining interesting locations and travel sequences from GPS trajectories. In <em>Proceedings of the 18th international conference on World wide web</em> (pp. 791-800). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Chandola, V., Banerjee, A., &amp; Kumar, V. (2009). Anomaly detection: A survey. <em>ACM computing surveys</em>, 41(3), 1-58. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Ge, W., Collins, R. T., &amp; Ruback, R. B. (2012). Vision-based analysis of small groups in pedestrian crowds. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 34(5), 1003-1016. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008). Isolation forest. In <em>2008 eighth ieee international conference on data mining</em> (pp. 413-422). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Araujo, M., Günnemann, S., Mateos, G., &amp; Christakis, N. A. (2014). Discrete signal processing on graphs: Frequency analysis. <em>IEEE transactions on signal processing</em>, 62(12), 3042-3054. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>LYMob-4Cities Dataset. (2024). Available at: https://zenodo.org/records/14219563 <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="gps" /><category term="mobility" /><category term="cobad" /><summary type="html"><![CDATA[The blog post introduces CoBAD (Collective Behavior Anomaly Detection), a deep learning approach that detects anomalous group behaviors in human mobility data by analyzing stay points and collective patterns rather than individual trajectories.]]></summary></entry><entry><title type="html">Experiment 21</title><link href="https://dlfelps.github.io/2025/08/21/mobility.html" rel="alternate" type="text/html" title="Experiment 21" /><published>2025-08-21T00:00:00+00:00</published><updated>2025-08-21T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/08/21/mobility</id><content type="html" xml:base="https://dlfelps.github.io/2025/08/21/mobility.html"><![CDATA[<p>This post converts GPS trajectory data into learned embedded features to reveal human mobility patterns, enabling applications in urban planning, personalized services, and behavioral analysis.</p>

<h1 id="introduction">Introduction</h1>

<p>Human mobility patterns tell fascinating stories. Where we go, when we travel, and how we move through space reveals insights about urban planning, social behavior, economic activity, and even public health. But extracting meaningful patterns from raw GPS trajectories is challenging—until now.</p>

<p>In this post, I’ll walk you through an exciting deep learning project that transforms complex mobility data into intelligent embeddings using a hybrid transformer-LSTM architecture. We’ll explore how this approach can revolutionize everything from urban planning to personalized location services.</p>

<h2 id="the-challenge-making-sense-of-mobility-data">The Challenge: Making Sense of Mobility Data</h2>

<p>GPS trajectories are deceptively complex. A single user’s movement data contains:</p>
<ul>
  <li><strong>Spatial information</strong>: Latitude and longitude coordinates</li>
  <li><strong>Temporal patterns</strong>: When movements occur, day-of-week effects</li>
  <li><strong>Sequential dependencies</strong>: How current location influences future destinations</li>
  <li><strong>Individual behaviors</strong>: Personal routines and preferences</li>
</ul>

<p>Traditional analysis methods struggle to capture these multi-dimensional relationships. That’s where deep learning comes in.</p>

<h2 id="introducing-lymob-4cities-a-rich-mobility-dataset">Introducing LYMob-4Cities: A Rich Mobility Dataset</h2>

<p>Our project leverages the <strong>LYMob-4Cities dataset</strong>, a comprehensive collection of anonymized mobility traces from four major cities. This dataset provides:</p>

<ul>
  <li><strong>Multi-city coverage</strong>: Diverse urban environments with different characteristics</li>
  <li><strong>Temporal depth</strong>: Extended observation periods capturing seasonal and weekly patterns</li>
  <li><strong>User diversity</strong>: Thousands of anonymized users with varying mobility behaviors</li>
  <li><strong>High resolution</strong>: Detailed GPS coordinates with timestamp information</li>
</ul>

<p>The dataset structure includes:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">uid</code>: Anonymized user identifier</li>
  <li><code class="language-plaintext highlighter-rouge">d</code>: Day number in the observation period</li>
  <li>Geographic coordinates (latitude/longitude)</li>
  <li>Temporal metadata including timestamps and day-of-week indicators</li>
</ul>

<p>This rich dataset enables us to build models that generalize across different urban contexts while respecting user privacy through anonymization.</p>

<h2 id="the-architecture-hybrid-transformer-lstm-for-mobility">The Architecture: Hybrid Transformer-LSTM for Mobility</h2>

<p>Our approach combines the best of both worlds: transformers for attention-based pattern recognition and LSTMs for sequential modeling.</p>

<h3 id="core-components">Core Components</h3>

<p><strong>1. Multi-Modal Embeddings</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Spatial embedding: lat/lon → dense vector
</span><span class="n">spatial_emb</span> <span class="o">=</span> <span class="nc">SpatialEmbedding</span><span class="p">(</span><span class="n">coordinates</span><span class="p">)</span>

<span class="c1"># Temporal embedding: time features → dense vector  
</span><span class="n">temporal_emb</span> <span class="o">=</span> <span class="nc">TemporalEmbedding</span><span class="p">(</span><span class="n">timestamps</span><span class="p">,</span> <span class="n">weekend_flags</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>2. Cross-Attention Mechanism</strong>
The model learns how spatial and temporal features interact through dedicated attention layers, capturing patterns like “this user typically visits restaurants on Friday evenings.”</p>

<p><strong>3. Transformer Blocks</strong>
Multi-head self-attention processes the entire trajectory sequence simultaneously, identifying important locations and movement patterns.</p>

<p><strong>4. Bidirectional LSTM</strong>
Sequential processing captures temporal dependencies and generates the final 512-dimensional embedding that summarizes the entire trajectory.</p>

<h3 id="why-this-architecture-works">Why This Architecture Works</h3>

<ul>
  <li><strong>Transformers</strong> excel at identifying important locations through attention</li>
  <li><strong>LSTMs</strong> capture sequential dependencies in movement</li>
  <li><strong>Cross-attention</strong> learns spatio-temporal interactions</li>
  <li><strong>Bidirectional processing</strong> considers both past and future context</li>
</ul>

<h2 id="the-complete-pipeline">The Complete Pipeline</h2>

<h3 id="stage-1-model-training">Stage 1: Model Training</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train_s2s.py
</code></pre></div></div>

<p>The training process:</p>
<ul>
  <li>Loads mobility data from multiple cities</li>
  <li>Applies data augmentation (Gaussian noise) for robustness</li>
  <li>Trains the hybrid model with early stopping</li>
  <li>Saves the best checkpoint for inference</li>
</ul>

<p><strong>Key Innovation</strong>: The model uses only users with ID &lt; 4000 for training, reserving higher IDs for testing—ensuring true out-of-sample evaluation.</p>

<h3 id="stage-2-embedding-generation">Stage 2: Embedding Generation</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python inference.py
</code></pre></div></div>

<p>This stage:</p>
<ul>
  <li>Processes test users to generate trajectory embeddings</li>
  <li>Applies PCA to reduce dimensionality for visualization</li>
  <li>Creates analysis outputs including scatter plots and statistical summaries</li>
</ul>

<p><strong>Technical Detail</strong>: The 512-dimensional embeddings come from a bidirectional LSTM with 256 hidden units, effectively doubling the representation capacity.</p>

<h3 id="stage-3-interactive-exploration">Stage 3: Interactive Exploration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python interactive_plot.py
</code></pre></div></div>

<p>The crown jewel: a Dash web application that enables:</p>
<ul>
  <li>Interactive exploration of embedding space</li>
  <li>Point selection and trajectory heatmap generation</li>
  <li>Real-time analysis of mobility patterns</li>
</ul>

<p><img src="/assets/images/Screenshot_20-8-2025_15056_127.0.0.1.jpeg" alt="Interactive Dashboard" />
<em>Interactive dashboard showing PCA projections (left) and trajectory heatmaps (right)</em></p>

<h2 id="remarkable-results-what-the-embeddings-reveal">Remarkable Results: What the Embeddings Reveal</h2>

<p>The PCA analysis reveals fascinating insights:</p>

<p><strong>Clustering by Behavior</strong>: Users with similar mobility patterns cluster together in embedding space, even across different cities.</p>

<p><strong>Spatial Preservation</strong>: Geographic relationships are maintained—users who frequent similar areas have similar embeddings.</p>

<p><strong>Temporal Patterns</strong>: Weekend vs. weekday behaviors create distinct embedding regions.</p>

<p><strong>Generalization</strong>: The model successfully captures mobility patterns for completely unseen users.</p>

<h2 id="real-world-applications">Real-World Applications</h2>

<p>This technology opens doors to numerous applications:</p>

<h3 id="️-urban-planning">🏙️ <strong>Urban Planning</strong></h3>
<ul>
  <li><strong>Traffic optimization</strong>: Predict congestion patterns before infrastructure changes</li>
  <li><strong>Public transit</strong>: Design routes based on actual movement patterns</li>
  <li><strong>Zoning decisions</strong>: Understand how people actually use urban spaces</li>
</ul>

<h3 id="-personalized-services">📱 <strong>Personalized Services</strong></h3>
<ul>
  <li><strong>Location recommendations</strong>: “Users with similar patterns also enjoyed…”</li>
  <li><strong>Predictive routing</strong>: Anticipate destinations based on historical behavior</li>
  <li><strong>Context-aware apps</strong>: Adapt interfaces based on movement patterns</li>
</ul>

<h3 id="-anomaly-detection">🚨 <strong>Anomaly Detection</strong></h3>
<ul>
  <li><strong>Security applications</strong>: Identify unusual movement patterns</li>
  <li><strong>Public health</strong>: Track population mobility during emergencies</li>
  <li><strong>Fraud detection</strong>: Verify location-based claims</li>
</ul>

<h3 id="-business-intelligence">📊 <strong>Business Intelligence</strong></h3>
<ul>
  <li><strong>Retail analytics</strong>: Understand customer foot traffic patterns</li>
  <li><strong>Real estate</strong>: Evaluate locations based on mobility data</li>
  <li><strong>Marketing</strong>: Target audiences based on movement behaviors</li>
</ul>

<h3 id="-research-applications">🔬 <strong>Research Applications</strong></h3>
<ul>
  <li><strong>Social science</strong>: Study human behavior at scale</li>
  <li><strong>Transportation research</strong>: Validate mobility models</li>
  <li><strong>Economic analysis</strong>: Understand commuting and economic patterns</li>
</ul>

<h2 id="privacy-and-ethics-a-responsible-approach">Privacy and Ethics: A Responsible Approach</h2>

<p>Our implementation prioritizes privacy:</p>
<ul>
  <li><strong>Anonymized data</strong>: No personally identifiable information</li>
  <li><strong>Aggregated insights</strong>: Focus on population-level patterns</li>
  <li><strong>Secure processing</strong>: Local analysis without data sharing</li>
  <li><strong>Transparent methods</strong>: Open-source implementation for scrutiny</li>
</ul>

<h2 id="technical-innovation-why-this-matters">Technical Innovation: Why This Matters</h2>

<p>This project demonstrates several key advances:</p>

<p><strong>1. Multi-Modal Learning</strong>: Successfully combining spatial and temporal information in a unified architecture.</p>

<p><strong>2. Scalable Embeddings</strong>: 512-dimensional representations that capture complex mobility patterns while remaining computationally tractable.</p>

<p><strong>3. Interactive Analysis</strong>: Real-time exploration tools that make complex AI accessible to domain experts.</p>

<p><strong>4. Generalization</strong>: Models that work across different cities and user populations.</p>

<h2 id="getting-started-try-it-yourself">Getting Started: Try It Yourself</h2>

<p>The complete implementation is available with detailed documentation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install dependencies</span>
pip <span class="nb">install </span>torch pandas scikit-learn plotly dash

<span class="c"># Train the model</span>
python train_s2s.py

<span class="c"># Generate embeddings</span>
python inference.py

<span class="c"># Launch interactive dashboard</span>
python interactive_plot.py
</code></pre></div></div>

<p>The modular design makes it easy to:</p>
<ul>
  <li>Adapt to your own mobility datasets</li>
  <li>Experiment with different architectures</li>
  <li>Add new visualization components</li>
  <li>Integrate with existing systems</li>
</ul>

<h2 id="the-future-of-mobility-intelligence">The Future of Mobility Intelligence</h2>

<p>This project represents just the beginning. Future directions include:</p>

<ul>
  <li><strong>Federated learning</strong>: Training on distributed datasets while preserving privacy</li>
  <li><strong>Multi-modal integration</strong>: Incorporating weather, events, and social data</li>
  <li><strong>Real-time processing</strong>: Streaming analysis for live mobility insights</li>
  <li><strong>Causal modeling</strong>: Understanding why people move, not just where</li>
</ul>

<h2 id="conclusion-from-data-to-intelligence">Conclusion: From Data to Intelligence</h2>

<p>Human mobility data contains incredible insights waiting to be unlocked. By combining modern deep learning architectures with thoughtful data science, we can transform raw GPS coordinates into intelligent representations that reveal the hidden patterns of human behavior.</p>

<p>The implications extend far beyond technology—this work can inform better cities, more personalized services, and deeper understanding of human society. As we continue to generate more location data through our digital lives, tools like these become essential for making sense of our increasingly connected world.</p>

<p>Whether you’re an urban planner designing the cities of tomorrow, a data scientist exploring new applications, or simply curious about the intersection of AI and human behavior, this project demonstrates the power of applying modern machine learning to one of humanity’s most fundamental activities: movement through space and time.</p>

<hr />

<p><em>Want to explore the code? Check out the <a href="https://github.com/dlfelps/lstm-trajectory">GitHub repository</a> for complete implementation details, documentation, and examples.</em></p>

<h2 id="about-the-dataset">About the Dataset</h2>

<p>The LYMob-4Cities dataset used in this project represents a significant contribution to mobility research, providing researchers with access to large-scale, anonymized mobility data from diverse urban environments. This type of dataset is crucial for developing and validating mobility models that can generalize across different cities and cultures.</p>

<p>For researchers interested in working with similar data, consider exploring:</p>
<ul>
  <li>Academic partnerships with transportation authorities</li>
  <li>Open mobility datasets from cities with data sharing initiatives</li>
  <li>Synthetic mobility data generation techniques</li>
  <li>Privacy-preserving data collection methods</li>
</ul>

<p>The responsible use of such datasets, with proper anonymization and ethical oversight, enables breakthrough research while protecting individual privacy—a balance that’s essential as we advance mobility intelligence.</p>

<h2 id="claude-gotchas">Claude Gotchas</h2>

<p>I was impressed by Claude Code’s ability to assist and expedite with the exploration of this dataset and creation of the LSTM model. I wanted to document a few gotchas that I came across during the development process that would have been difficult for a non-developer / non-ML person to identify since they didn’t cause any errors:</p>

<ol>
  <li>ISSUE - The model was actually predicting the final location from the full trajectory (i.e. including the final location). <strong>RESOLUTION - I identified the problem by when I noticed the model converged quickly and had excellent performance on test datasets. I found the problem, told Claude, and Claude fixed it.</strong></li>
  <li>ISSUE - The LSTM used bad values for padding points. <strong>RESOLUTION - Trajectories are not the same length and the LSTM model requires that you pad short sequences to make them equal. One is supposed to choose padding values that do not appear in the real data. However, Claude chose “0” for user_id, x, y, and time. These are all valid values. I manually changed them to acceptable values.</strong></li>
  <li>ISSUE - The model initially treated the user_id variable as a continuous variable instead of a categorical variable. <strong>RESOLUTION - When I pointed out the mistake Claude had a good suggestion to use an embedding layer and updated the code accordingly.</strong></li>
  <li>ISSUE - Claude chose the wrong normalization algorithm. <strong>RESOLUTION - I noticed that the first two PCA dimensions captured almost 97% of the variance. This was surprising given the complexity of the data. Upon further inspection, I noticed that before performing PCA Claude applied a matrix L2 norm to the trajectory embeddings. The correct normalization method is a zero-mean, unit-variance normalization method. I was able to tell Claude to replace the method.</strong></li>
</ol>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="gps" /><category term="mobility" /><summary type="html"><![CDATA[This post converts GPS trajectory data into learned embedded features to reveal human mobility patterns, enabling applications in urban planning, personalized services, and behavioral analysis.]]></summary></entry><entry><title type="html">Experiment 20</title><link href="https://dlfelps.github.io/2024/12/01/scraper.html" rel="alternate" type="text/html" title="Experiment 20" /><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/12/01/scraper</id><content type="html" xml:base="https://dlfelps.github.io/2024/12/01/scraper.html"><![CDATA[<p>This post describes a command line tool I created that scrapes a website and generates reports.</p>

<h1 id="introduction">Introduction</h1>

<p>It is greatly rewarding to build a tool that solves a real problem. The scale of the problem dictates the number of potential users your tool can help. This project solves a small problem - improving the registration process for my son’s summer camps. The problem is mostly created by a poor website design that doesn’t allow parents to easily find the correct camps.</p>

<p>I built this tool to scrape the camp data on <a href="https://fairfax.usedirect.com/FairfaxFCPAWeb/Default.aspx">Fairfax County Park Authority</a> (FCPA) website and generate useful camp options. I also added new functionality like calculating the estimated commute times to each camp.</p>

<h2 id="goals">Goals</h2>
<p>The goals of this project are to:</p>
<ul>
  <li>Improve user experience registering for camps hosted by FCPA</li>
  <li>Demonstrate the use of developing a command line tool</li>
  <li>Demonstrate the 3rd party API (Google MAP API)</li>
  <li>Demonstrate report creation</li>
</ul>

<h2 id="fcpa-site-limitations">FCPA site limitations</h2>
<p>Why is this tool needed? Because FCPA offers hundreds of potential camps to register for and their filter options are rather limited. Here is a screenshot of the FCPA camp search site:</p>

<p><img src="/assets/images/fcpa_search.jpeg" alt="" /></p>

<p>At first glance the fields look reasonable. We should be able to narrow down the list of available camps and register for one. Let’s see what happens if I try to find a camp for my 6 year old son for the two days after New Years Day (Jan 2-3).</p>

<p><img src="/assets/images/fcpa_results.jpeg" alt="" /></p>

<p>Due to the broad age range filter (6-12 yrs old), only one of the first six camps listed in the results are appropriate for a 6 year old. Unfortunately that camp is for the wrong dates (January 20). This happens because the date filter only allows you to specify the date “starting on or after”. So the best I can do is limit results from a starting date to the end of that month. This is especially frustrating when trying to book camps over the summer break. Typically I want to book one camp per week for the entire summer. To do this on the FCPA website I can change the “starting on or after” date, but for the first week in a month I get all camps in that month (which could be more than 100 sessions!).</p>

<h2 id="desired-capabilities">Desired capabilities</h2>

<p>I would like to build a tool that can output a tailored report to facilitate selecting and registring for camps. Ideally I would be able to quickly update the data based on the current openings as classes can fill up quickly. So I need a way to:</p>
<ol>
  <li>Automatically scrape the camp data from the FCPA website</li>
  <li>Select camps by various criteria</li>
  <li>Generate a custom report</li>
</ol>

<h1 id="installation">Installation</h1>
<ol>
  <li>Clone the repo (git clone https://github.com/dlfelps/camp-little-scrape.git)</li>
  <li>Install dependencies (pip install -r requirements.txt)</li>
  <li>Install Chrome (https://www.google.com/chrome/)</li>
  <li>Sign up for a Google Maps API key (https://developers.google.com/maps)</li>
</ol>

<h1 id="usage">Usage</h1>

<h2 id="process-overview">Process overview</h2>

<p>The overall process is described in the flowchart below. I will demonstrate each step below in detail, but at a high level, the tool completes a series of steps to:</p>
<ol>
  <li>Scrape camp links</li>
  <li>Scrape camp details</li>
  <li>(optional) Calculate commute times to each camp</li>
  <li>Filter camps and generate report</li>
</ol>

<p><img src="/assets/images/camp%20scraper-2024-11-21-162211.svg" alt="" /></p>

<h2 id="step-1---scrape-camp-links">Step 1 - Scrape camp links</h2>
<p>The first step in the process uses <a href="https://selenium-python.readthedocs.io/">Selenium</a> to semi-automatically scrape the camp session links from the FCPA website.</p>
<ol>
  <li>launch jupyter lab</li>
  <li>open step1.ipynb</li>
  <li>Follow instructions in step1.ipynb to open remote Chrome browser</li>
  <li>Go to FCPA website and search for ALL camps</li>
  <li>Follow instructions in step1.ipynb to scrape each page</li>
</ol>

<p>This step produces a <code class="language-plaintext highlighter-rouge">links.txt</code> file containing all the camp session links.</p>

<h2 id="step-2---scrape-camp-details">Step 2 - Scrape camp details</h2>
<p>Steps 2-4 all use the command line gui provided by the camp.py file. It is built using <a href="https://typer.tiangolo.com/">Typer</a>. View the list of possible commands by calling <code class="language-plaintext highlighter-rouge">python camp.py --help</code></p>

<p><img src="/assets/images/gui_help.PNG" alt="" /></p>

<p>Step 2 is initiated by calling <code class="language-plaintext highlighter-rouge">python camp.py details</code>. This step scrapes the camp details (e.g. location, dates, times) for each camp found in the <code class="language-plaintext highlighter-rouge">links.txt</code>.</p>

<h2 id="step-3---optional-calculate-commutes">Step 3 - (optional) Calculate commutes</h2>

<p>The third step uses Google Maps API to calculate the typical daily commute to each camp. This calculation is the sum of the following distances between home (user specified) and camp location:</p>
<ul>
  <li>morning dropoff (arriving by 9 AM)</li>
  <li>morning return (departing at 9 AM)</li>
  <li>afternoon pickup (arriving by 4 PM)</li>
  <li>afternooon return (departing at 4 PM)</li>
</ul>

<p>Commute time is one of the most important factors when selecting camps. Since full day camps are only 7 hours, it would not be time efficient to spend 2 hours in the car, which is shockingly easy to do even staying within Fairfax county (where all FCPA camps are located). This step does require a Google Map API key, which can be created at https://developers.google.com/maps. The code retrieves your individial key from the environment variable <code class="language-plaintext highlighter-rouge">MAP_API_KEY</code>. Here is a <a href="https://lazyprogrammer.me/how-to-set-environment-variables-permanently-in-windows-linux-and-mac/">guide</a> if you need help.</p>

<h2 id="step-4---filter-camps-and-generate-report">Step 4 - Filter camps and generate report</h2>

<p>The final step filters the available camps to generate a report. We can see the full list of available options by looking at the report help command <code class="language-plaintext highlighter-rouge">python camp.py report --help</code>.</p>

<p><img src="/assets/images/gui_report_help.PNG" alt="" /></p>

<p>The filtering options for this step are as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Required?</th>
      <th style="text-align: center">Default value</th>
      <th style="text-align: center">Description</th>
      <th style="text-align: center">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">first_day</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">The camp must start on this date</td>
      <td style="text-align: center">“01/02/2025”</td>
    </tr>
    <tr>
      <td style="text-align: center">last_day</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">The camp must end on this date</td>
      <td style="text-align: center">“01/03/2025”</td>
    </tr>
    <tr>
      <td style="text-align: center">min_age</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">5 (years)</td>
      <td style="text-align: center">The camp must accept kids of this age</td>
      <td style="text-align: center">–min-age=5</td>
    </tr>
    <tr>
      <td style="text-align: center">max_commute</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">120 (minutes)</td>
      <td style="text-align: center">The camp must not have an estimated daily commute greater than this many minutes</td>
      <td style="text-align: center">–max-commute=120</td>
    </tr>
    <tr>
      <td style="text-align: center">remove_half_day</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">True</td>
      <td style="text-align: center">Half day camps (from 9-noon) are not shown</td>
      <td style="text-align: center">–remove-half-day</td>
    </tr>
    <tr>
      <td style="text-align: center">remove_schools</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">True</td>
      <td style="text-align: center">Camps located at schools are not shown</td>
      <td style="text-align: center">–remove-schools</td>
    </tr>
    <tr>
      <td style="text-align: center">show_full</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">False</td>
      <td style="text-align: center">Camps that are already fully registered are not shown</td>
      <td style="text-align: center">–no-show-full</td>
    </tr>
  </tbody>
</table>

<p>If your desired parameters are different from default, then change them when you call the report command, e.g. <code class="language-plaintext highlighter-rouge">python camp.py report "01/02/2025" "01/03/2025" --min-age=7</code>.</p>

<p>This will produce an HTML file in the reports directory with all matching camps:</p>

<p><img src="/assets/images/report.PNG" alt="" /></p>

<p>The report is generated using the <a href="https://www.yattag.org/">yattag</a> library to procedurally generate the HTML report from python code.
A few highlights of the report:</p>
<ul>
  <li>The camps are sorted by shortest commute first</li>
  <li>The report highlights which camps include a swimming break</li>
  <li>Each camp includes a session link for easy registration</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>Overall I found this side project to be quite rewarding. I learned how to:</p>
<ul>
  <li>scrape data from websites (selenium and httpx)</li>
  <li>parse structured data from websites (beautifulsoup)</li>
  <li>keep API keys secret using environment variables</li>
  <li>use 3rd party APIs</li>
  <li>generate HTML from Python</li>
</ul>

<p>Finally I found the tool to actually make finding and registering my son for camp much easier!</p>

<h2 id="possible-improvements">Possible improvements</h2>

<p>I was not able to find a way for <a href="https://typer.tiangolo.com/">Typer</a> to hide commands that were not yet applicable. Ideally, only the <code class="language-plaintext highlighter-rouge">details</code> command would be initially available (taking in the links.txt that was produced during the semi-manual scraping step). Then after creating “camps.pkl” and “places.txt” the <code class="language-plaintext highlighter-rouge">commute</code> command would be available. Finally the <code class="language-plaintext highlighter-rouge">report</code> command could be used.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="scraper" /><category term="cli" /><category term="selenium" /><category term="httpx" /><category term="yattag" /><category term="typer" /><category term="backend" /><summary type="html"><![CDATA[This post describes a command line tool I created that scrapes a website and generates reports.]]></summary></entry><entry><title type="html">Experiment 19</title><link href="https://dlfelps.github.io/2024/08/01/generative.html" rel="alternate" type="text/html" title="Experiment 19" /><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/08/01/generative</id><content type="html" xml:base="https://dlfelps.github.io/2024/08/01/generative.html"><![CDATA[<p>This post describes the stable diffusion architecture for generative AI.</p>

<h1 id="introduction">Introduction</h1>

<p>I am going to introduce this post with a real conversation I had with Microsoft’s <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you explain Generative AI to me like I am 5 years old?</p>

  <p>Copilot:</p>

  <p>Certainly! Imagine you have a magical drawing book. When you open it, it creates new pictures all by itself! That’s what generative AI does—it makes up things like drawings, stories, or even songs without anyone telling it exactly what to create. It’s like having a creative friend who invents cool stuff just by thinking about it! 🎨✨</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Are you an example of a Generative AI?</p>

  <p>Copilot:</p>

  <p>Indeed! I’m like a friendly digital artist. When you ask me for something, I use my magical algorithms to create new sentences, poems, or even drawings. Just like a painter mixes colors to create beautiful pictures, I mix words and ideas to make interesting content! 🎨✨</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you generate an image of an Eastern Bluebird drinking a milkshake?</p>

  <p>Copilot:</p>

  <p><img src="/assets/images/bluebird_milkshake_dalle.PNG" alt="bluebird milkshake" /></p>
</blockquote>

<p>END OF CONVERSATION</p>

<p>I was impressed by the generated images. Behind the scenes, Copilot uses an OpenAI product called <a href="https://openai.com/dall-e-3/">DALLE-3</a> to generate images. Unfortunately DALLE-3 is not publicly available. So, for the remainder of this post, we will instead examine a closely related model called Stable Diffusion. How do they compare? Here is the output of Stable Diffusion for the same prompt above:</p>

<p><img src="/assets/images/bluebird_milkshake_stable.webp" alt="stable diffusion" width="400" /></p>

<p>Generally Stable Diffusion tends to produce images that are more photorealistic whereas DALLE-3 creates images that feel like computer-generated art.</p>

<h1 id="unconditional-image-generation-denoising-diffusion-probabilistic-models">Unconditional Image Generation (Denoising Diffusion Probabilistic Models)</h1>

<p>I will explain Stable Diffusion through a series of increasingly complex models. The first - and most foundational model -  is the Denoising Diffusion Probabilistic Model (DDPM), which serves as the core technology behind Stable Diffusion and DALLE-3.</p>

<p>DDPMs are a class of generative models that work by iteratively denoising a diffusion process, which involves adding noise to an image and then trying to remove it. The underlying theory of DDPMs is based on the idea that transforming a simple distribution, such as a Gaussian, through a series of diffusion steps can result in a complex and expressive image data distribution. This allows the model to generate new images by reversing the diffusion process, starting from the full Gaussian distribution and ending up with the image distribution.</p>

<p><img src="https://huggingface.co/blog/assets/78_annotated-diffusion/diffusion_figure.png" alt="forward diffusion" /></p>

<p>To further demonstrate the contribution of each step I will show what kind of generative images are possible using the CUB-2011 dataset. Our final goal will be to produce fully synthetic birds. In this first step we will only be able to generate small bird-like shapes using the DDPM architecture alone.</p>

<p><img src="/assets/images/generative_figure1.png" alt="ddpm" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The output of training the model is shown below. Here I take a snapshot of the sample images every 10 epochs to show the learning progression of the model.</p>

<p><img src="/assets/images/ddpm-64.gif" alt="ddpm animation" /></p>

<p>You can see birds if you squint, but the low resolution (64x64) makes these images less useful. DDPMs top out around (256x256) and become increasingly slow to train on a single GPU. We next explore two options for increasing the resolution.</p>

<h1 id="generating-higher-resolution-images">Generating Higher Resolution Images</h1>

<p>The following two sections offer alternatives to increasing the resolution of the generated images.</p>

<h2 id="upscaling">Upscaling</h2>

<p>The first approach uses image upscaling to increase the resolution of an image. Traditional upscaling methods, such as interpolation, don’t increase the inherent resolution of the image. The upscaler used here actually fills in missing details to increase resolution (NOTE: the upscaler itself is based on stable diffusion architecture). The architecture for the approach is outlined below:</p>

<p><img src="/assets/images/generative_figure2.png" alt="upscaling" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The benefit of this approach is that it uses the same DDPM from step 1 (which produces images of 64x64) and then the upscaler increases the resolution to 256x256.</p>

<p>The disadvantage of this approach is that the upscaler is not tuned specifically for your data (although it could be with additional training). It also makes a slow process (sampling the DDPM) even slower because you are performing two indepdenent steps (sampling then upscaling).</p>

<h2 id="latent-model">Latent model</h2>

<p>The second approach to increasing resolution was introduced in <a href="https://arxiv.org/abs/2112.10752">“High-Resolution Image Synthesis with Latent Diffusion Models”</a>. The idea behind this approach is to surround the DDPM with a pretrained variational autoencoder<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. The DDPM then learns to denoise the latent space (which also happens to be 64x64) instead of the image space. To create a new sample:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. generate random noise in the shape of the latent space
2. use the DDPM to denoise the input
3. use the decoder to transform the latent space back into image space (512x512)
</code></pre></div></div>

<p><img src="/assets/images/generative_figure3.png" alt="latent" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Unfortunately I did not have enough Colab credits to fully train this variant<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Although the model outputs images that are 512x512, I admit that they don’t look very bird-like. Here is a single full image created after 50 epochs of training:</p>

<p><img src="/assets/images/latent_bird.png" alt="latent image" /></p>

<h1 id="conditional-image-generation-stable-diffusion">Conditional Image Generation (Stable Diffusion)</h1>

<p>Up to this point we have not had any control over what kind of bird gets synthesized. The final step is to condition the DDPM on another input (usually text, but it could also be another image). NOTE: In order to train such a model, the training data would require both text and captions. The CUB-2011 dataset does not provide captions and I also discovered that significantly more compute is needed to train the latent models so I will use a pretrained Stable Diffusion model.</p>

<p><img src="/assets/images/generative_figure4.png" alt="stable diffusion" width="600" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Here is an example output:</p>

<p><img src="/assets/images/eastern_bluebird_stable.png" alt="stable diffusion output" /></p>

<p>I generated the image above with the implementation provided by Lyumin Zhang called <a href="https://github.com/lllyasviel/Fooocus">Fooocus</a>. It is a pre-tuned version of <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">StabilityAI’s Stable Diffusion XL model</a>. You can generate your own images using the provided Colab notebook -&gt; 
<a target="_blank" href="https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a></p>

<h1 id="conclusion">Conclusion</h1>

<p>Generative AI is currently experiencing a boom, with new research and products coming out every day. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>. I would like to leave you with a final conversation from Microsoft’s <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you write a poem about generative ai?</p>

  <p>Copilot:</p>

  <p>Sure, I’d love to! Here’s a poem about generative AI:</p>

  <p>In circuits deep, where data flows, A spark ignites, a thought that grows. From silicon dreams, a mind takes flight, Crafting worlds in the dead of night.</p>

  <p>With algorithms as its guiding star, It reaches realms both near and far. From pixels bright to words that sing, Generative AI creates everything.</p>

  <p>It learns from patterns, vast and wide, In every byte, where secrets hide. A painter’s brush, a poet’s pen, It brings to life what once was then.</p>

  <p>Yet in its core, a humble start, A tool to aid the human heart. For in each line and crafted scene, It mirrors dreams, both bold and serene.</p>

  <p>So here’s to code that dreams and dares, To generative AI, with whom we share, A future bright, where minds entwine, In harmony, both yours and mine.</p>
</blockquote>

<p>END OF CONVERSATION</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>This particular autoencoder preserves spatial information by compressing a 512x512x3 image into a latent space of 64x64x4. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>As a comparison, in <a href="https://arxiv.org/abs/2112.10752">“High-Resolution Image Synthesis with Latent Diffusion Models”</a>, they trained a latent model on FFHQ for 283 hours on an A100. This would cost an equivalent of $370 in Colab compute units so you will have to forgive me for stopping early! <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="generative" /><category term="stable-diffusion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes the stable diffusion architecture for generative AI.]]></summary></entry><entry><title type="html">Experiment 18</title><link href="https://dlfelps.github.io/2024/07/01/adversarial.html" rel="alternate" type="text/html" title="Experiment 18" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/07/01/adversarial</id><content type="html" xml:base="https://dlfelps.github.io/2024/07/01/adversarial.html"><![CDATA[<p>This post describes simple approaches to attack and defend machine learning models.</p>

<h1 id="introduction-to-adversarial-machine-learning">Introduction to adversarial machine learning</h1>
<p>Adversarial machine learning refers to a set of techniques that adversaries use to attack machine learning systems. These attacks exploit vulnerabilities in ML models, aiming to manipulate their behavior or compromise their performance. Adversarial attacks can be used against any system that employs machine learning, including finance, security, and autonomous systems.</p>

<h2 id="types-of-adversarial-attacks">Types of Adversarial Attacks</h2>
<p>There are several types of known attacks against machine learning models. Please see the NIST  <a href="https://csrc.nist.gov/pubs/ai/100/2/e2023/final">taxonomy</a> for a comprehensive breakdown with references. The remainder of the post focuses on the evasion attack, which modifies the input in a way that alters the prediction of the model under attack.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center"><strong>Poisoning</strong></th>
      <th style="text-align: center"><strong>Evasion</strong></th>
      <th style="text-align: center"><strong>Extraction</strong></th>
      <th style="text-align: center"><strong>Inference</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>during</em></td>
      <td style="text-align: center">Training</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
    </tr>
    <tr>
      <td style="text-align: center"><em>purpose</em></td>
      <td style="text-align: center">Mistrain the model</td>
      <td style="text-align: center">Deceive the model</td>
      <td style="text-align: center">Steal the model</td>
      <td style="text-align: center">Infer training data</td>
    </tr>
  </tbody>
</table>

<h2 id="threat-models">Threat models</h2>

<p>An important consideration when attacking/defending a machine learning model is the threat model. The two primary threat models are white-box and black-box attacks.</p>

<p><img src="/assets/images/white_box-Photoroom.png" alt="white box" /></p>

<blockquote>
  <p><strong>White-Box Attack:</strong> The attacker has <strong>full knowledge</strong> of the ML model’s architecture, parameters, and training data. e.g. <a href="https://arxiv.org/pdf/1412.6572v3.pdf">L-BFGS Attack</a>, <a href="https://arxiv.org/abs/1312.6199">Fast Gradient Sign Method</a></p>
</blockquote>

<p><img src="/assets/images/black_box-Photoroom.png" alt="black box" /></p>

<blockquote>
  <p><strong>Black-Box Attacks:</strong> The attacker has <strong>limited information</strong> about the ML model (e.g., input-output interactions).  e.g. <a href="https://arxiv.org/abs/1809.02861">Transfer Attack</a>, <a href="https://arxiv.org/abs/1708.03999">Score-based Black-Box Attacks</a>, <a href="https://arxiv.org/abs/1712.04248">Decision-based Attack</a></p>
</blockquote>

<p>Note that adversarial exmaples can be used under either threat model, however they are much more effective when an adversary has access to the internals of your model (i.e. white-box) and can verify that the attack works prior to deploying it.</p>

<h2 id="experimental-setup">Experimental setup</h2>

<p>As with a number of my previous posts, I will be using a pretrained <a href="https://github.com/osmr/imgclsmob/tree/master/pytorch#cub-200-2011">ResNet-18 model</a> trained on the <a href="https://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB2011 dataset</a>; this will serve as the model under attack.</p>

<h1 id="scenario-1-attacking-a-model-with-non-adversarial-methods">Scenario 1: Attacking a model with non-adversarial methods</h1>
<p>This “attack” can also be seen as a model hardening test. I recommend running it on all computer vision models prior to deploying them. It tests the extent to which the model can handle common image transformations in the wild. Depending on the results, we might discover a cheap form of the evasion attack (applies to both black-box and white-box settings). The attack works as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select a common image transform (e.g. rotation, brightness, contrast)
  2. Select a random test image
  3. Run the test image through the model and save the predicted class
  4. Run the transformed test image through the model and save the predicted class
  5. If #3 and #4 are different then increase counter
  6. Repeat steps 2-5 100 times to estimate the effect of the transform
</code></pre></div></div>

<h2 id="results">Results</h2>
<p>The following plots visually demonstrate the effect of a single transform. The results (represented by the blue bar to the right of each image) represent the average performance of 100 test images. A higher bar signifies that the model is robust to that transform (i.e. the counter in step#5 was low).</p>

<p><img src="/assets/images/blur.png" alt="blur" /></p>

<p><img src="/assets/images/hue.png" alt="hue" /></p>

<p><img src="/assets/images/rotate.png" alt="rotate" /></p>

<p><img src="/assets/images/brightness.png" alt="brightness" /></p>

<p><img src="/assets/images/contrast.png" alt="contrast" /></p>

<p><img src="/assets/images/gamma.png" alt="gamma" /></p>

<p><img src="/assets/images/saturation.png" alt="saturation" /></p>

<h2 id="discussion">Discussion</h2>
<p>The model under attack was relatively robust to subtle image transforms and should be expected to perform well in the wild. Hue was the only transform that significantly impacted performance and that is expected since color is a differentiating factor in bird species. Can we use hue as a cheap form of adversarial attack? No, because the hue transform would also fool a human. In other words, the hue transform fundamentally changes the input so it isn’t “fooling” anything. We next turn to more advanced methods.</p>

<h1 id="scenario-2-attacking-a-model-with-adversarial-methods">Scenario 2: Attacking a model with adversarial methods</h1>

<p>The goal of an evasion attack is to minimally modify an input (i.e. image) in such a way that the model alters its class prediction, but a human still percieves the original class. It’s this combination that makes the evasion attack so potent - humans don’t know anything is wrong and the machine is fooled. The attack succeeds because the altered image lies outside of the training distribution.</p>

<h2 id="pgd">PGD</h2>
<p>The Projected Gradient Descent (PGD) attack <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> is an iterative optimization technique that seeks to find the most adversarial perturbation within a predefined constraint set. Here is psuedocode for the algorithm:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select an unmodified image as the target
  2. Select a random point within the allowed perturbation region of the target image
  3. Perform gradient descent on the model's loss function
  4. Project the perturbed input back into the feasible set after each iteration
</code></pre></div></div>
<p>This iterative process ensures that the adversarial perturbation remains within the defined constraints around the original input, making the adversarial example both effective and imperceptible to human observers.</p>

<p>However, the PGD attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Over-optimization against one constraint</li>
  <li>Potential for unstable optimization</li>
  <li>Limited transferability to other models</li>
</ul>

<h3 id="example">Example</h3>

<p><img src="/assets/images/pgd_400.png" alt="pgd_400" /></p>
<blockquote>
  <p>A successful example of a PGD attack (left). The modified image was classified differently than the original image, but the modifications are barely visible to the human eye. The modifications added to the original image are magnified 10x in the (right) image.</p>
</blockquote>

<h2 id="carlini-wagner">Carlini-Wagner</h2>
<p>The Carlini &amp; Wagner (C&amp;W) attack is a powerful adversarial evasion attack that aims to generate adversarial examples that are misclassified by a target model. Adversarial examples are created by formulating an optimization problem, where the goal is to minimize the perturbation while ensuring the classifier mislabels the adversarial example. C&amp;W is particularly notable for its ability to generate adversarial examples that are transferable across different models and defenses, making it a significant threat to the security of machine learning systems.</p>

<p>However, the C&amp;W attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Computational cost</li>
  <li>Requires domain-specific adaptations to shape the perturbations and the optimization problem depending on the type of data being manipulated</li>
</ul>

<h3 id="example-with-l0-norm">Example with L0 norm</h3>

<p><img src="/assets/images/carliniL0_392.png" alt="carliniL0_392" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. Although the modifications are mostly imperceptible, a small eye-like patch is visible in the magnitude-magnified image (right).</p>
</blockquote>

<h3 id="example-with-linf-norm">Example with LInf norm</h3>

<p><img src="/assets/images/carliniLinf_3207.png" alt="carliniLinf_3207" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. The modifications in this case are noticable and global in the original image and magnitude-magnified image (right).</p>
</blockquote>

<h1 id="scenario-3-defending-a-model-with-non-adversarial-methods">Scenario 3: Defending a model with non-adversarial methods</h1>

<p><img src="/assets/images/adv-flowchart.png" alt="adv-flowchart" /></p>

<p>This scenario explores the exent to which non-adversarial methods can be used to defend a model from a strong adversarial attack (e.g. PGD or Carlini-Wagner). The defense exploits the fact that the evasion attack itself is typically brittle. In other words, the attack noise was calculated to minimize the amount of change in the input space to achieve the desired result. Therefore, it stands to reason that the attack may not be able to withstand additional transformations. We constrain the suite of transformations considered in Scenario #1 to only those transformations that maintained greater than 99% performance on natural images. This allows us to limit the amount of false positives in our adversarial detector to less than 1%.</p>

<h2 id="results-1">Results</h2>
<p>In the tables below we report the prediction rates for the best parameter combination per transformation type (e.g. rotation, brightness).</p>

<h3 id="pgd-1">PGD</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.049505</td>
      <td style="text-align: center">0.950495</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.283019</td>
      <td style="text-align: center">0.716981</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.441441</td>
      <td style="text-align: center">0.553864</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.454545</td>
      <td style="text-align: center">0.545455</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.555556</td>
      <td style="text-align: center">0.444444</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-l0">CW-L0</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.114286</td>
      <td style="text-align: center">0.885714</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.6</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.558442</td>
      <td style="text-align: center">0.441558</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.561905</td>
      <td style="text-align: center">0.4334</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.741935</td>
      <td style="text-align: center">0.258065</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">0.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.815126</td>
      <td style="text-align: center">0.184874</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-linf">CW-LInf</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.819444</td>
      <td style="text-align: center">0.180556</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.910256</td>
      <td style="text-align: center">0.089744</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.939394</td>
      <td style="text-align: center">0.060606</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.943182</td>
      <td style="text-align: center">0.056818</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.944444</td>
      <td style="text-align: center">0.055556</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h2 id="discussion-1">Discussion</h2>

<p>The highest average difference between performance on natural images and adversarial images is achieved by rotating images by 7 degrees. This transformation is ideal from a false-positive point of view as all natural images are unphased by it. NOTE: this does not mean that the predicted class is correct, only that the predicted class is not altered by rotating the image 7 degrees. Our ability to detect adversarial examples depends on the attack:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">TP rate</th>
      <th style="text-align: center">FP rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">PGD</td>
      <td style="text-align: center">0.95</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-L0</td>
      <td style="text-align: center">0.885</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-LInf</td>
      <td style="text-align: center">0.18</td>
      <td style="text-align: center">0.0</td>
    </tr>
  </tbody>
</table>

<p>The results are quite promising for PGD and CW-L0, but the CW-LInf attack proved to be more resilient (to all tested transformations). The results appear to confirm our hypothesis about the fragility of the evasion attacks - more subtle attacks are more easily detected precisely because they are more subtle. In other words, if the attack has a low signal-to-noise ratio then rotating the image has a better chance to render it ineffective.</p>

<h1 id="scenario-4-defending-a-model-with-adversarial-methods">Scenario 4: Defending a model with adversarial methods</h1>

<p>Adversarial training is a technique used to enhance the robustness of machine learning models against adversarial examples. The simplest way to perform adversarial training is to include adversarial examples in the training set.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Train a model with the original training data
2. Use the model to generate adversarial examples
3. Add the adversarial examples (with correct labels) to the training data
4. Retrain the model
</code></pre></div></div>
<p>A more sophisticated approach called Robust training <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> alters the optimization process itself in order to make models more resilient to adversarial perturbations. These perturbations are carefully crafted to deceive the model into making incorrect predictions.</p>

<h2 id="comparing-detection-and-adversarial-training">Comparing detection and adversarial training</h2>

<p>Adversarial training strengthens models intrinsically, while detection approaches focus on identifying adversarial examples during inference. Both play crucial roles in addressing the challenges posed by adversarial attacks.</p>

<ul>
  <li>advantages of adversarial detection
    <ul>
      <li>can defend against white-box and black-box threat models</li>
      <li>can be used to protect existing/pretrained models</li>
      <li>may work against UNKNOWN attacks</li>
    </ul>
  </li>
  <li>advantages of adversarial training
    <ul>
      <li>moderate success against all KNOWN attacks</li>
    </ul>
  </li>
</ul>

<p>NOTE: If you are training your own model then you can do BOTH!</p>

<h1 id="conclusion">Conclusion</h1>
<p>This was a long post, so I will summarize the results here:</p>
<ol>
  <li>It is good practice to test your model’s performance against non-adversarial image transformations.</li>
  <li>Evasion attacks (e.g. PGD and CW) can be used to alter the predictions of a model.</li>
  <li>Some evasion attacks (e.g. PGD and CW-L0) can be detected at inference time due to their lack of robustness against non-adversarial image transformations.</li>
  <li>Other evasion attacks (e.g. CW-LInf) may be rendered inneffective using adversarial training.</li>
</ol>

<p>As ML adoption grows, understanding and addressing these vulnerabilities become crucial for maintaining system integrity and security. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="adversarial" /><category term="evasion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes simple approaches to attack and defend machine learning models.]]></summary></entry></feed>