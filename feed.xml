<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://dlfelps.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dlfelps.github.io/" rel="alternate" type="text/html" /><updated>2025-10-28T16:13:49+00:00</updated><id>https://dlfelps.github.io/feed.xml</id><title type="html">.NET Experiments</title><subtitle>Bill Nye meets Bill Gates ;)</subtitle><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><entry><title type="html">Experiment 25</title><link href="https://dlfelps.github.io/2025/12/01/sparse-rl.html" rel="alternate" type="text/html" title="Experiment 25" /><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/12/01/sparse-rl</id><content type="html" xml:base="https://dlfelps.github.io/2025/12/01/sparse-rl.html"><![CDATA[<p>This post demonstrates that reinforcement learning agents can be trained with sparse reward signals as effectively as carefully tuned dense rewards.</p>

<h1 id="optimizing-last-mile-delivery-with-reinforcement-learning-from-dense-to-sparse-rewards">Optimizing Last-Mile Delivery with Reinforcement Learning: From Dense to Sparse Rewards</h1>

<h2 id="introduction-the-problem-of-last-mile-delivery">Introduction: The Problem of Last-Mile Delivery</h2>

<p>Last-mile delivery‚Äîthe final leg of a package‚Äôs journey from distribution center to customer‚Äîrepresents one of the most expensive and operationally challenging aspects of modern logistics. According to industry research, last-mile costs can account for 50-60% of total shipping expenses, and with the explosive growth of e-commerce, the problem has only become more acute.</p>

<p>The core challenge is deceptively simple to state: Given a fleet of vehicles with limited capacity, a stream of daily package arrivals with varying priorities and delivery deadlines, and geographic constraints on which truck can serve which zone, how do you decide <strong>which packages to deliver today and which to defer</strong> to maximize operational value while meeting customer expectations?</p>

<p>This is a constrained optimization problem that sits at the intersection of:</p>
<ul>
  <li><strong>Vehicle Routing Problems (VRP)</strong>: Classical logistics optimization</li>
  <li><strong>Knapsack Problems</strong>: Capacity-constrained selection</li>
  <li><strong>Temporal Scheduling</strong>: Deadline-aware decision making</li>
  <li><strong>Reinforcement Learning</strong>: Learning optimal policies from experience</li>
</ul>

<p>Traditional approaches to logistics optimization have relied on integer linear programming, metaheuristics (genetic algorithms, simulated annealing), and handcrafted rule-based systems. But what if we could train an agent to learn better strategies through experience? And what if we could do it <strong>without designing a complex reward function</strong>?</p>

<p>That‚Äôs the premise of this project, and the surprising answer lies in <strong>sparse reward learning</strong>.</p>

<hr />

<p>The Simulation Model</p>

<p>We model the delivery optimization problem as a <strong>constrained capacity problem with 10 trucks, 10 zones, and 30 daily package arrivals</strong>:</p>

<p><strong>Infrastructure:</strong></p>
<ul>
  <li><strong>Warehouse</strong>: Central depot (location 0)</li>
  <li><strong>Zones</strong>: 10 geographic zones, each with 4 delivery addresses (40 total addresses)</li>
  <li><strong>Trucks</strong>: 10 trucks, one assigned per zone
    <ul>
      <li>Capacity: 2 packages per truck per day</li>
      <li>Total daily fleet capacity: 20 packages</li>
    </ul>
  </li>
</ul>

<p><strong>Daily Arrivals:</strong></p>
<ul>
  <li><strong>Volume</strong>: 30 packages per day</li>
  <li><strong>Shortage</strong>: 33% capacity shortage (must defer 10 packages)</li>
  <li><strong>Priority</strong>: Bimodal distribution
    <ul>
      <li>40% low-priority (1-3)</li>
      <li>40% high-priority (8-10)</li>
      <li>20% medium-priority (4-7)</li>
    </ul>
  </li>
  <li><strong>Deadlines</strong>: 1-7 days from arrival</li>
  <li><strong>Destinations</strong>: Uniformly distributed across zones</li>
</ul>

<hr />

<p>The Core Decision Problem</p>

<p>Each day, each truck independently faces the decision:</p>
<blockquote>
  <p><strong>‚ÄúOf my available packages (new arrivals + deferred backlog), which 2 should I deliver today?‚Äù</strong></p>
</blockquote>

<p>This is non-trivial because:</p>
<ol>
  <li><strong>Capacity is limited</strong>: Can‚Äôt deliver everything</li>
  <li><strong>Deadlines create urgency</strong>: Some packages expire soon</li>
  <li><strong>Priorities are heterogeneous</strong>: High-priority packages are worth more</li>
  <li><strong>Deferrals have costs</strong>: Deferred packages tie up future capacity</li>
  <li><strong>Information is incomplete</strong>: Don‚Äôt know future arrivals</li>
</ol>

<h2 id="algorithmic-solutions-from-dense-to-sparse-rewards">Algorithmic Solutions: From Dense to Sparse Rewards</h2>

<h3 id="1-greedy-baseline">1. Greedy Baseline</h3>

<p><strong>Algorithm</strong>: Deliver all available packages (up to capacity limit)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For each truck:
  available_packages = deferred + new_arrivals
  deliver = sort_by_priority(available_packages)[:capacity]
</code></pre></div></div>

<p><strong>Characteristics:</strong></p>
<ul>
  <li>Deterministic</li>
  <li>No learning required</li>
  <li>Simple to implement</li>
  <li>Treats all packages equally (ignores deadline pressure)</li>
</ul>

<p><strong>Performance Baseline</strong>:</p>
<ul>
  <li>Average daily reward: 805.7</li>
  <li>On-time delivery: 75.6%</li>
  <li>No variance (deterministic)</li>
</ul>

<h3 id="2-dense-reward-pposac-traditional-rl-approach">2. Dense Reward PPO/SAC (Traditional RL Approach)</h3>

<p><strong>Reward Function</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total Reward = Œ£(delivered_package_value) - Œ£(missed_deadline_penalty) - distance_cost

Where:
  delivered_package_value = 100 √ó (priority / 10)
  missed_deadline_penalty = -50 √ó (priority / 10)
  distance_cost = -0.01 √ó kilometers_traveled
</code></pre></div></div>

<p><strong>Key Characteristics</strong>:</p>
<ul>
  <li>Complex reward engineering required</li>
  <li>Hyperparameters: 100 (delivery weight), -50 (miss penalty), -0.01 (distance)</li>
  <li>Requires extensive domain knowledge to set correctly</li>
  <li>Dense signal ranges from -500 to +1000 per decision</li>
  <li>Scaling factors need careful tuning</li>
</ul>

<h3 id="3-sparse-reward-pposac-novel-approach">3. Sparse Reward PPO/SAC (Novel Approach)</h3>

<p><strong>Core Insight</strong>: What if we don‚Äôt need a complex reward function at all? What if agents can learn from <strong>comparative signals only</strong>?</p>

<p><strong>Sparse Reward Signal</strong>:</p>

<p>Instead of <code class="language-plaintext highlighter-rouge">-500 to +1000</code>, agents receive:</p>
<ul>
  <li><strong>+1</strong>: When policy outperforms what the greedy baseline would have done</li>
  <li><strong>-1</strong>: When policy underperforms the baseline</li>
  <li><strong>0</strong>: When policy matches the baseline</li>
</ul>

<p><strong>Why This Matters</strong>:</p>
<ul>
  <li>No hyperparameter tuning needed (just +1 and -1)</li>
  <li>No domain knowledge required to scale rewards</li>
  <li>Agents learn from pure comparison: ‚ÄúAm I better or worse?‚Äù</li>
  <li>Signal is invariant to absolute reward magnitude</li>
  <li><strong>Simpler, more robust, more generalizable</strong></li>
</ul>

<p><strong>Architecture Unchanged</strong>:</p>
<ul>
  <li>Same zone-agnostic shared policy</li>
  <li>Same observations: priority, deadline_urgency, distance, truck_load</li>
  <li>Same action space: continuous confidence scores for packages</li>
  <li>Same training (PPO and SAC) and duration (500k timesteps)</li>
</ul>

<p><strong>The only difference</strong>: The reward signal during training</p>

<hr />

<h2 id="the-surprising-results-sparse-wins-over-dense">The Surprising Results: Sparse Wins Over Dense</h2>

<p>We evaluated all agents (Greedy, Dense PPO, Dense SAC, Sparse PPO, Sparse SAC) using a <strong>relative value metric</strong>: How much better/worse than the baseline?</p>

<table>
  <thead>
    <tr>
      <th>Agent</th>
      <th>Daily Advantage</th>
      <th>Win Rate</th>
      <th>Max Advantage</th>
      <th>Std Dev</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Sparse PPO</strong> üèÜ</td>
      <td><strong>+464.8</strong></td>
      <td>98.6%</td>
      <td>+1020.2</td>
      <td>200.7</td>
    </tr>
    <tr>
      <td><strong>Dense PPO</strong></td>
      <td>+461.5</td>
      <td>99.6%</td>
      <td>+980.0</td>
      <td>192.5</td>
    </tr>
    <tr>
      <td>Dense SAC</td>
      <td>+399.7</td>
      <td>96.8%</td>
      <td>+999.5</td>
      <td>220.0</td>
    </tr>
    <tr>
      <td>Sparse SAC</td>
      <td>+362.4</td>
      <td>96.6%</td>
      <td>+860.1</td>
      <td>198.8</td>
    </tr>
    <tr>
      <td><strong>Greedy Baseline</strong></td>
      <td>0 (reference)</td>
      <td>50%</td>
      <td>0</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/images/01_absolute_rewards.png" alt="Absolute Rewards" /></p>

<p>Both PPO agents significantly outperform SAC and the baseline. The Sparse PPO‚Äôs slight edge over Dense PPO (127,054 vs 126,720) is noteworthy because it achieves this with a 1000x simpler reward signal.</p>

<hr />

<p><img src="/assets/images/02_relative_values.png" alt="Relative Values" /></p>

<p>This is the most important metric: How much better/worse than the greedy baseline?</p>

<p>The heights are nearly identical for the PPO variants, showing that sparse and dense rewards achieve essentially the same end result. The slight Sparse PPO edge (+3.3 points) is statistically insignificant but philosophically important: <strong>simpler signals achieved better results</strong>.</p>

<p>The SAC agents‚Äô lower performance suggests that SAC‚Äôs off-policy nature may require denser reward signals for effective learning, or that our SAC hyperparameters weren‚Äôt as well-tuned as PPO‚Äôs.</p>

<hr />

<p><img src="/assets/images/03_win_rates.png" alt="Win Rates" /></p>

<p>Win rate = percentage of days the agent beats the greedy baseline</p>

<p><strong>Interpretation</strong>:</p>
<ul>
  <li>All agents are highly reliable, beating the baseline &gt;96% of the time</li>
  <li>Dense PPO has a slight edge in consistency (99.6% vs 98.6%)</li>
  <li>But this comes at the cost of complex reward engineering</li>
  <li>The trade-off is acceptable: Sparse PPO‚Äôs 98.6% win rate is still excellent</li>
  <li>Only 7 days out of 500 where Sparse PPO underperforms (1.4% failure rate)</li>
</ul>

<hr />

<p><img src="/assets/images/04_relative_evolution.png" alt="Relative Evolution" /></p>

<p>This plot shows how the daily advantage changes throughout a 100-day episode</p>

<p><strong>Interpretation</strong>:</p>
<ul>
  <li>All agents maintain stable performance across the full episode</li>
  <li>No degradation as days progress (agents don‚Äôt ‚Äútire out‚Äù)</li>
  <li>No improvement trend (agents don‚Äôt learn within an episode; learning happens across episodes)</li>
  <li>Sparse PPO‚Äôs consistency mirrors Dense PPO perfectly</li>
  <li>This stability is crucial for real-world deployment: reliable day-after-day performance</li>
</ul>

<h2 id="why-does-sparse-reward-learning-work">Why Does Sparse Reward Learning Work?</h2>

<p>Agents trained with only <strong>-1, 0, +1</strong> signals matched or exceeded agents trained with <strong>-500 to +1000</strong> dense rewards. This challenges conventional RL wisdom. Why?</p>

<h3 id="hypothesis-1-signal-noise-reduction">Hypothesis 1: Signal Noise Reduction</h3>

<p>Dense rewards encode both:</p>
<ol>
  <li><strong>Direction</strong>: Should I improve or degrade my behavior?</li>
  <li><strong>Magnitude</strong>: By how much?</li>
</ol>

<p>The magnitude component introduces noise:</p>
<ul>
  <li>How much should I weight a +500 reward vs +550?</li>
  <li>Are the scaling factors (100, -50, -0.01) optimal?</li>
  <li>What if I increase scale 2x? The agent relearns everything.</li>
</ul>

<p>Sparse signals provide only <strong>direction</strong>, removing scaling noise:</p>
<ul>
  <li>Better than baseline? ‚Üí +1 (learn this policy)</li>
  <li>Worse? ‚Üí -1 (unlearn this policy)</li>
  <li>Same? ‚Üí 0 (neutral)</li>
</ul>

<p>This <strong>cleaner signal</strong> might be more learnable.</p>

<h3 id="hypothesis-2-relative-framebench-built-in">Hypothesis 2: Relative Framebench Built-In</h3>

<p>Dense rewards are <strong>absolute</strong>: ‚ÄúThis decision is worth 547 points.‚Äù</p>

<p>Sparse rewards are <strong>relative</strong>: ‚ÄúThis decision beats the baseline.‚Äù</p>

<p>Relative framebench is more robust:</p>
<ul>
  <li>Generalizes across different problem scales</li>
  <li>Adapts if baseline changes (e.g., competitor improves)</li>
  <li>Built-in regularization against extreme behaviors</li>
  <li>Agents learn what matters: being <strong>better, not just good</strong></li>
</ul>

<h3 id="hypothesis-3-simpler-loss-landscape">Hypothesis 3: Simpler Loss Landscape</h3>

<p>Dense reward functions create complex, multi-objective loss surfaces:</p>
<ul>
  <li>Deliver high-priority: +100 √ó priority/10</li>
  <li>Avoid misses: -50 √ó priority/10</li>
  <li>Minimize distance: -0.01 √ó km</li>
</ul>

<p>Three conflicting objectives, each with scaling factors.</p>

<p>Sparse signals create simpler landscapes:</p>
<ul>
  <li>Single objective: maximize probability of beating baseline</li>
  <li>Binary feedback (better/worse)</li>
  <li>Cleaner gradient flow</li>
</ul>

<p>Simpler landscapes = faster convergence, better local optima.</p>

<h2 id="real-world-advantages-of-sparse-reward-learning">Real-World Advantages of Sparse Reward Learning</h2>

<p>In academic settings, we can carefully design reward functions. In the real world, this is <strong>hard</strong>:</p>

<ol>
  <li><strong>Unknown Objectives</strong>: What‚Äôs the true business objective?
    <ul>
      <li>Maximize revenue? Minimize costs? Customer satisfaction? All of the above?</li>
      <li>Often these conflict</li>
    </ul>
  </li>
  <li><strong>Scaling Uncertainty</strong>: How to weight different objectives?
    <ul>
      <li>Is on-time delivery worth 2x cost savings or 10x?</li>
      <li>How much does customer satisfaction matter vs revenue?</li>
      <li>These weights differ by market, season, customer segment</li>
    </ul>
  </li>
  <li><strong>Lack of Domain Knowledge</strong>: What if you‚Äôre optimizing something new?
    <ul>
      <li>Traditional logistics expertise doesn‚Äôt transfer perfectly</li>
      <li>New market = new constraints</li>
      <li>Hand-tuned weights from one domain fail in another</li>
    </ul>
  </li>
  <li><strong>Hyperparameter Sensitivity</strong>: Dense rewards require constant retuning
    <ul>
      <li>Market conditions change ‚Üí tune weights again</li>
      <li>Competitor behavior changes ‚Üí tune again</li>
      <li>Seasonal variations ‚Üí more tuning</li>
      <li><strong>No end to the tuning work</strong></li>
    </ul>
  </li>
</ol>

<p>With sparse rewards, you <strong>only need one thing</strong>: A baseline policy</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define baseline (e.g., greedy heuristic, existing system, competitor)
</span><span class="n">baseline_action</span> <span class="o">=</span> <span class="nf">get_greedy_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="n">baseline_reward</span> <span class="o">=</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">baseline_action</span><span class="p">)</span>

<span class="c1"># Define learned policy
</span><span class="n">learned_action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="n">learned_reward</span> <span class="o">=</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">learned_action</span><span class="p">)</span>

<span class="c1"># That's it! No hyperparameter tuning needed
</span><span class="k">if</span> <span class="n">learned_reward</span> <span class="o">&gt;</span> <span class="n">baseline_reward</span><span class="p">:</span>
    <span class="n">reward_signal</span> <span class="o">=</span> <span class="o">+</span><span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">reward_signal</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div>

<p><strong>Advantages</strong>:</p>
<ul>
  <li>‚úÖ No reward scaling needed</li>
  <li>‚úÖ Works with any baseline (greedy, heuristic, human expert, previous system)</li>
  <li>‚úÖ Automatically adapts as baseline changes</li>
  <li>‚úÖ Same code works for different objectives (just change baseline)</li>
  <li>‚úÖ Explainable: ‚ÄúWe learned a policy better than your existing system‚Äù</li>
</ul>

<p>This is <strong>profoundly more practical</strong> for real systems.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This project demonstrates two critical insights:</p>

<p><strong>First</strong>: Reinforcement learning can discover sophisticated logistics optimization strategies. Our shared policy agents beat a greedy baseline by 57-58% on the same problem instances that professional logistics systems solve with static heuristics.</p>

<p><strong>Second, and more surprising</strong>: You might not need a complex reward function to achieve this. A simple comparative signal (+1 if better, -1 if worse) matched or exceeded carefully tuned dense rewards (-500 to +1000).</p>

<p>This challenges a fundamental assumption in RL: <strong>that bigger, richer reward signals are always better</strong>. In practice, simpler signals that provide clear direction (‚Äúbeat the baseline‚Äù) might be more learnable and generalizable than complex signals with precise magnitudes.</p>

<p>The counter-intuitive result‚Äîthat sparse PPO (+464.8 advantage) slightly outperformed dense PPO (+461.5)‚Äîsuggests that <strong>how you train matters more than what you train with</strong>. A cleaner learning signal, even if simpler, can outweigh richer but more complex information.</p>

<p>For practitioners building real-world RL systems: <strong>Stop optimizing your reward function. Start comparing against your baseline.</strong> It‚Äôs simpler, more practical, and achieves the same (or better) results.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Optimization problems in logistics have been solved for decades using mathematical programming. Traditional RL adds learning capability. But <strong>sparse reward learning adds something different: simplicity and adaptability</strong>.</p>

<p>A traditional solver finds the optimal solution given a model. But if your model is wrong or incomplete, you‚Äôve optimized the wrong thing.</p>

<p>A dense reward agent learns a policy that maximizes your hand-tuned objective. But if that objective changes (market shifts, new constraint discovered), the training loses relevance.</p>

<p>A sparse reward agent learns one simple thing: <strong>How to beat what you already have.</strong> This is more robust, more adaptable, and more practical.</p>

<p>The next generation of logistics software‚Äîand optimization software broadly‚Äîwon‚Äôt just optimize given constraints. It will learn baseline-relative improvement through sparse, adaptive signals.</p>

<p>And sometimes, the simpler signal is the stronger one.</p>

<p>Code: Available in this <a href="https://github.com/dlfelps/sparse-rl">repository</a></p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="reinforcement-learning" /><category term="rl" /><category term="ppo" /><category term="sparse" /><category term="soft-actor-critic" /><category term="optimization" /><category term="policy" /><category term="last-mile" /><summary type="html"><![CDATA[This post demonstrates that reinforcement learning agents can be trained with sparse reward signals as effectively as carefully tuned dense rewards.]]></summary></entry><entry><title type="html">Experiment 24</title><link href="https://dlfelps.github.io/2025/11/01/langextract.html" rel="alternate" type="text/html" title="Experiment 24" /><published>2025-11-01T00:00:00+00:00</published><updated>2025-11-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/11/01/langextract</id><content type="html" xml:base="https://dlfelps.github.io/2025/11/01/langextract.html"><![CDATA[<p>The blog post demonstrates that few-shot LLMs can match 96% of fine-tuned model accuracy on scene graph extraction with 103x faster inference.</p>

<h1 id="comparing-scene-graph-extraction-methods-from-fine-tuned-t5-to-few-shot-llms">Comparing Scene Graph Extraction Methods: From Fine-tuned T5 to Few-Shot LLMs</h1>

<h2 id="introduction">Introduction</h2>

<h3 id="what-is-scene-graph-parsing">What is Scene Graph Parsing?</h3>

<p>In computer vision and natural language processing, <strong>scene graph parsing</strong> is the task of extracting structured representations from image captions or visual scenes. A scene graph breaks down a description into three key components:</p>

<ol>
  <li><strong>Entities</strong>: The objects, people, or things in the scene (e.g., ‚Äúdog‚Äù, ‚Äúbench‚Äù, ‚Äúperson‚Äù)</li>
  <li><strong>Attributes</strong>: Properties describing entities (e.g., ‚Äúbrown dog‚Äù, ‚Äúwooden bench‚Äù)</li>
  <li><strong>Relationships</strong>: Interactions and spatial relationships between entities (e.g., ‚Äúdog sits on bench‚Äù, ‚Äúperson next to tree‚Äù)</li>
</ol>

<p>For example, given the caption:</p>
<blockquote>
  <p>‚ÄúA brown dog sitting on a wooden bench in a park‚Äù</p>
</blockquote>

<p>A scene graph would extract:</p>
<ul>
  <li><strong>Entities</strong>: dog, bench, park</li>
  <li><strong>Attributes</strong>: (dog, brown), (dog, sitting), (bench, wooden)</li>
  <li><strong>Relationships</strong>: (dog, sit on, bench), (bench, in, park)</li>
</ul>

<h3 id="why-scene-graphs-matter">Why Scene Graphs Matter</h3>

<p>Scene graphs provide structured, machine-readable representations of visual information, enabling:</p>
<ul>
  <li><strong>Visual Question Answering</strong>: Understanding spatial relationships to answer questions like ‚ÄúWhat is the dog sitting on?‚Äù</li>
  <li><strong>Image Retrieval</strong>: Finding images based on specific object relationships</li>
  <li><strong>Robotics</strong>: Helping robots understand and navigate physical environments</li>
  <li><strong>Accessibility</strong>: Generating rich descriptions for visually impaired users</li>
</ul>

<h3 id="the-challenge">The Challenge</h3>

<p>Traditional approaches to scene graph extraction rely on fine-tuning specialized models on domain-specific datasets. While effective, this approach has limitations:</p>
<ul>
  <li>Requires large amounts of labeled training data</li>
  <li>Models are domain-specific and don‚Äôt generalize well</li>
  <li>Fine-tuning is computationally expensive</li>
  <li>Slow inference times due to model size</li>
</ul>

<p><strong>Can modern large language models (LLMs) match or exceed fine-tuned models using only few-shot learning?</strong></p>

<p>This blog post documents a series of experiments comparing:</p>
<ol>
  <li>A fine-tuned T5 model (baseline)</li>
  <li>LLM-based extraction using Google‚Äôs LangExtract framework with Gemini</li>
  <li>Native Gemini structured output</li>
  <li>Iteratively improved few-shot prompting</li>
</ol>

<p>All experiments use the <strong>FACTUAL Scene Graph dataset</strong>, which contains 50,000+ image captions with ground-truth scene graph annotations.</p>

<hr />

<h2 id="experiment-1-t5-baseline-performance">Experiment 1: T5 Baseline Performance</h2>

<p>Establish baseline performance using a fine-tuned T5 model pre-trained on the FACTUAL dataset.</p>

<ul>
  <li><strong>Model</strong>: <code class="language-plaintext highlighter-rouge">flan-t5-base-VG-factual-sg</code> (220M parameters)</li>
  <li><strong>Test Set</strong>: 100 complex samples (captions with &gt;20 words)</li>
  <li><strong>Evaluation</strong>: Precision, Recall, and F1 for entities, attributes, and relationships</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1 Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Entities</strong></td>
      <td>0.946</td>
      <td>0.884</td>
      <td><strong>0.907</strong></td>
      <td>343</td>
    </tr>
    <tr>
      <td><strong>Attributes</strong></td>
      <td>0.822</td>
      <td>0.770</td>
      <td><strong>0.782</strong></td>
      <td>305</td>
    </tr>
    <tr>
      <td><strong>Relationships</strong></td>
      <td>0.722</td>
      <td>0.635</td>
      <td><strong>0.662</strong></td>
      <td>227</td>
    </tr>
    <tr>
      <td><strong>Macro F1</strong></td>
      <td>-</td>
      <td>-</td>
      <td><strong>0.784</strong></td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p><strong>Inference Speed</strong>: 4.64 seconds per sample</p>

<p>The T5 baseline demonstrates that fine-tuning on domain-specific data yields strong performance, particularly for relationship extraction. However, the 4.6-second inference time and lack of flexibility make it less practical for production use cases requiring fast inference or cross-domain generalization. The high performance on the FACTUAL dataset is to be expected since this model was specifically trained using FACTUAL data. In the following experiments we only provide a handful of examples to the LLM during the prompt to achieve a similar level of performance.</p>

<hr />

<h2 id="experiment-2-langextract-proof-of-concept">Experiment 2: LangExtract Proof of Concept</h2>

<p>Validate whether LangExtract with Gemini can extract scene graphs with reasonable accuracy using few-shot learning (no fine-tuning required).</p>

<ul>
  <li><strong>Framework</strong>: Google LangExtract with Gemini 2.5 Flash</li>
  <li><strong>Test Set</strong>: 30 diverse samples (subset of Experiment 1)</li>
  <li><strong>Format</strong>: Flat Entities (separate classes for entity, attribute, relationship)</li>
  <li><strong>Examples</strong>: 5 few-shot examples demonstrating extraction patterns</li>
  <li><strong>Processing</strong>: Batch processing (all 30 samples in single API call)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1 Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Entities</strong></td>
      <td>0.956</td>
      <td>0.932</td>
      <td><strong>0.944</strong></td>
      <td>114</td>
    </tr>
    <tr>
      <td><strong>Attributes</strong></td>
      <td>0.899</td>
      <td>0.888</td>
      <td><strong>0.893</strong></td>
      <td>99</td>
    </tr>
    <tr>
      <td><strong>Relationships</strong></td>
      <td>0.237</td>
      <td>0.139</td>
      <td><strong>0.174</strong></td>
      <td>72</td>
    </tr>
    <tr>
      <td><strong>Macro F1</strong></td>
      <td>-</td>
      <td>-</td>
      <td><strong>0.670</strong></td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p><strong>Inference Speed</strong>: 0.045 seconds per sample (103x faster than T5)</p>

<p>LangExtract demonstrates the power of few-shot learning: with just 5 examples, it matches or exceeds T5‚Äôs performance on entities and attributes while being dramatically faster. However, relationship extraction is significantly weaker, suggesting the few-shot examples need improvement or the task requires more sophisticated prompting.</p>

<hr />

<h2 id="experiment-3-format-optimization">Experiment 3: Format Optimization</h2>

<p>Identify the optimal output format for LangExtract to maximize extraction accuracy.</p>

<ul>
  <li><strong>Framework</strong>: LangExtract + Gemini 2.5 Flash</li>
  <li><strong>Test Set</strong>: 50 diverse samples (subset of Experiment 1)</li>
  <li><strong>Formats Tested</strong>:
    <ol>
      <li><strong>Flat Entities</strong>: Separate classes (entity, attribute, relationship)</li>
      <li><strong>Tuple Format</strong>: Direct FACTUAL format <code class="language-plaintext highlighter-rouge">(subject, predicate, object)</code></li>
      <li><strong>Hierarchical</strong>: Nested objects with properties</li>
      <li><strong>JSON Structured</strong>: Clean JSON with entities/attributes/relationships arrays</li>
    </ol>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Flat Entities</td>
      <td>0.922</td>
      <td>0.865</td>
      <td>0.161</td>
      <td><strong>0.649</strong></td>
    </tr>
    <tr>
      <td>Tuple Format</td>
      <td>0.905</td>
      <td>0.851</td>
      <td>0.145</td>
      <td><strong>0.634</strong></td>
    </tr>
    <tr>
      <td>Hierarchical</td>
      <td>0.898</td>
      <td>0.842</td>
      <td>0.138</td>
      <td><strong>0.626</strong></td>
    </tr>
    <tr>
      <td><strong>JSON Structured</strong></td>
      <td><strong>0.928</strong></td>
      <td><strong>0.878</strong></td>
      <td><strong>0.173</strong></td>
      <td><strong>0.660</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Winner</strong>: JSON Structured (0.660 macro F1)</p>

<p>While JSON Structured emerges as the winner, the relatively small differences between formats (0.626-0.660) suggest that representation format is not the primary bottleneck. The consistent weakness in relationship extraction across all formats points to a deeper issue with the few-shot examples or prompting strategy.</p>

<hr />

<h2 id="experiment-4-backend-comparison-langextract-vs-native-gemini">Experiment 4: Backend Comparison (LangExtract vs Native Gemini)</h2>

<p>Compare LangExtract framework against native Gemini structured output using the winning JSON format.</p>

<ul>
  <li><strong>Approaches</strong>:
    <ol>
      <li><strong>LangExtract</strong>: Framework with 2 few-shot examples</li>
      <li><strong>Native Gemini</strong>: Direct API with <code class="language-plaintext highlighter-rouge">response_schema</code> + same 2 examples</li>
    </ol>
  </li>
  <li><strong>Test Set</strong>: 50 samples (same as Experiment 3)</li>
  <li><strong>Format</strong>: JSON Structured (winner from Experiment 3)</li>
</ul>

<p>After implementing centralized dataset loading to ensure all experiments use identical test data:</p>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
      <th>Speed (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>LangExtract</strong></td>
      <td><strong>0.944</strong></td>
      <td><strong>0.893</strong></td>
      <td>0.174</td>
      <td><strong>0.670</strong></td>
      <td><strong>0.045</strong></td>
    </tr>
    <tr>
      <td>Native Gemini</td>
      <td>0.784</td>
      <td>0.713</td>
      <td><strong>0.392</strong></td>
      <td>0.630</td>
      <td>3.762</td>
    </tr>
  </tbody>
</table>

<p>LangExtract‚Äôs framework optimizations for batch processing and few-shot learning make it more effective than raw Gemini API calls for entity and attribute extraction. However, Native Gemini‚Äôs structured schema enforcement provides better relationship extraction. The dramatic speed difference (83x) favors LangExtract for production use cases.</p>

<hr />

<h2 id="experiment-4b-targeted-improvement-with-analysis-driven-examples">Experiment 4b: Targeted Improvement with Analysis-Driven Examples</h2>

<p>Improve LangExtract‚Äôs relationship extraction by analyzing specific failures and creating targeted few-shot examples.</p>

<p><strong>Step 1: Failure Analysis</strong></p>
<ul>
  <li>Compared detailed results from LangExtract vs Native Gemini</li>
  <li>Identified specific relationships Native Gemini extracted correctly but LangExtract missed</li>
  <li>Found the root cause of failures</li>
</ul>

<p><strong>Key Discovery</strong>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Analysis Results:
- Total failed relationships: 52
- "sit on" predicate failures: 48 (92%)
- "with" predicate failures: 4 (8%)

Root Cause: LangExtract was extracting "sitting on" instead of "sit on"
despite having instructions to use base verb forms.
</code></pre></div></div>

<p><strong>Step 2: Create Targeted Examples</strong>
Added 5 new examples specifically demonstrating ‚Äúsitting on‚Äù ‚Üí ‚Äúsit on‚Äù normalization:</p>

<ol>
  <li>‚ÄúA white teddy bear sitting on a green carpeted stair‚Äù ‚Üí uses ‚Äúsit on‚Äù</li>
  <li>‚ÄúA small child sitting on a wooden chair‚Äù ‚Üí uses ‚Äúsit on‚Äù</li>
  <li>‚ÄúA dog sitting beside a tree‚Äù ‚Üí uses ‚Äúbeside‚Äù (not ‚Äúsitting beside‚Äù)</li>
  <li>‚ÄúPeople sitting on benches next to tables‚Äù ‚Üí uses ‚Äúsit on‚Äù</li>
  <li>‚ÄúA man standing on a ladder‚Äù ‚Üí uses ‚Äústand on‚Äù (same pattern)</li>
</ol>

<p><strong>Step 3: Re-run Evaluation</strong></p>
<ul>
  <li>Same 50 samples as Experiment 4</li>
  <li>Same LangExtract framework</li>
  <li>Now with 7 total examples (2 original + 5 targeted)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Improved LangExtract</strong></td>
      <td><strong>0.901</strong></td>
      <td><strong>0.900</strong></td>
      <td><strong>0.450</strong></td>
      <td><strong>0.750</strong></td>
    </tr>
    <tr>
      <td>Original LangExtract</td>
      <td>0.944</td>
      <td>0.893</td>
      <td>0.174</td>
      <td>0.670</td>
    </tr>
    <tr>
      <td>Native Gemini</td>
      <td>0.784</td>
      <td>0.713</td>
      <td>0.392</td>
      <td>0.630</td>
    </tr>
    <tr>
      <td>T5 Baseline</td>
      <td>0.907</td>
      <td>0.782</td>
      <td>0.662</td>
      <td>0.784</td>
    </tr>
  </tbody>
</table>

<p><strong>Improvement from Targeted Examples:</strong></p>
<ul>
  <li>Relationships: 0.174 ‚Üí 0.450 (+159% improvement!)</li>
  <li>Macro F1: 0.670 ‚Üí 0.750 (+12% improvement)</li>
</ul>

<p>This experiment demonstrates a powerful methodology for improving LLM performance:</p>
<ol>
  <li><strong>Analyze failures</strong>: Don‚Äôt just look at aggregate metrics</li>
  <li><strong>Identify patterns</strong>: Find common error modes (92% were ‚Äúsit on‚Äù failures)</li>
  <li><strong>Create targeted examples</strong>: Address specific weaknesses with focused demonstrations</li>
  <li><strong>Iterate</strong>: Measure impact and repeat</li>
</ol>

<p>The result: Improved LangExtract now <strong>outperforms all other approaches</strong> including Native Gemini, while remaining extremely fast. It comes within 4.3% of T5‚Äôs performance (0.750 vs 0.784) despite using only 7 examples instead of full fine-tuning.</p>

<hr />

<h2 id="comprehensive-comparison">Comprehensive Comparison</h2>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Test Samples</th>
      <th>Entities F1</th>
      <th>Attributes F1</th>
      <th>Relationships F1</th>
      <th>Macro F1</th>
      <th>Speed (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>T5 Baseline</strong></td>
      <td>100</td>
      <td>0.907</td>
      <td>0.782</td>
      <td><strong>0.662</strong></td>
      <td><strong>0.784</strong></td>
      <td>4.641</td>
    </tr>
    <tr>
      <td>LangExtract Original</td>
      <td>50</td>
      <td><strong>0.944</strong></td>
      <td><strong>0.893</strong></td>
      <td>0.174</td>
      <td>0.670</td>
      <td><strong>0.045</strong></td>
    </tr>
    <tr>
      <td>Native Gemini</td>
      <td>50</td>
      <td>0.784</td>
      <td>0.713</td>
      <td>0.392</td>
      <td>0.630</td>
      <td>3.762</td>
    </tr>
    <tr>
      <td><strong>LangExtract Improved</strong></td>
      <td>50</td>
      <td>0.901</td>
      <td><strong>0.900</strong></td>
      <td>0.450</td>
      <td><strong>0.750</strong></td>
      <td><strong>0.045</strong></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="conclusions-and-recommendations">Conclusions and Recommendations</h2>

<ol>
  <li><strong>Few-Shot Learning is Surprisingly Effective</strong>
    <ul>
      <li>With just 7 examples, LangExtract achieved 96% of T5‚Äôs performance (0.750 vs 0.784)</li>
      <li>No fine-tuning, no domain-specific training data required</li>
      <li>Dramatically faster inference (103x speedup)</li>
    </ul>
  </li>
  <li><strong>Analysis-Driven Few-Shot Engineering is Powerful</strong>
    <ul>
      <li>Analyzing specific failures revealed 92% of errors came from one pattern</li>
      <li>Creating 5 targeted examples improved relationship F1 by +159%</li>
      <li>This methodology can be repeated iteratively to close the gap with fine-tuned models</li>
    </ul>
  </li>
  <li><strong>Dataset Consistency is Critical</strong>
    <ul>
      <li>Initial experiment 4 results were misleading due to inconsistent test data</li>
      <li>Centralizing dataset loading (dataset_utils.py) ensured fair comparisons</li>
      <li>Lesson: Always use identical test sets when comparing approaches</li>
    </ul>
  </li>
  <li><strong>LLMs Excel at Different Tasks Than Fine-tuned Models</strong>
    <ul>
      <li><strong>LLMs better at</strong>: Entities (0.90+ F1), Attributes (0.90 F1)</li>
      <li><strong>Fine-tuned models better at</strong>: Relationships (0.66 vs 0.45 F1)</li>
      <li>This suggests relationships require more domain-specific knowledge</li>
    </ul>
  </li>
  <li><strong>Speed vs Accuracy Trade-offs</strong>
    <ul>
      <li>T5: Best accuracy, slowest (4.6s/sample)</li>
      <li>Improved LangExtract: Excellent accuracy, extremely fast (0.045s/sample)</li>
      <li>Native Gemini: Worst of both worlds (lower accuracy, slow)</li>
    </ul>
  </li>
</ol>

<h3 id="recommendations-by-use-case">Recommendations by Use Case</h3>

<h4 id="choose-t5-fine-tuned-model-if">Choose <strong>T5 Fine-tuned Model</strong> if:</h4>
<p>‚úÖ You need the absolute best relationship extraction (0.662 F1)</p>

<p>‚úÖ Inference speed is not a constraint</p>

<p>‚úÖ You‚Äôre working with FACTUAL-style data</p>

<p>‚úÖ You have computational resources for model loading</p>

<h4 id="choose-improved-langextract-if">Choose <strong>Improved LangExtract</strong> if:</h4>
<p>‚úÖ You need fast inference (22 samples/second)</p>

<p>‚úÖ You want excellent entity/attribute extraction (0.90 F1)</p>

<p>‚úÖ You need flexibility to adapt to new domains (just change examples)</p>

<p>‚úÖ You want good all-around performance without fine-tuning</p>

<p>‚úÖ <strong>Recommended for most production use cases</strong></p>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>This series of experiments demonstrates that <strong>modern LLMs with few-shot learning can approach the performance of fine-tuned models</strong> while offering dramatic speed advantages and flexibility. The key insight: <strong>analysis-driven few-shot engineering</strong> is a powerful technique for iteratively improving LLM performance on structured extraction tasks.</p>

<p>For scene graph extraction specifically:</p>
<ul>
  <li><strong>Improved LangExtract achieved 96% of T5‚Äôs performance with 103x faster inference</strong></li>
  <li><strong>Adding just 5 targeted examples improved relationship extraction by +159%</strong></li>
  <li><strong>The gap between few-shot and fine-tuning continues to narrow</strong></li>
</ul>

<p>As LLMs continue to improve and few-shot learning techniques become more sophisticated, we expect the gap to close further. For practitioners, this means:</p>
<ul>
  <li><strong>Start with few-shot LLMs</strong> for their speed and flexibility</li>
  <li><strong>Use fine-tuned models</strong> only when you need the absolute best accuracy</li>
  <li><strong>Invest time in analysis-driven example engineering</strong> rather than collecting labeled data for fine-tuning</li>
</ul>

<p>The future of structured extraction is fast, flexible, and increasingly accurate‚Äîpowered by few-shot learning with large language models.</p>

<hr />

<h2 id="appendix-reproduction">Appendix: Reproduction</h2>

<p>All experiments are reproducible using:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install dependencies</span>
uv add FactualSceneGraph langextract anthropic datasets torch pandas numpy matplotlib seaborn python-dotenv scikit-learn

<span class="c"># Run experiments</span>
uv run experiment_1_t5_baseline.py
uv run experiment_2_langextract_poc_batched.py
uv run experiment_3_format_optimization_batched.py
uv run experiment_4_backend_comparison.py
uv run experiment_4b_improved_langextract.py

<span class="c"># Compare all results</span>
uv run compare_all_experiments.py
</code></pre></div></div>

<p>Dataset: <a href="https://huggingface.co/datasets/lizhuang144/FACTUAL_Scene_Graph">FACTUAL Scene Graph Dataset</a></p>

<p>Code: Available in this <a href="https://github.com/dlfelps/semantic-extraction">repository</a></p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="semantic" /><category term="scene-graph" /><category term="captioning" /><category term="langextract" /><category term="t5" /><category term="few-shot" /><category term="llm" /><category term="gemini" /><summary type="html"><![CDATA[The blog post demonstrates that few-shot LLMs can match 96% of fine-tuned model accuracy on scene graph extraction with 103x faster inference.]]></summary></entry><entry><title type="html">Experiment 23</title><link href="https://dlfelps.github.io/2025/10/01/devit.html" rel="alternate" type="text/html" title="Experiment 23" /><published>2025-10-01T00:00:00+00:00</published><updated>2025-10-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/10/01/devit</id><content type="html" xml:base="https://dlfelps.github.io/2025/10/01/devit.html"><![CDATA[<p>The blog post describes a fork of the DE-ViT algorithm that adapts it for few-shot object detection on satellite imagery by using DINOv3 vision transformers pretrained on the SAT-493M dataset, with a target application of detecting objects in the xView dataset.</p>

<h1 id="zero-shot-object-detection-for-overhead-imagery-a-de-vit-adaptation">Zero-Shot Object Detection for Overhead Imagery: A DE-ViT Adaptation</h1>

<h2 id="the-de-vit-approach">The DE-ViT Approach</h2>

<p>DE-ViT (Detection Transformer with Vision Transformers) represents a significant advancement in few-shot object detection, leveraging the powerful feature representations learned by vision transformers. The approach combines a pretrained vision transformer backbone with a region propagation network to enable detection with minimal training examples. The core innovation lies in how DE-ViT propagates information between support (example) images and query (target) images through a sophisticated attention mechanism, allowing the model to generalize to novel object categories with limited data.</p>

<p>The original DE-ViT architecture employs a subspace projection mechanism to align features from the vision transformer backbone, followed by region proposal generation and a region propagation network that refines detections based on support set prototypes. For a comprehensive understanding of the theoretical foundations, architectural details, and empirical validations, readers are directed to the <a href="https://arxiv.org/abs/2309.12969">original research paper</a>.</p>

<p><img src="/assets/images/main-rpropnet.jpg" alt="devit overview" />
<em>Overview of the DE-ViT method</em></p>

<h2 id="overhead-imagery-unique-challenges-and-the-xview-dataset">Overhead Imagery: Unique Challenges and the xView Dataset</h2>

<p>Overhead imagery presents distinct challenges that differentiate it from natural image domains. The most critical challenge is <strong>rotation invariance</strong>: objects in satellite and aerial imagery can appear at arbitrary orientations, unlike natural images where vehicles are typically upright and buildings follow gravity-aligned perspectives. A car photographed from above may appear at any angle from 0 to 360 degrees, and detection systems must recognize it regardless of orientation.</p>

<p>The <strong>xView dataset</strong> has become a cornerstone benchmark for evaluating object detection algorithms in overhead imagery. Released in 2018, xView contains over 1 million object instances across 60 categories in high-resolution satellite imagery, spanning diverse geographic regions and imaging conditions. The dataset was specifically designed to challenge computer vision systems with the complexities of overhead imagery: dense object clustering, extreme scale variation (objects ranging from small vehicles to large buildings), and the aforementioned rotation problem.</p>

<p>Historically, xView has driven innovation in several areas: anchor-free detection methods to handle arbitrary orientations, multi-scale feature pyramids to address the extreme scale variation, and attention mechanisms to manage dense scenes. However, most successful approaches have required extensive training on large labeled datasets, limiting their applicability in scenarios where labeled data is scarce or when adapting to new object categories.</p>

<p><img src="/assets/images/dataset-cover.jpeg" alt="sample xview image" />
<em>Example image from xView with detections</em></p>

<h2 id="novel-contributions-toward-zero-shot-detection">Novel Contributions: Toward Zero-Shot Detection</h2>

<p>This project introduces several key modifications to the original DE-ViT framework, fundamentally shifting from a few-shot to a <strong>zero-shot detection paradigm</strong> optimized for overhead imagery.</p>

<h3 id="1-removal-of-the-region-propagation-network">1. Removal of the Region Propagation Network</h3>

<p>The most significant architectural change is the <strong>elimination of the region propagation component</strong>. While the original DE-ViT uses this network to refine detections through learned interactions between support and query features, this requires training on a base dataset. By removing this component, we transition to a true zero-shot scenario where no training is required. Instead, the system relies entirely on the rich feature representations from the DINOv3 vision transformer, which has been pretrained on 493 million satellite images (SAT-493M dataset). This pretrained knowledge serves as the foundation for detection without any task-specific fine-tuning.</p>

<h3 id="2-differentiated-prototype-generation">2. Differentiated Prototype Generation</h3>

<p>The approach to <strong>prototype generation</strong> differs fundamentally between base and novel categories:</p>

<p><strong>Base Data Prototypes</strong>: For established object categories in the base dataset, prototypes are generated using standard feature extraction and pooling techniques. A large number of examples from each base class is sampled from the training data. Once the DINOv3 features are extracted and pooled, a the KMeans clustering algorithm is used to capture the centers that provide coverage for 75% of the examples. These centers become the prototypes for that base class.</p>

<p><strong>Novel Data Prototypes with Rotation Augmentation</strong>: For novel categories where rotation invariance cannot be assumed, the system employs a <strong>rotation augmentation strategy</strong>. For each image in the few-shot set (e.g. 5), 23 rotations (at 15 degree increments) are created. Next DINOv3 features are calculated and pooled from each of the rotated images. These serve as the novel class prototypes. NOTE: the original orientation of the object is not needed, but at the end of this process it we are guaranteed to have full 360 degree rotation examples.</p>

<h3 id="3-support-vector-machine-ensemble-for-confidence-scoring">3. Support Vector Machine Ensemble for Confidence Scoring</h3>

<p>The final novel component is the integration of a <strong>Support Vector Machine (SVM) ensemble</strong> as a confidence gating mechanism. First and SVM is trained to maximally separate the novel classes, base classes, and background. Given a new image, it is rotated using the same scheme from the novel data prototype yielding 24 total images covering all angles. Finally these 24 samples are classified by the SVM and assigned a confidence corresponding to the highest percentage of class agreement (e.g. if 12 of the 24 examples are assigned to background class then it is classified as background with 50% confidence).</p>

<p>This ensemble approach provides several benefits: it captures non-linear decision boundaries that simple cosine similarity cannot, it reduces false positives by requiring consensus across multiple orientations of the target, and it produces calibrated confidence scores that better reflect true match probability. The SVM ensemble acts as a final filter, ensuring that only high-confidence detections propagate to the final output.</p>

<h2 id="experiment">Experiment</h2>

<p>I did not perform a formal experiment, but here is a rough description of the anecdotal test:</p>
<ul>
  <li>2 prototype base classes that cover +70% of xView‚Äôs dataset (building &amp; bus)</li>
  <li>1 background base class</li>
  <li>1 novel class for testing (excavator)</li>
  <li>5 full view satellite images with known excavators present</li>
</ul>

<h2 id="results">Results</h2>

<p>The results were <em>dissapointing</em>. Despite the favorable results published in the <a href="https://arxiv.org/abs/2309.12969">paper</a>, the author admits on his <a href="https://github.com/mlzxy/devit/tree/main">Github site</a> that:</p>

<blockquote>
  <p>[DeVIT] tends to detect objects that do not belong to the prototypes, especially for retailed products that are not presented in the training data. For example, if you have ‚Äúcan‚Äù, ‚Äúbottle‚Äù, and ‚Äútoy‚Äù in the scene, but you only have ‚Äúcan‚Äù and ‚Äúbottle‚Äù in the class prototypes. The ideal performance is to mark ‚Äútoy‚Äù as background, but DE-ViT tends to detect ‚Äútoy‚Äù as either ‚Äúcan‚Äù or ‚Äúbottle‚Äù.</p>
</blockquote>

<p>I noticed a similar trend. While the approach is promising for tapping into the power of foundation vision models for few-shot learning, it fails to provide good performance at scale for satellite imagery. I didn‚Äôt bother calculating any formal statistics, but you can see for yourself the quality of the classifications below:</p>

<p><a href="/assets/images/31_detections.png"><img src="/assets/images/31_detections_thumb.jpg" alt="Click for full size" /></a></p>

<p><em>(Click to download full resolution) The green boxes are the novel detections (excavators) and blue boxes are classified as one of the base classes. NOTE: the few-shot adaptation did not affect the region proposal network, which was taken from a Faster RCNN model trained on xView, so the bounding box proposals themselves are not part of this evaluation. If you review the contents of the green boxes, however, you will find a low classification accuracy for the excavator class.</em></p>

<h2 id="conclusion">Conclusion</h2>

<p>This project demonstrates some potential improvements for zero-shot object detection in overhead imagery. Given the results, I cannot recommend the approach without additional changes.</p>

<p>Code for this project can be found at <a href="https://github.com/dlfelps/devit-xview">https://github.com/dlfelps/devit-xview</a>.</p>

<h2 id="claude-protips">CLAUDE PROTIPS</h2>

<p>In the last few blog posts I provided examples where CLAUDE needed extra guidance. I thought I would do something different in this post.</p>

<ol>
  <li>PROTIP #1: When working in complex code bases (such as a research level machine learning project), work in small measured steps.</li>
  <li>PROTIP #2: After using CLAUDE to implement a new feature (or in this case replace an existing module with something else), ask CLAUDE to write a simple script to demonstrate/test the new feature. Optional: treat this as a temporary test - you don‚Äôt have to add it to git.</li>
  <li>PROTIP #3: If there is a problem that ‚Äúfeels small‚Äù and I think I have a good idea how to fix it, I just give CLAUDE the error and CLAUDE will usually fix it.</li>
  <li>PROTIP #4: If there is a problem that ‚Äúfeels big‚Äù and I don‚Äôt understand it or know how to fix it, I ask CLAUDE to write a debugging script to log values up to the point at which the error occurs. Then I review the output. Next I ask CLAUDE to fix the problem and then run the debugging script again. If all goes well and values make sense I rerun the original code.</li>
</ol>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="few-shot" /><category term="dino" /><category term="devit" /><summary type="html"><![CDATA[The blog post describes a fork of the DE-ViT algorithm that adapts it for few-shot object detection on satellite imagery by using DINOv3 vision transformers pretrained on the SAT-493M dataset, with a target application of detecting objects in the xView dataset.]]></summary></entry><entry><title type="html">Experiment 22</title><link href="https://dlfelps.github.io/2025/09/01/cobad.html" rel="alternate" type="text/html" title="Experiment 22" /><published>2025-09-01T00:00:00+00:00</published><updated>2025-09-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/09/01/cobad</id><content type="html" xml:base="https://dlfelps.github.io/2025/09/01/cobad.html"><![CDATA[<p>The blog post introduces CoBAD (Collective Behavior Anomaly Detection), a deep learning approach that detects anomalous group behaviors in human mobility data by analyzing stay points and collective patterns rather than individual trajectories.</p>

<h2 id="introduction-beyond-individual-anomalies">Introduction: Beyond Individual Anomalies</h2>

<p>Traditional anomaly detection in human mobility has predominantly focused on identifying individual outliers‚Äîdetecting when a single person deviates from their typical movement patterns. While effective for personal applications like location privacy or individual behavior analysis, this individual-centric approach misses a critical dimension of human behavior: <strong>collective dynamics</strong>.</p>

<h3 id="the-paradigm-shift-from-individual-to-collective">The Paradigm Shift: From Individual to Collective</h3>

<p>Individual anomaly detection, as established by foundational works like Hawkins (1980) <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> and more recently applied to mobility by Zheng et al. (2008) <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, typically examines single trajectories against learned normal patterns. These approaches excel at detecting when <em>one person</em> visits an unusual location or travels at an atypical time.</p>

<p>However, human behavior is inherently social. Consider these scenarios that individual anomaly detection would miss:</p>

<ul>
  <li><strong>Flash mob gatherings</strong>: Each individual trajectory might appear normal, but the collective convergence at a specific location and time is highly anomalous</li>
  <li><strong>Emergency evacuations</strong>: Individual movements may follow normal route patterns, but the simultaneous mass exodus represents a collective anomaly</li>
  <li><strong>Coordinated activities</strong>: Groups meeting at unusual times or locations that would be undetectable when analyzing individual patterns in isolation</li>
</ul>

<p>This limitation led researchers to explore <strong>collective anomaly detection</strong>‚Äîidentifying when groups of entities exhibit anomalous behavior as a cohesive unit, even if individual behaviors appear normal.</p>

<h3 id="theoretical-foundations">Theoretical Foundations</h3>

<p>The concept of collective anomalies was formalized by Chandola et al. (2009) <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, who distinguished between:</p>

<ul>
  <li><strong>Point anomalies</strong>: Individual data points that deviate from normal patterns</li>
  <li><strong>Contextual anomalies</strong>: Data points that are anomalous in specific contexts but normal otherwise</li>
  <li><strong>Collective anomalies</strong>: Collections of data points that are anomalous when considered together</li>
</ul>

<p>In mobility analysis, collective anomaly detection has gained traction through works like:</p>

<ul>
  <li><strong>Ge et al. (2010)</strong> <sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>: Early work on detecting abnormal crowd behaviors in video surveillance</li>
  <li><strong>Liu et al. (2013)</strong> <sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>: Collective outlier detection in spatial data using density-based approaches</li>
  <li><strong>Araujo et al. (2014)</strong> <sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>: Group anomaly detection in social networks and mobility patterns</li>
</ul>

<p>However, existing collective anomaly detection methods face significant challenges when applied to large-scale human mobility data:</p>

<ol>
  <li><strong>Scalability</strong>: Many algorithms struggle with the volume and dimensionality of modern mobility datasets</li>
  <li><strong>Temporal dynamics</strong>: Most approaches fail to capture the evolving nature of collective behaviors over time</li>
  <li><strong>Spatial complexity</strong>: Urban mobility involves complex spatial relationships that simple clustering approaches miss</li>
  <li><strong>Ground truth</strong>: Validating collective anomalies is inherently more challenging than individual anomalies</li>
</ol>

<hr />

<h2 id="cobad-a-novel-approach-to-collective-behavior-anomaly-detection">CoBAD: A Novel Approach to Collective Behavior Anomaly Detection</h2>

<p><strong>CoBAD (Collective Behavior Anomaly Detection)</strong> addresses these challenges through a fundamentally different approach that focuses on <strong>stay point events</strong> and <strong>collective behavior modeling</strong> using deep learning techniques.</p>

<h3 id="core-innovation-stay-point-centric-analysis">Core Innovation: Stay Point-Centric Analysis</h3>

<p>Unlike traditional trajectory-based approaches that analyze continuous movement paths, CoBAD operates on <strong>stay points</strong>‚Äîlocations where individuals or groups remain for significant durations. This paradigm shift offers several advantages:</p>

<ol>
  <li><strong>Semantic relevance</strong>: Stay points correspond to meaningful activities (meetings, events, gatherings)</li>
  <li><strong>Noise reduction</strong>: Filters out transitional movements that may not represent intentional collective behavior</li>
  <li><strong>Computational efficiency</strong>: Reduces data dimensionality while preserving behavioral significance</li>
  <li><strong>Collective focus</strong>: Natural aggregation point for identifying group behaviors</li>
</ol>

<h3 id="algorithm-architecture">Algorithm Architecture</h3>

<p>CoBAD employs a multi-stage pipeline that transforms raw trajectory data into collective behavior patterns:</p>

<h4 id="stage-1-stay-point-extraction">Stage 1: Stay Point Extraction</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Raw trajectory data ‚Üí Stay point events
Features: (user_id, location, duration, temporal_context, weekend_flag)
</code></pre></div></div>

<p>The system converts GPS trajectories into discrete stay events, capturing not just <em>where</em> people are, but <em>how long</em> they stay and <em>when</em> these stays occur.</p>

<h4 id="stage-2-spatial-temporal-clustering">Stage 2: Spatial-Temporal Clustering</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Stay points ‚Üí Collective behaviors via DBSCAN
Spatial proximity + Temporal alignment ‚Üí Group identification
</code></pre></div></div>

<p>Using density-based clustering (DBSCAN), CoBAD identifies groups of people who are:</p>
<ul>
  <li><strong>Spatially co-located</strong> (within configurable distance threshold)</li>
  <li><strong>Temporally synchronized</strong> (occurring in the same time window)</li>
  <li><strong>Sufficiently numerous</strong> (meeting minimum collective size threshold)</li>
</ul>

<h4 id="stage-3-collective-feature-engineering">Stage 3: Collective Feature Engineering</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Group clusters ‚Üí 8-dimensional feature vectors
Features: [time_window, center_x, center_y, spread_x, spread_y, 
          relative_size, weekend_ratio, avg_stay_duration]
</code></pre></div></div>

<p>Each collective behavior is characterized by an 8-dimensional feature vector capturing:</p>
<ul>
  <li><strong>Temporal context</strong>: When the collective behavior occurs</li>
  <li><strong>Spatial characteristics</strong>: Location and spatial spread of the group</li>
  <li><strong>Group dynamics</strong>: Size and composition metrics</li>
  <li><strong>Activity patterns</strong>: Duration and weekend/weekday distinctions</li>
</ul>

<h4 id="stage-4-paper-accurate-cobad-architecture">Stage 4: Paper-Accurate CoBAD Architecture</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Individual trajectories ‚Üí Cross-time attention ‚Üí Graph transformer ‚Üí Anomaly scoring
Features: [norm_x, norm_y, is_weekend, start_time, duration, placeholder] (6D)
</code></pre></div></div>

<p>The paper-accurate CoBAD implementation employs a sophisticated neural architecture:</p>
<ol>
  <li><strong>Cross-time attention</strong>: Captures temporal dependencies within individual trajectory sequences</li>
  <li><strong>Graph transformer</strong>: Models collective behaviors through cross-people attention mechanisms</li>
  <li><strong>Dual reconstruction</strong>: Reconstructs both event attributes and co-occurrence link patterns</li>
  <li><strong>Masked pretraining</strong>: Uses BERT-style masking for robust pattern learning</li>
  <li><strong>Multi-component scoring</strong>: Combines learned patterns, event reconstruction, and link reconstruction errors</li>
</ol>

<h3 id="what-makes-cobad-unique">What Makes CoBAD Unique</h3>

<h4 id="1-multi-resolution-anomaly-analysis">1. <strong>Multi-Resolution Anomaly Analysis</strong></h4>
<p>The paper-accurate CoBAD provides detailed anomaly interpretation through <strong>eight trajectory-level subscore types</strong>:</p>

<ul>
  <li><strong>Spatial dispersion</strong>: Unusual location variance within trajectories</li>
  <li><strong>Temporal pattern</strong>: Rare timing patterns (weekend vs weekday, start times)</li>
  <li><strong>Duration anomaly</strong>: Atypical stay duration patterns</li>
  <li><strong>Mobility anomaly</strong>: Unusual movement patterns and distances</li>
  <li><strong>Length anomaly</strong>: Trajectory complexity outliers</li>
  <li><strong>Event reconstruction</strong>: Neural network event attribute reconstruction errors</li>
  <li><strong>Link reconstruction</strong>: Co-occurrence pattern reconstruction errors</li>
  <li><strong>Pattern learned</strong>: Learned anomaly patterns from the neural scorer</li>
</ul>

<p><img src="/assets/images/cobad_radar.jpeg" alt="Radar chart" />
<em>Subscore radar chart showing breakdown of top anomalies</em></p>

<h4 id="2-dynamic-threshold-adaptation">2. <strong>Dynamic Threshold Adaptation</strong></h4>
<p>CoBAD implements multiple robust threshold adjustment methods to handle dataset distribution shifts:</p>

<ul>
  <li><strong>Percentile-based</strong>: Maintains target anomaly rates (default 5%) across datasets</li>
  <li><strong>Statistical outlier</strong>: Mean + 2œÉ approach for statistical anomaly boundaries</li>
  <li><strong>IQR-based</strong>: Q3 + 1.5√óIQR for robust outlier detection</li>
  <li><strong>Adaptive</strong>: Automatically uses median of all three methods and detects distribution shifts</li>
  <li><strong>Test-time adaptation</strong>: Recalculates thresholds when significant distribution shifts are detected</li>
</ul>

<h4 id="3-trajectory-level-pattern-analysis">3. <strong>Trajectory-Level Pattern Analysis</strong></h4>
<p>Each detected anomaly includes detailed pattern analysis explaining the anomalous behavior:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example trajectory anomaly breakdown:
üö® RANK 1 - TRAJECTORY #4521:
üìä Anomaly Score: 42.3847
üìè Trajectory Length: 18 points
üó∫Ô∏è  Spatial Range: X[0.234, 0.891], Y[0.123, 0.567]  
üìÖ Weekend Activity: 85.2%
‚è±Ô∏è  Average Duration: 0.82

üîç Sub-score Breakdown:
   spatial_dispersion  :  8.2341 (98.2nd percentile)
   temporal_pattern   :  6.7834 (94.1st percentile)
   mobility_anomaly    :  5.9123 (91.5th percentile)

üéØ Key anomaly drivers: Extreme spatial dispersion, High temporal pattern
</code></pre></div></div>

<p>This detailed analysis enables analysts to understand <em>why</em> specific trajectories are flagged as anomalous and what makes them unusual.</p>

<h4 id="4-embedding-space-analysis-and-clustering">4. <strong>Embedding Space Analysis and Clustering</strong></h4>
<p>The paper-accurate CoBAD performs sophisticated embedding space analysis:</p>
<ul>
  <li><strong>Dimensionality reduction</strong>: PCA and t-SNE visualization of trajectory embeddings</li>
  <li><strong>Trajectory clustering</strong>: K-means and DBSCAN clustering of learned embeddings</li>
  <li><strong>Cluster characterization</strong>: Analysis of anomaly rates and patterns per cluster</li>
  <li><strong>Cross-cluster anomaly analysis</strong>: Understanding how different trajectory types produce different anomaly patterns</li>
</ul>

<p><img src="/assets/images/cobad_tsne.jpeg" alt="t-SNE chart" />
<em>t-SNE visualization showing trajectory clusters colored by anomaly scores</em></p>

<h3 id="scalability-and-performance">Scalability and Performance</h3>

<p>The paper-accurate CoBAD is designed for large-scale trajectory datasets with CPU-friendly optimizations:</p>

<ul>
  <li><strong>Mini-batch processing</strong>: Configurable batch sizes (default 32) for memory-constrained systems</li>
  <li><strong>Lazy loading</strong>: Efficient data streaming without loading entire datasets into memory</li>
  <li><strong>Batched inference</strong>: Both training and inference use batched processing to prevent memory overflow</li>
  <li><strong>Dynamic batching</strong>: Automatic progress reporting and memory cleanup during processing</li>
  <li><strong>Checkpointing</strong>: Automatic model saving with best model tracking and early stopping</li>
  <li><strong>Validation splitting</strong>: Built-in train/validation splits with adaptive threshold setting</li>
</ul>

<hr />

<h2 id="case-study-anomaly-detection-in-urban-mobility">Case Study: Anomaly Detection in Urban Mobility</h2>

<p>To demonstrate CoBAD‚Äôs capabilities, we present results from analyzing the <strong>LYMob-4Cities dataset</strong> <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>‚Äîa comprehensive mobility dataset containing trajectory data from four Japanese metropolitan areas over a 75-day period.</p>

<h3 id="dataset-characteristics">Dataset Characteristics</h3>
<ul>
  <li><strong>Scale</strong>: 151,000+ individuals across 4 cities</li>
  <li><strong>Temporal coverage</strong>: 75 days with 30-minute intervals</li>
  <li><strong>Spatial resolution</strong>: 500m √ó 500m grid cells (200√ó200 grid)</li>
  <li><strong>Total records</strong>: 111+ million location records</li>
</ul>

<h3 id="key-findings">Key Findings</h3>

<p><img src="/assets/images/cobad_temporal.jpeg" alt="temporal chart" />
<em>Interactive dashboard showing temporal distribution of anomalies</em></p>

<h4 id="anomaly-detection-results">Anomaly Detection Results</h4>
<p>CoBAD successfully identified <strong>20 collective anomalies</strong> from <strong>393 collective behaviors</strong> (5.09% anomaly rate), revealing several interesting patterns:</p>

<p><strong>Top Anomaly Categories:</strong></p>
<ol>
  <li><strong>Duration anomalies</strong> (mean score: 2.11 ¬± 3.40): Groups with extremely long or short stay durations</li>
  <li><strong>Temporal anomalies</strong> (mean score: 0.94 ¬± 0.01): Collective activities at unusual hours</li>
  <li><strong>Spatial anomalies</strong> (mean score: 0.88 ¬± 0.19): Groups gathering in rare locations</li>
</ol>

<p><img src="/assets/images/cobad_spatial.jpeg" alt="spatial chart" />
<em>Plot showing spatial distribution of detected anomalies</em></p>

<h4 id="detailed-anomaly-analysis">Detailed Anomaly Analysis</h4>

<p><strong>Rank 1 Anomaly</strong> - Score: 29.27</p>
<ul>
  <li><strong>Location</strong>: (0.023, 0.550) - Peripheral urban area</li>
  <li><strong>Time</strong>: 5:00 AM - Off-peak collective activity</li>
  <li><strong>Duration</strong>: 0.84 time units - Extended stay duration</li>
  <li><strong>Primary contributors</strong>:
    <ul>
      <li>Stay duration (16.68) - Extremely long collective gathering</li>
      <li>Temporal pattern (11.71) - Very unusual timing</li>
      <li>Spatial spread (10.71) - Unusual spatial arrangement</li>
    </ul>
  </li>
</ul>

<p><strong>Rank 2 Anomaly</strong> - Score: 5.41</p>
<ul>
  <li><strong>Location</strong>: (0.458, 0.041) - Urban edge location</li>
  <li><strong>Time</strong>: 2:00 PM - Moderate temporal anomaly</li>
  <li><strong>Primary contributors</strong>:
    <ul>
      <li>Spatial location (6.75) - Rare meeting location</li>
      <li>Stay duration (6.59) - Unusual duration pattern</li>
      <li>Weekend activity (5.71) - Unexpected weekday pattern</li>
    </ul>
  </li>
</ul>

<h4 id="link-analysis-results">Link Analysis Results</h4>
<p>The network analysis revealed:</p>
<ul>
  <li><strong>8 anomaly communities</strong>: Distinct groups of related anomalous behaviors</li>
  <li><strong>30 connections</strong>: Links between similar anomalous patterns</li>
  <li><strong>Largest community</strong>: 6 related anomalies suggesting coordinated activities</li>
</ul>

<p><img src="/assets/images/cobad_network.jpeg" alt="cluster chart" />
<em>Network graph showing anomaly relationships and communities</em></p>

<h3 id="feature-attribution-insights">Feature Attribution Insights</h3>

<p>Across all detected anomalies, the most significant contributing features were:</p>
<ol>
  <li><strong>Average stay duration</strong> (2.36): Unusual activity lengths</li>
  <li><strong>Weekend ratio</strong> (3.17): Unexpected temporal patterns</li>
  <li><strong>Spatial coordinates</strong> (2.54-3.55): Rare location choices</li>
  <li><strong>Temporal windows</strong> (2.83): Off-peak collective activities</li>
</ol>

<p>These patterns suggest that anomalous collective behaviors are primarily characterized by <strong>temporal unusualness</strong> (when and how long) rather than just spatial rareness.</p>

<hr />

<h2 id="implementation-and-reproducibility">Implementation and Reproducibility</h2>

<p>CoBAD is implemented in Python using modern machine learning frameworks and is designed for reproducibility and extensibility:</p>

<h3 id="technical-stack">Technical Stack</h3>
<ul>
  <li><strong>PyTorch</strong>: Deep learning framework for transformer-based architecture</li>
  <li><strong>Scikit-learn</strong>: Dimensionality reduction, clustering, and preprocessing utilities</li>
  <li><strong>Plotly</strong>: Interactive visualization dashboards with trajectory analysis</li>
  <li><strong>NumPy/Pandas</strong>: Efficient data processing and manipulation</li>
  <li><strong>tqdm</strong>: Progress tracking for long-running trajectory analysis</li>
</ul>

<h3 id="key-outputs">Key Outputs</h3>
<p>The CoBAD analysis pipeline generates comprehensive outputs for further investigation:</p>

<h4 id="1-interactive-visualizations">1. Interactive Visualizations</h4>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">trajectory_overview_dashboard.html</code></strong>: 6-panel comprehensive analysis with score distributions, subscore correlations, temporal patterns, and spatial distributions</li>
  <li><strong><code class="language-plaintext highlighter-rouge">trajectory_embedding_analysis.html</code></strong>: t-SNE, PCA, and clustering visualizations of trajectory embeddings</li>
  <li>Real-time filtering and zooming capabilities</li>
  <li>Hover tooltips with detailed trajectory information</li>
</ul>

<h4 id="2-detailed-text-reports">2. Detailed Text Reports</h4>
<ul>
  <li>Console output with comprehensive anomaly reports including:
    <ul>
      <li>Top-N trajectory anomalies with detailed breakdowns</li>
      <li>Subscore analysis and percentile rankings</li>
      <li>Pattern analysis with human-readable explanations</li>
      <li>Cluster analysis with anomaly rates per cluster</li>
    </ul>
  </li>
</ul>

<h4 id="3-numerical-data-exports">3. Numerical Data Exports</h4>
<ul>
  <li><strong>Anomaly scores</strong>: <code class="language-plaintext highlighter-rouge">anomaly_scores.npy</code></li>
  <li><strong>Binary classifications</strong>: <code class="language-plaintext highlighter-rouge">anomaly_labels.npy</code></li>
  <li><strong>Pattern scores</strong>: <code class="language-plaintext highlighter-rouge">pattern_scores.npy</code></li>
  <li><strong>Reconstruction errors</strong>: <code class="language-plaintext highlighter-rouge">event_recon_errors.npy</code>, <code class="language-plaintext highlighter-rouge">link_recon_errors.npy</code></li>
  <li><strong>Embeddings</strong>: <code class="language-plaintext highlighter-rouge">embeddings.npy</code></li>
  <li><strong>Subscore data</strong>: <code class="language-plaintext highlighter-rouge">subscore_*.npy</code> files for each subscore type</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>CoBAD represents a significant advancement in collective anomaly detection for human mobility data. By shifting focus from individual trajectories to collective behaviors and employing deep learning techniques for pattern recognition, CoBAD enables the detection of complex group anomalies that traditional methods miss.</p>

<p>The approach‚Äôs strength lies in its comprehensive analysis framework‚Äîproviding not just binary anomaly classifications, but detailed explanations, feature attributions, and relationship networks that enable deeper understanding of anomalous collective behaviors.</p>

<p>As urban populations grow and mobility data becomes increasingly available, tools like CoBAD become essential for understanding and responding to collective human behavior patterns in smart city environments.</p>

<hr />

<p><em>Want to explore the code? Check out the <a href="https://github.com/dlfelps/cobad">GitHub repository</a> for complete implementation details, documentation, and examples.</em></p>

<h2 id="claude-gotchas">Claude Gotchas</h2>

<ol>
  <li>ISSUE - Claude initially implemented an autoencoder-based version of the CoBAD algorithm. Although it trained, and produced results, it did not implement the approach as described in the paper. <strong>RESOLUTION - I described the correct implementation using text from the CoBAD paper. Claude then made a faithful reimplementation of the attention-based model.</strong></li>
  <li>ISSUE - Claude produced an optimized, but incorrect version of my data-loader. I implemented the original data-loader manually, however when I scaled to +1M point CSV files my implementation was too slow. I asked Claude to optimize the loading function for performance. It produced a fast loading function, but it did not produce equivalent output to my function. <strong>RESOLUTION -  I manually inspected the output of the new loading function and noticed that the output was incorrect. I then asked Claude to produce a unit-test to verify the equivalence of the new function with my function. It did and then when it failed the test it fixed the optimized function.</strong></li>
  <li>ISSUE - Claude suggested a naive way to split the training/test data that was not optimal for the approach. When questioned it was too agreeable. <strong>RESOLUTION - I asked Claude about sorting the data before performing a split. It suggested the best way was to sort by time. I replied that that wouldn‚Äôt test the approach against unknown users and that withholding never-before seen users would produce a stronger test case. It agreed and implemented the latter.</strong></li>
  <li>ISSUE - Claude littered the code with import statements throughout the code. <strong>RESOLUTION - It is preferred style to have all import statements at the top of the file. Although it works to import code in the middle of a function it looks quite messy and it also imports the code each time the function is called. I asked Claude to move all import statements to the top and it did.</strong></li>
</ol>

<h2 id="future-directions-and-applications">Future Directions and Applications</h2>

<h3 id="potential-applications">Potential Applications</h3>

<p><strong>Urban Planning</strong>: Identifying unusual gathering patterns that may indicate:</p>
<ul>
  <li>Infrastructure bottlenecks requiring attention</li>
  <li>Emergency situation responses</li>
  <li>Public event impact assessment</li>
</ul>

<p><strong>Public Safety</strong>: Detecting coordinated activities that warrant investigation:</p>
<ul>
  <li>Unusual crowd formations</li>
  <li>Synchronized movement patterns</li>
  <li>Off-hours collective activities</li>
</ul>

<p><strong>Transportation Analysis</strong>: Understanding collective mobility patterns:</p>
<ul>
  <li>Mass transit disruption impacts</li>
  <li>Event-driven mobility changes</li>
  <li>Seasonal or periodic collective behavior variations</li>
</ul>

<h3 id="technical-enhancements">Technical Enhancements</h3>

<p><strong>Streaming Anomaly Detection</strong>: Extending CoBAD for real-time analysis of incoming mobility streams with concept drift adaptation.</p>

<p><strong>Multi-Modal Integration</strong>: Incorporating additional data sources (social media, weather, events) to improve context understanding.</p>

<p><strong>Hierarchical Anomaly Detection</strong>: Developing multi-scale approaches that detect anomalies at different spatial and temporal resolutions simultaneously.</p>

<p><strong>Causal Anomaly Analysis</strong>: Moving beyond correlation to understand causal relationships between anomalous collective behaviors and external factors.</p>

<h2 id="references">References</h2>

<hr />

<p><em>This research contributes to the growing field of collective behavior analysis in urban computing and demonstrates the potential of deep learning approaches for understanding complex mobility patterns in smart city applications.</em></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Hawkins, D. M. (1980). <em>Identification of Outliers</em>. Chapman and Hall.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Zheng, Y., Zhang, L., Xie, X., &amp; Ma, W. Y. (2009). Mining interesting locations and travel sequences from GPS trajectories. In <em>Proceedings of the 18th international conference on World wide web</em> (pp. 791-800).¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Chandola, V., Banerjee, A., &amp; Kumar, V. (2009). Anomaly detection: A survey. <em>ACM computing surveys</em>, 41(3), 1-58.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Ge, W., Collins, R. T., &amp; Ruback, R. B. (2012). Vision-based analysis of small groups in pedestrian crowds. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 34(5), 1003-1016.¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008). Isolation forest. In <em>2008 eighth ieee international conference on data mining</em> (pp. 413-422).¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Araujo, M., G√ºnnemann, S., Mateos, G., &amp; Christakis, N. A. (2014). Discrete signal processing on graphs: Frequency analysis. <em>IEEE transactions on signal processing</em>, 62(12), 3042-3054.¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>LYMob-4Cities Dataset. (2024). Available at: https://zenodo.org/records/14219563¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="gps" /><category term="mobility" /><category term="cobad" /><summary type="html"><![CDATA[The blog post introduces CoBAD (Collective Behavior Anomaly Detection), a deep learning approach that detects anomalous group behaviors in human mobility data by analyzing stay points and collective patterns rather than individual trajectories.]]></summary></entry><entry><title type="html">Experiment 21</title><link href="https://dlfelps.github.io/2025/08/21/mobility.html" rel="alternate" type="text/html" title="Experiment 21" /><published>2025-08-21T00:00:00+00:00</published><updated>2025-08-21T00:00:00+00:00</updated><id>https://dlfelps.github.io/2025/08/21/mobility</id><content type="html" xml:base="https://dlfelps.github.io/2025/08/21/mobility.html"><![CDATA[<p>This post converts GPS trajectory data into learned embedded features to reveal human mobility patterns, enabling applications in urban planning, personalized services, and behavioral analysis.</p>

<h1 id="introduction">Introduction</h1>

<p>Human mobility patterns tell fascinating stories. Where we go, when we travel, and how we move through space reveals insights about urban planning, social behavior, economic activity, and even public health. But extracting meaningful patterns from raw GPS trajectories is challenging‚Äîuntil now.</p>

<p>In this post, I‚Äôll walk you through an exciting deep learning project that transforms complex mobility data into intelligent embeddings using a hybrid transformer-LSTM architecture. We‚Äôll explore how this approach can revolutionize everything from urban planning to personalized location services.</p>

<h2 id="the-challenge-making-sense-of-mobility-data">The Challenge: Making Sense of Mobility Data</h2>

<p>GPS trajectories are deceptively complex. A single user‚Äôs movement data contains:</p>
<ul>
  <li><strong>Spatial information</strong>: Latitude and longitude coordinates</li>
  <li><strong>Temporal patterns</strong>: When movements occur, day-of-week effects</li>
  <li><strong>Sequential dependencies</strong>: How current location influences future destinations</li>
  <li><strong>Individual behaviors</strong>: Personal routines and preferences</li>
</ul>

<p>Traditional analysis methods struggle to capture these multi-dimensional relationships. That‚Äôs where deep learning comes in.</p>

<h2 id="introducing-lymob-4cities-a-rich-mobility-dataset">Introducing LYMob-4Cities: A Rich Mobility Dataset</h2>

<p>Our project leverages the <strong>LYMob-4Cities dataset</strong>, a comprehensive collection of anonymized mobility traces from four major cities. This dataset provides:</p>

<ul>
  <li><strong>Multi-city coverage</strong>: Diverse urban environments with different characteristics</li>
  <li><strong>Temporal depth</strong>: Extended observation periods capturing seasonal and weekly patterns</li>
  <li><strong>User diversity</strong>: Thousands of anonymized users with varying mobility behaviors</li>
  <li><strong>High resolution</strong>: Detailed GPS coordinates with timestamp information</li>
</ul>

<p>The dataset structure includes:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">uid</code>: Anonymized user identifier</li>
  <li><code class="language-plaintext highlighter-rouge">d</code>: Day number in the observation period</li>
  <li>Geographic coordinates (latitude/longitude)</li>
  <li>Temporal metadata including timestamps and day-of-week indicators</li>
</ul>

<p>This rich dataset enables us to build models that generalize across different urban contexts while respecting user privacy through anonymization.</p>

<h2 id="the-architecture-hybrid-transformer-lstm-for-mobility">The Architecture: Hybrid Transformer-LSTM for Mobility</h2>

<p>Our approach combines the best of both worlds: transformers for attention-based pattern recognition and LSTMs for sequential modeling.</p>

<h3 id="core-components">Core Components</h3>

<p><strong>1. Multi-Modal Embeddings</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Spatial embedding: lat/lon ‚Üí dense vector
</span><span class="n">spatial_emb</span> <span class="o">=</span> <span class="nc">SpatialEmbedding</span><span class="p">(</span><span class="n">coordinates</span><span class="p">)</span>

<span class="c1"># Temporal embedding: time features ‚Üí dense vector  
</span><span class="n">temporal_emb</span> <span class="o">=</span> <span class="nc">TemporalEmbedding</span><span class="p">(</span><span class="n">timestamps</span><span class="p">,</span> <span class="n">weekend_flags</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>2. Cross-Attention Mechanism</strong>
The model learns how spatial and temporal features interact through dedicated attention layers, capturing patterns like ‚Äúthis user typically visits restaurants on Friday evenings.‚Äù</p>

<p><strong>3. Transformer Blocks</strong>
Multi-head self-attention processes the entire trajectory sequence simultaneously, identifying important locations and movement patterns.</p>

<p><strong>4. Bidirectional LSTM</strong>
Sequential processing captures temporal dependencies and generates the final 512-dimensional embedding that summarizes the entire trajectory.</p>

<h3 id="why-this-architecture-works">Why This Architecture Works</h3>

<ul>
  <li><strong>Transformers</strong> excel at identifying important locations through attention</li>
  <li><strong>LSTMs</strong> capture sequential dependencies in movement</li>
  <li><strong>Cross-attention</strong> learns spatio-temporal interactions</li>
  <li><strong>Bidirectional processing</strong> considers both past and future context</li>
</ul>

<h2 id="the-complete-pipeline">The Complete Pipeline</h2>

<h3 id="stage-1-model-training">Stage 1: Model Training</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train_s2s.py
</code></pre></div></div>

<p>The training process:</p>
<ul>
  <li>Loads mobility data from multiple cities</li>
  <li>Applies data augmentation (Gaussian noise) for robustness</li>
  <li>Trains the hybrid model with early stopping</li>
  <li>Saves the best checkpoint for inference</li>
</ul>

<p><strong>Key Innovation</strong>: The model uses only users with ID &lt; 4000 for training, reserving higher IDs for testing‚Äîensuring true out-of-sample evaluation.</p>

<h3 id="stage-2-embedding-generation">Stage 2: Embedding Generation</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python inference.py
</code></pre></div></div>

<p>This stage:</p>
<ul>
  <li>Processes test users to generate trajectory embeddings</li>
  <li>Applies PCA to reduce dimensionality for visualization</li>
  <li>Creates analysis outputs including scatter plots and statistical summaries</li>
</ul>

<p><strong>Technical Detail</strong>: The 512-dimensional embeddings come from a bidirectional LSTM with 256 hidden units, effectively doubling the representation capacity.</p>

<h3 id="stage-3-interactive-exploration">Stage 3: Interactive Exploration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python interactive_plot.py
</code></pre></div></div>

<p>The crown jewel: a Dash web application that enables:</p>
<ul>
  <li>Interactive exploration of embedding space</li>
  <li>Point selection and trajectory heatmap generation</li>
  <li>Real-time analysis of mobility patterns</li>
</ul>

<p><img src="/assets/images/Screenshot_20-8-2025_15056_127.0.0.1.jpeg" alt="Interactive Dashboard" />
<em>Interactive dashboard showing PCA projections (left) and trajectory heatmaps (right)</em></p>

<h2 id="remarkable-results-what-the-embeddings-reveal">Remarkable Results: What the Embeddings Reveal</h2>

<p>The PCA analysis reveals fascinating insights:</p>

<p><strong>Clustering by Behavior</strong>: Users with similar mobility patterns cluster together in embedding space, even across different cities.</p>

<p><strong>Spatial Preservation</strong>: Geographic relationships are maintained‚Äîusers who frequent similar areas have similar embeddings.</p>

<p><strong>Temporal Patterns</strong>: Weekend vs. weekday behaviors create distinct embedding regions.</p>

<p><strong>Generalization</strong>: The model successfully captures mobility patterns for completely unseen users.</p>

<h2 id="real-world-applications">Real-World Applications</h2>

<p>This technology opens doors to numerous applications:</p>

<h3 id="Ô∏è-urban-planning">üèôÔ∏è <strong>Urban Planning</strong></h3>
<ul>
  <li><strong>Traffic optimization</strong>: Predict congestion patterns before infrastructure changes</li>
  <li><strong>Public transit</strong>: Design routes based on actual movement patterns</li>
  <li><strong>Zoning decisions</strong>: Understand how people actually use urban spaces</li>
</ul>

<h3 id="-personalized-services">üì± <strong>Personalized Services</strong></h3>
<ul>
  <li><strong>Location recommendations</strong>: ‚ÄúUsers with similar patterns also enjoyed‚Ä¶‚Äù</li>
  <li><strong>Predictive routing</strong>: Anticipate destinations based on historical behavior</li>
  <li><strong>Context-aware apps</strong>: Adapt interfaces based on movement patterns</li>
</ul>

<h3 id="-anomaly-detection">üö® <strong>Anomaly Detection</strong></h3>
<ul>
  <li><strong>Security applications</strong>: Identify unusual movement patterns</li>
  <li><strong>Public health</strong>: Track population mobility during emergencies</li>
  <li><strong>Fraud detection</strong>: Verify location-based claims</li>
</ul>

<h3 id="-business-intelligence">üìä <strong>Business Intelligence</strong></h3>
<ul>
  <li><strong>Retail analytics</strong>: Understand customer foot traffic patterns</li>
  <li><strong>Real estate</strong>: Evaluate locations based on mobility data</li>
  <li><strong>Marketing</strong>: Target audiences based on movement behaviors</li>
</ul>

<h3 id="-research-applications">üî¨ <strong>Research Applications</strong></h3>
<ul>
  <li><strong>Social science</strong>: Study human behavior at scale</li>
  <li><strong>Transportation research</strong>: Validate mobility models</li>
  <li><strong>Economic analysis</strong>: Understand commuting and economic patterns</li>
</ul>

<h2 id="privacy-and-ethics-a-responsible-approach">Privacy and Ethics: A Responsible Approach</h2>

<p>Our implementation prioritizes privacy:</p>
<ul>
  <li><strong>Anonymized data</strong>: No personally identifiable information</li>
  <li><strong>Aggregated insights</strong>: Focus on population-level patterns</li>
  <li><strong>Secure processing</strong>: Local analysis without data sharing</li>
  <li><strong>Transparent methods</strong>: Open-source implementation for scrutiny</li>
</ul>

<h2 id="technical-innovation-why-this-matters">Technical Innovation: Why This Matters</h2>

<p>This project demonstrates several key advances:</p>

<p><strong>1. Multi-Modal Learning</strong>: Successfully combining spatial and temporal information in a unified architecture.</p>

<p><strong>2. Scalable Embeddings</strong>: 512-dimensional representations that capture complex mobility patterns while remaining computationally tractable.</p>

<p><strong>3. Interactive Analysis</strong>: Real-time exploration tools that make complex AI accessible to domain experts.</p>

<p><strong>4. Generalization</strong>: Models that work across different cities and user populations.</p>

<h2 id="getting-started-try-it-yourself">Getting Started: Try It Yourself</h2>

<p>The complete implementation is available with detailed documentation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install dependencies</span>
pip <span class="nb">install </span>torch pandas scikit-learn plotly dash

<span class="c"># Train the model</span>
python train_s2s.py

<span class="c"># Generate embeddings</span>
python inference.py

<span class="c"># Launch interactive dashboard</span>
python interactive_plot.py
</code></pre></div></div>

<p>The modular design makes it easy to:</p>
<ul>
  <li>Adapt to your own mobility datasets</li>
  <li>Experiment with different architectures</li>
  <li>Add new visualization components</li>
  <li>Integrate with existing systems</li>
</ul>

<h2 id="the-future-of-mobility-intelligence">The Future of Mobility Intelligence</h2>

<p>This project represents just the beginning. Future directions include:</p>

<ul>
  <li><strong>Federated learning</strong>: Training on distributed datasets while preserving privacy</li>
  <li><strong>Multi-modal integration</strong>: Incorporating weather, events, and social data</li>
  <li><strong>Real-time processing</strong>: Streaming analysis for live mobility insights</li>
  <li><strong>Causal modeling</strong>: Understanding why people move, not just where</li>
</ul>

<h2 id="conclusion-from-data-to-intelligence">Conclusion: From Data to Intelligence</h2>

<p>Human mobility data contains incredible insights waiting to be unlocked. By combining modern deep learning architectures with thoughtful data science, we can transform raw GPS coordinates into intelligent representations that reveal the hidden patterns of human behavior.</p>

<p>The implications extend far beyond technology‚Äîthis work can inform better cities, more personalized services, and deeper understanding of human society. As we continue to generate more location data through our digital lives, tools like these become essential for making sense of our increasingly connected world.</p>

<p>Whether you‚Äôre an urban planner designing the cities of tomorrow, a data scientist exploring new applications, or simply curious about the intersection of AI and human behavior, this project demonstrates the power of applying modern machine learning to one of humanity‚Äôs most fundamental activities: movement through space and time.</p>

<hr />

<p><em>Want to explore the code? Check out the <a href="https://github.com/dlfelps/lstm-trajectory">GitHub repository</a> for complete implementation details, documentation, and examples.</em></p>

<h2 id="about-the-dataset">About the Dataset</h2>

<p>The LYMob-4Cities dataset used in this project represents a significant contribution to mobility research, providing researchers with access to large-scale, anonymized mobility data from diverse urban environments. This type of dataset is crucial for developing and validating mobility models that can generalize across different cities and cultures.</p>

<p>For researchers interested in working with similar data, consider exploring:</p>
<ul>
  <li>Academic partnerships with transportation authorities</li>
  <li>Open mobility datasets from cities with data sharing initiatives</li>
  <li>Synthetic mobility data generation techniques</li>
  <li>Privacy-preserving data collection methods</li>
</ul>

<p>The responsible use of such datasets, with proper anonymization and ethical oversight, enables breakthrough research while protecting individual privacy‚Äîa balance that‚Äôs essential as we advance mobility intelligence.</p>

<h2 id="claude-gotchas">Claude Gotchas</h2>

<p>I was impressed by Claude Code‚Äôs ability to assist and expedite with the exploration of this dataset and creation of the LSTM model. I wanted to document a few gotchas that I came across during the development process that would have been difficult for a non-developer / non-ML person to identify since they didn‚Äôt cause any errors:</p>

<ol>
  <li>ISSUE - The model was actually predicting the final location from the full trajectory (i.e. including the final location). <strong>RESOLUTION - I identified the problem by when I noticed the model converged quickly and had excellent performance on test datasets. I found the problem, told Claude, and Claude fixed it.</strong></li>
  <li>ISSUE - The LSTM used bad values for padding points. <strong>RESOLUTION - Trajectories are not the same length and the LSTM model requires that you pad short sequences to make them equal. One is supposed to choose padding values that do not appear in the real data. However, Claude chose ‚Äú0‚Äù for user_id, x, y, and time. These are all valid values. I manually changed them to acceptable values.</strong></li>
  <li>ISSUE - The model initially treated the user_id variable as a continuous variable instead of a categorical variable. <strong>RESOLUTION - When I pointed out the mistake Claude had a good suggestion to use an embedding layer and updated the code accordingly.</strong></li>
  <li>ISSUE - Claude chose the wrong normalization algorithm. <strong>RESOLUTION - I noticed that the first two PCA dimensions captured almost 97% of the variance. This was surprising given the complexity of the data. Upon further inspection, I noticed that before performing PCA Claude applied a matrix L2 norm to the trajectory embeddings. The correct normalization method is a zero-mean, unit-variance normalization method. I was able to tell Claude to replace the method.</strong></li>
</ol>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="claude" /><category term="gps" /><category term="mobility" /><summary type="html"><![CDATA[This post converts GPS trajectory data into learned embedded features to reveal human mobility patterns, enabling applications in urban planning, personalized services, and behavioral analysis.]]></summary></entry><entry><title type="html">Experiment 20</title><link href="https://dlfelps.github.io/2024/12/01/scraper.html" rel="alternate" type="text/html" title="Experiment 20" /><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/12/01/scraper</id><content type="html" xml:base="https://dlfelps.github.io/2024/12/01/scraper.html"><![CDATA[<p>This post describes a command line tool I created that scrapes a website and generates reports.</p>

<h1 id="introduction">Introduction</h1>

<p>It is greatly rewarding to build a tool that solves a real problem. The scale of the problem dictates the number of potential users your tool can help. This project solves a small problem - improving the registration process for my son‚Äôs summer camps. The problem is mostly created by a poor website design that doesn‚Äôt allow parents to easily find the correct camps.</p>

<p>I built this tool to scrape the camp data on <a href="https://fairfax.usedirect.com/FairfaxFCPAWeb/Default.aspx">Fairfax County Park Authority</a> (FCPA) website and generate useful camp options. I also added new functionality like calculating the estimated commute times to each camp.</p>

<h2 id="goals">Goals</h2>
<p>The goals of this project are to:</p>
<ul>
  <li>Improve user experience registering for camps hosted by FCPA</li>
  <li>Demonstrate the use of developing a command line tool</li>
  <li>Demonstrate the 3rd party API (Google MAP API)</li>
  <li>Demonstrate report creation</li>
</ul>

<h2 id="fcpa-site-limitations">FCPA site limitations</h2>
<p>Why is this tool needed? Because FCPA offers hundreds of potential camps to register for and their filter options are rather limited. Here is a screenshot of the FCPA camp search site:</p>

<p><img src="/assets/images/fcpa_search.jpeg" alt="" /></p>

<p>At first glance the fields look reasonable. We should be able to narrow down the list of available camps and register for one. Let‚Äôs see what happens if I try to find a camp for my 6 year old son for the two days after New Years Day (Jan 2-3).</p>

<p><img src="/assets/images/fcpa_results.jpeg" alt="" /></p>

<p>Due to the broad age range filter (6-12 yrs old), only one of the first six camps listed in the results are appropriate for a 6 year old. Unfortunately that camp is for the wrong dates (January 20). This happens because the date filter only allows you to specify the date ‚Äústarting on or after‚Äù. So the best I can do is limit results from a starting date to the end of that month. This is especially frustrating when trying to book camps over the summer break. Typically I want to book one camp per week for the entire summer. To do this on the FCPA website I can change the ‚Äústarting on or after‚Äù date, but for the first week in a month I get all camps in that month (which could be more than 100 sessions!).</p>

<h2 id="desired-capabilities">Desired capabilities</h2>

<p>I would like to build a tool that can output a tailored report to facilitate selecting and registring for camps. Ideally I would be able to quickly update the data based on the current openings as classes can fill up quickly. So I need a way to:</p>
<ol>
  <li>Automatically scrape the camp data from the FCPA website</li>
  <li>Select camps by various criteria</li>
  <li>Generate a custom report</li>
</ol>

<h1 id="installation">Installation</h1>
<ol>
  <li>Clone the repo (git clone https://github.com/dlfelps/camp-little-scrape.git)</li>
  <li>Install dependencies (pip install -r requirements.txt)</li>
  <li>Install Chrome (https://www.google.com/chrome/)</li>
  <li>Sign up for a Google Maps API key (https://developers.google.com/maps)</li>
</ol>

<h1 id="usage">Usage</h1>

<h2 id="process-overview">Process overview</h2>

<p>The overall process is described in the flowchart below. I will demonstrate each step below in detail, but at a high level, the tool completes a series of steps to:</p>
<ol>
  <li>Scrape camp links</li>
  <li>Scrape camp details</li>
  <li>(optional) Calculate commute times to each camp</li>
  <li>Filter camps and generate report</li>
</ol>

<p><img src="/assets/images/camp%20scraper-2024-11-21-162211.svg" alt="" /></p>

<h2 id="step-1---scrape-camp-links">Step 1 - Scrape camp links</h2>
<p>The first step in the process uses <a href="https://selenium-python.readthedocs.io/">Selenium</a> to semi-automatically scrape the camp session links from the FCPA website.</p>
<ol>
  <li>launch jupyter lab</li>
  <li>open step1.ipynb</li>
  <li>Follow instructions in step1.ipynb to open remote Chrome browser</li>
  <li>Go to FCPA website and search for ALL camps</li>
  <li>Follow instructions in step1.ipynb to scrape each page</li>
</ol>

<p>This step produces a <code class="language-plaintext highlighter-rouge">links.txt</code> file containing all the camp session links.</p>

<h2 id="step-2---scrape-camp-details">Step 2 - Scrape camp details</h2>
<p>Steps 2-4 all use the command line gui provided by the camp.py file. It is built using <a href="https://typer.tiangolo.com/">Typer</a>. View the list of possible commands by calling <code class="language-plaintext highlighter-rouge">python camp.py --help</code></p>

<p><img src="/assets/images/gui_help.PNG" alt="" /></p>

<p>Step 2 is initiated by calling <code class="language-plaintext highlighter-rouge">python camp.py details</code>. This step scrapes the camp details (e.g. location, dates, times) for each camp found in the <code class="language-plaintext highlighter-rouge">links.txt</code>.</p>

<h2 id="step-3---optional-calculate-commutes">Step 3 - (optional) Calculate commutes</h2>

<p>The third step uses Google Maps API to calculate the typical daily commute to each camp. This calculation is the sum of the following distances between home (user specified) and camp location:</p>
<ul>
  <li>morning dropoff (arriving by 9 AM)</li>
  <li>morning return (departing at 9 AM)</li>
  <li>afternoon pickup (arriving by 4 PM)</li>
  <li>afternooon return (departing at 4 PM)</li>
</ul>

<p>Commute time is one of the most important factors when selecting camps. Since full day camps are only 7 hours, it would not be time efficient to spend 2 hours in the car, which is shockingly easy to do even staying within Fairfax county (where all FCPA camps are located). This step does require a Google Map API key, which can be created at https://developers.google.com/maps. The code retrieves your individial key from the environment variable <code class="language-plaintext highlighter-rouge">MAP_API_KEY</code>. Here is a <a href="https://lazyprogrammer.me/how-to-set-environment-variables-permanently-in-windows-linux-and-mac/">guide</a> if you need help.</p>

<h2 id="step-4---filter-camps-and-generate-report">Step 4 - Filter camps and generate report</h2>

<p>The final step filters the available camps to generate a report. We can see the full list of available options by looking at the report help command <code class="language-plaintext highlighter-rouge">python camp.py report --help</code>.</p>

<p><img src="/assets/images/gui_report_help.PNG" alt="" /></p>

<p>The filtering options for this step are as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Required?</th>
      <th style="text-align: center">Default value</th>
      <th style="text-align: center">Description</th>
      <th style="text-align: center">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">first_day</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">The camp must start on this date</td>
      <td style="text-align: center">‚Äú01/02/2025‚Äù</td>
    </tr>
    <tr>
      <td style="text-align: center">last_day</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">The camp must end on this date</td>
      <td style="text-align: center">‚Äú01/03/2025‚Äù</td>
    </tr>
    <tr>
      <td style="text-align: center">min_age</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">5 (years)</td>
      <td style="text-align: center">The camp must accept kids of this age</td>
      <td style="text-align: center">‚Äìmin-age=5</td>
    </tr>
    <tr>
      <td style="text-align: center">max_commute</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">120 (minutes)</td>
      <td style="text-align: center">The camp must not have an estimated daily commute greater than this many minutes</td>
      <td style="text-align: center">‚Äìmax-commute=120</td>
    </tr>
    <tr>
      <td style="text-align: center">remove_half_day</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">True</td>
      <td style="text-align: center">Half day camps (from 9-noon) are not shown</td>
      <td style="text-align: center">‚Äìremove-half-day</td>
    </tr>
    <tr>
      <td style="text-align: center">remove_schools</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">True</td>
      <td style="text-align: center">Camps located at schools are not shown</td>
      <td style="text-align: center">‚Äìremove-schools</td>
    </tr>
    <tr>
      <td style="text-align: center">show_full</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">False</td>
      <td style="text-align: center">Camps that are already fully registered are not shown</td>
      <td style="text-align: center">‚Äìno-show-full</td>
    </tr>
  </tbody>
</table>

<p>If your desired parameters are different from default, then change them when you call the report command, e.g. <code class="language-plaintext highlighter-rouge">python camp.py report "01/02/2025" "01/03/2025" --min-age=7</code>.</p>

<p>This will produce an HTML file in the reports directory with all matching camps:</p>

<p><img src="/assets/images/report.PNG" alt="" /></p>

<p>The report is generated using the <a href="https://www.yattag.org/">yattag</a> library to procedurally generate the HTML report from python code.
A few highlights of the report:</p>
<ul>
  <li>The camps are sorted by shortest commute first</li>
  <li>The report highlights which camps include a swimming break</li>
  <li>Each camp includes a session link for easy registration</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>Overall I found this side project to be quite rewarding. I learned how to:</p>
<ul>
  <li>scrape data from websites (selenium and httpx)</li>
  <li>parse structured data from websites (beautifulsoup)</li>
  <li>keep API keys secret using environment variables</li>
  <li>use 3rd party APIs</li>
  <li>generate HTML from Python</li>
</ul>

<p>Finally I found the tool to actually make finding and registering my son for camp much easier!</p>

<h2 id="possible-improvements">Possible improvements</h2>

<p>I was not able to find a way for <a href="https://typer.tiangolo.com/">Typer</a> to hide commands that were not yet applicable. Ideally, only the <code class="language-plaintext highlighter-rouge">details</code> command would be initially available (taking in the links.txt that was produced during the semi-manual scraping step). Then after creating ‚Äúcamps.pkl‚Äù and ‚Äúplaces.txt‚Äù the <code class="language-plaintext highlighter-rouge">commute</code> command would be available. Finally the <code class="language-plaintext highlighter-rouge">report</code> command could be used.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="scraper" /><category term="cli" /><category term="selenium" /><category term="httpx" /><category term="yattag" /><category term="typer" /><category term="backend" /><summary type="html"><![CDATA[This post describes a command line tool I created that scrapes a website and generates reports.]]></summary></entry><entry><title type="html">Experiment 19</title><link href="https://dlfelps.github.io/2024/08/01/generative.html" rel="alternate" type="text/html" title="Experiment 19" /><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/08/01/generative</id><content type="html" xml:base="https://dlfelps.github.io/2024/08/01/generative.html"><![CDATA[<p>This post describes the stable diffusion architecture for generative AI.</p>

<h1 id="introduction">Introduction</h1>

<p>I am going to introduce this post with a real conversation I had with Microsoft‚Äôs <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you explain Generative AI to me like I am 5 years old?</p>

  <p>Copilot:</p>

  <p>Certainly! Imagine you have a magical drawing book. When you open it, it creates new pictures all by itself! That‚Äôs what generative AI does‚Äîit makes up things like drawings, stories, or even songs without anyone telling it exactly what to create. It‚Äôs like having a creative friend who invents cool stuff just by thinking about it! üé®‚ú®</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Are you an example of a Generative AI?</p>

  <p>Copilot:</p>

  <p>Indeed! I‚Äôm like a friendly digital artist. When you ask me for something, I use my magical algorithms to create new sentences, poems, or even drawings. Just like a painter mixes colors to create beautiful pictures, I mix words and ideas to make interesting content! üé®‚ú®</p>

  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you generate an image of an Eastern Bluebird drinking a milkshake?</p>

  <p>Copilot:</p>

  <p><img src="/assets/images/bluebird_milkshake_dalle.PNG" alt="bluebird milkshake" /></p>
</blockquote>

<p>END OF CONVERSATION</p>

<p>I was impressed by the generated images. Behind the scenes, Copilot uses an OpenAI product called <a href="https://openai.com/dall-e-3/">DALLE-3</a> to generate images. Unfortunately DALLE-3 is not publicly available. So, for the remainder of this post, we will instead examine a closely related model called Stable Diffusion. How do they compare? Here is the output of Stable Diffusion for the same prompt above:</p>

<p><img src="/assets/images/bluebird_milkshake_stable.webp" alt="stable diffusion" width="400" /></p>

<p>Generally Stable Diffusion tends to produce images that are more photorealistic whereas DALLE-3 creates images that feel like computer-generated art.</p>

<h1 id="unconditional-image-generation-denoising-diffusion-probabilistic-models">Unconditional Image Generation (Denoising Diffusion Probabilistic Models)</h1>

<p>I will explain Stable Diffusion through a series of increasingly complex models. The first - and most foundational model -  is the Denoising Diffusion Probabilistic Model (DDPM), which serves as the core technology behind Stable Diffusion and DALLE-3.</p>

<p>DDPMs are a class of generative models that work by iteratively denoising a diffusion process, which involves adding noise to an image and then trying to remove it. The underlying theory of DDPMs is based on the idea that transforming a simple distribution, such as a Gaussian, through a series of diffusion steps can result in a complex and expressive image data distribution. This allows the model to generate new images by reversing the diffusion process, starting from the full Gaussian distribution and ending up with the image distribution.</p>

<p><img src="https://huggingface.co/blog/assets/78_annotated-diffusion/diffusion_figure.png" alt="forward diffusion" /></p>

<p>To further demonstrate the contribution of each step I will show what kind of generative images are possible using the CUB-2011 dataset. Our final goal will be to produce fully synthetic birds. In this first step we will only be able to generate small bird-like shapes using the DDPM architecture alone.</p>

<p><img src="/assets/images/generative_figure1.png" alt="ddpm" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The output of training the model is shown below. Here I take a snapshot of the sample images every 10 epochs to show the learning progression of the model.</p>

<p><img src="/assets/images/ddpm-64.gif" alt="ddpm animation" /></p>

<p>You can see birds if you squint, but the low resolution (64x64) makes these images less useful. DDPMs top out around (256x256) and become increasingly slow to train on a single GPU. We next explore two options for increasing the resolution.</p>

<h1 id="generating-higher-resolution-images">Generating Higher Resolution Images</h1>

<p>The following two sections offer alternatives to increasing the resolution of the generated images.</p>

<h2 id="upscaling">Upscaling</h2>

<p>The first approach uses image upscaling to increase the resolution of an image. Traditional upscaling methods, such as interpolation, don‚Äôt increase the inherent resolution of the image. The upscaler used here actually fills in missing details to increase resolution (NOTE: the upscaler itself is based on stable diffusion architecture). The architecture for the approach is outlined below:</p>

<p><img src="/assets/images/generative_figure2.png" alt="upscaling" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>The benefit of this approach is that it uses the same DDPM from step 1 (which produces images of 64x64) and then the upscaler increases the resolution to 256x256.</p>

<p>The disadvantage of this approach is that the upscaler is not tuned specifically for your data (although it could be with additional training). It also makes a slow process (sampling the DDPM) even slower because you are performing two indepdenent steps (sampling then upscaling).</p>

<h2 id="latent-model">Latent model</h2>

<p>The second approach to increasing resolution was introduced in <a href="https://arxiv.org/abs/2112.10752">‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models‚Äù</a>. The idea behind this approach is to surround the DDPM with a pretrained variational autoencoder<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. The DDPM then learns to denoise the latent space (which also happens to be 64x64) instead of the image space. To create a new sample:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. generate random noise in the shape of the latent space
2. use the DDPM to denoise the input
3. use the decoder to transform the latent space back into image space (512x512)
</code></pre></div></div>

<p><img src="/assets/images/generative_figure3.png" alt="latent" width="400" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Unfortunately I did not have enough Colab credits to fully train this variant<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Although the model outputs images that are 512x512, I admit that they don‚Äôt look very bird-like. Here is a single full image created after 50 epochs of training:</p>

<p><img src="/assets/images/latent_bird.png" alt="latent image" /></p>

<h1 id="conditional-image-generation-stable-diffusion">Conditional Image Generation (Stable Diffusion)</h1>

<p>Up to this point we have not had any control over what kind of bird gets synthesized. The final step is to condition the DDPM on another input (usually text, but it could also be another image). NOTE: In order to train such a model, the training data would require both text and captions. The CUB-2011 dataset does not provide captions and I also discovered that significantly more compute is needed to train the latent models so I will use a pretrained Stable Diffusion model.</p>

<p><img src="/assets/images/generative_figure4.png" alt="stable diffusion" width="600" /></p>

<p>Figure adapted from https://huggingface.co/blog/stable_diffusion</p>

<p>Here is an example output:</p>

<p><img src="/assets/images/eastern_bluebird_stable.png" alt="stable diffusion output" /></p>

<p>I generated the image above with the implementation provided by Lyumin Zhang called <a href="https://github.com/lllyasviel/Fooocus">Fooocus</a>. It is a pre-tuned version of <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">StabilityAI‚Äôs Stable Diffusion XL model</a>. You can generate your own images using the provided Colab notebook -&gt; 
<a target="_blank" href="https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a></p>

<h1 id="conclusion">Conclusion</h1>

<p>Generative AI is currently experiencing a boom, with new research and products coming out every day. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>. I would like to leave you with a final conversation from Microsoft‚Äôs <a href="https://copilot.microsoft.com/">Copilot</a>:</p>

<blockquote>
  <p style="text-align: right;">Me:</p>
  <p style="text-align: right;">Can you write a poem about generative ai?</p>

  <p>Copilot:</p>

  <p>Sure, I‚Äôd love to! Here‚Äôs a poem about generative AI:</p>

  <p>In circuits deep, where data flows, A spark ignites, a thought that grows. From silicon dreams, a mind takes flight, Crafting worlds in the dead of night.</p>

  <p>With algorithms as its guiding star, It reaches realms both near and far. From pixels bright to words that sing, Generative AI creates everything.</p>

  <p>It learns from patterns, vast and wide, In every byte, where secrets hide. A painter‚Äôs brush, a poet‚Äôs pen, It brings to life what once was then.</p>

  <p>Yet in its core, a humble start, A tool to aid the human heart. For in each line and crafted scene, It mirrors dreams, both bold and serene.</p>

  <p>So here‚Äôs to code that dreams and dares, To generative AI, with whom we share, A future bright, where minds entwine, In harmony, both yours and mine.</p>
</blockquote>

<p>END OF CONVERSATION</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>This particular autoencoder preserves spatial information by compressing a 512x512x3 image into a latent space of 64x64x4.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>As a comparison, in <a href="https://arxiv.org/abs/2112.10752">‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models‚Äù</a>, they trained a latent model on FFHQ for 283 hours on an A100. This would cost an equivalent of $370 in Colab compute units so you will have to forgive me for stopping early!¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="generative" /><category term="stable-diffusion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes the stable diffusion architecture for generative AI.]]></summary></entry><entry><title type="html">Experiment 18</title><link href="https://dlfelps.github.io/2024/07/01/adversarial.html" rel="alternate" type="text/html" title="Experiment 18" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/07/01/adversarial</id><content type="html" xml:base="https://dlfelps.github.io/2024/07/01/adversarial.html"><![CDATA[<p>This post describes simple approaches to attack and defend machine learning models.</p>

<h1 id="introduction-to-adversarial-machine-learning">Introduction to adversarial machine learning</h1>
<p>Adversarial machine learning refers to a set of techniques that adversaries use to attack machine learning systems. These attacks exploit vulnerabilities in ML models, aiming to manipulate their behavior or compromise their performance. Adversarial attacks can be used against any system that employs machine learning, including finance, security, and autonomous systems.</p>

<h2 id="types-of-adversarial-attacks">Types of Adversarial Attacks</h2>
<p>There are several types of known attacks against machine learning models. Please see the NIST  <a href="https://csrc.nist.gov/pubs/ai/100/2/e2023/final">taxonomy</a> for a comprehensive breakdown with references. The remainder of the post focuses on the evasion attack, which modifies the input in a way that alters the prediction of the model under attack.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center"><strong>Poisoning</strong></th>
      <th style="text-align: center"><strong>Evasion</strong></th>
      <th style="text-align: center"><strong>Extraction</strong></th>
      <th style="text-align: center"><strong>Inference</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>during</em></td>
      <td style="text-align: center">Training</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Inference</td>
    </tr>
    <tr>
      <td style="text-align: center"><em>purpose</em></td>
      <td style="text-align: center">Mistrain the model</td>
      <td style="text-align: center">Deceive the model</td>
      <td style="text-align: center">Steal the model</td>
      <td style="text-align: center">Infer training data</td>
    </tr>
  </tbody>
</table>

<h2 id="threat-models">Threat models</h2>

<p>An important consideration when attacking/defending a machine learning model is the threat model. The two primary threat models are white-box and black-box attacks.</p>

<p><img src="/assets/images/white_box-Photoroom.png" alt="white box" /></p>

<blockquote>
  <p><strong>White-Box Attack:</strong> The attacker has <strong>full knowledge</strong> of the ML model‚Äôs architecture, parameters, and training data. e.g. <a href="https://arxiv.org/pdf/1412.6572v3.pdf">L-BFGS Attack</a>, <a href="https://arxiv.org/abs/1312.6199">Fast Gradient Sign Method</a></p>
</blockquote>

<p><img src="/assets/images/black_box-Photoroom.png" alt="black box" /></p>

<blockquote>
  <p><strong>Black-Box Attacks:</strong> The attacker has <strong>limited information</strong> about the ML model (e.g., input-output interactions).  e.g. <a href="https://arxiv.org/abs/1809.02861">Transfer Attack</a>, <a href="https://arxiv.org/abs/1708.03999">Score-based Black-Box Attacks</a>, <a href="https://arxiv.org/abs/1712.04248">Decision-based Attack</a></p>
</blockquote>

<p>Note that adversarial exmaples can be used under either threat model, however they are much more effective when an adversary has access to the internals of your model (i.e. white-box) and can verify that the attack works prior to deploying it.</p>

<h2 id="experimental-setup">Experimental setup</h2>

<p>As with a number of my previous posts, I will be using a pretrained <a href="https://github.com/osmr/imgclsmob/tree/master/pytorch#cub-200-2011">ResNet-18 model</a> trained on the <a href="https://www.vision.caltech.edu/visipedia/CUB-200-2011.html">CUB2011 dataset</a>; this will serve as the model under attack.</p>

<h1 id="scenario-1-attacking-a-model-with-non-adversarial-methods">Scenario 1: Attacking a model with non-adversarial methods</h1>
<p>This ‚Äúattack‚Äù can also be seen as a model hardening test. I recommend running it on all computer vision models prior to deploying them. It tests the extent to which the model can handle common image transformations in the wild. Depending on the results, we might discover a cheap form of the evasion attack (applies to both black-box and white-box settings). The attack works as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select a common image transform (e.g. rotation, brightness, contrast)
  2. Select a random test image
  3. Run the test image through the model and save the predicted class
  4. Run the transformed test image through the model and save the predicted class
  5. If #3 and #4 are different then increase counter
  6. Repeat steps 2-5 100 times to estimate the effect of the transform
</code></pre></div></div>

<h2 id="results">Results</h2>
<p>The following plots visually demonstrate the effect of a single transform. The results (represented by the blue bar to the right of each image) represent the average performance of 100 test images. A higher bar signifies that the model is robust to that transform (i.e. the counter in step#5 was low).</p>

<p><img src="/assets/images/blur.png" alt="blur" /></p>

<p><img src="/assets/images/hue.png" alt="hue" /></p>

<p><img src="/assets/images/rotate.png" alt="rotate" /></p>

<p><img src="/assets/images/brightness.png" alt="brightness" /></p>

<p><img src="/assets/images/contrast.png" alt="contrast" /></p>

<p><img src="/assets/images/gamma.png" alt="gamma" /></p>

<p><img src="/assets/images/saturation.png" alt="saturation" /></p>

<h2 id="discussion">Discussion</h2>
<p>The model under attack was relatively robust to subtle image transforms and should be expected to perform well in the wild. Hue was the only transform that significantly impacted performance and that is expected since color is a differentiating factor in bird species. Can we use hue as a cheap form of adversarial attack? No, because the hue transform would also fool a human. In other words, the hue transform fundamentally changes the input so it isn‚Äôt ‚Äúfooling‚Äù anything. We next turn to more advanced methods.</p>

<h1 id="scenario-2-attacking-a-model-with-adversarial-methods">Scenario 2: Attacking a model with adversarial methods</h1>

<p>The goal of an evasion attack is to minimally modify an input (i.e. image) in such a way that the model alters its class prediction, but a human still percieves the original class. It‚Äôs this combination that makes the evasion attack so potent - humans don‚Äôt know anything is wrong and the machine is fooled. The attack succeeds because the altered image lies outside of the training distribution.</p>

<h2 id="pgd">PGD</h2>
<p>The Projected Gradient Descent (PGD) attack <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> is an iterative optimization technique that seeks to find the most adversarial perturbation within a predefined constraint set. Here is psuedocode for the algorithm:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Select an unmodified image as the target
  2. Select a random point within the allowed perturbation region of the target image
  3. Perform gradient descent on the model's loss function
  4. Project the perturbed input back into the feasible set after each iteration
</code></pre></div></div>
<p>This iterative process ensures that the adversarial perturbation remains within the defined constraints around the original input, making the adversarial example both effective and imperceptible to human observers.</p>

<p>However, the PGD attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Over-optimization against one constraint</li>
  <li>Potential for unstable optimization</li>
  <li>Limited transferability to other models</li>
</ul>

<h3 id="example">Example</h3>

<p><img src="/assets/images/pgd_400.png" alt="pgd_400" /></p>
<blockquote>
  <p>A successful example of a PGD attack (left). The modified image was classified differently than the original image, but the modifications are barely visible to the human eye. The modifications added to the original image are magnified 10x in the (right) image.</p>
</blockquote>

<h2 id="carlini-wagner">Carlini-Wagner</h2>
<p>The Carlini &amp; Wagner (C&amp;W) attack is a powerful adversarial evasion attack that aims to generate adversarial examples that are misclassified by a target model. Adversarial examples are created by formulating an optimization problem, where the goal is to minimize the perturbation while ensuring the classifier mislabels the adversarial example. C&amp;W is particularly notable for its ability to generate adversarial examples that are transferable across different models and defenses, making it a significant threat to the security of machine learning systems.</p>

<p>However, the C&amp;W attack has certain limitations that make it less practical for real-world applications:</p>
<ul>
  <li>Computational cost</li>
  <li>Requires domain-specific adaptations to shape the perturbations and the optimization problem depending on the type of data being manipulated</li>
</ul>

<h3 id="example-with-l0-norm">Example with L0 norm</h3>

<p><img src="/assets/images/carliniL0_392.png" alt="carliniL0_392" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. Although the modifications are mostly imperceptible, a small eye-like patch is visible in the magnitude-magnified image (right).</p>
</blockquote>

<h3 id="example-with-linf-norm">Example with LInf norm</h3>

<p><img src="/assets/images/carliniLinf_3207.png" alt="carliniLinf_3207" /></p>
<blockquote>
  <p>A successful example of a C&amp;W attack (left). The modified image was classified differently than the original image. The modifications in this case are noticable and global in the original image and magnitude-magnified image (right).</p>
</blockquote>

<h1 id="scenario-3-defending-a-model-with-non-adversarial-methods">Scenario 3: Defending a model with non-adversarial methods</h1>

<p><img src="/assets/images/adv-flowchart.png" alt="adv-flowchart" /></p>

<p>This scenario explores the exent to which non-adversarial methods can be used to defend a model from a strong adversarial attack (e.g. PGD or Carlini-Wagner). The defense exploits the fact that the evasion attack itself is typically brittle. In other words, the attack noise was calculated to minimize the amount of change in the input space to achieve the desired result. Therefore, it stands to reason that the attack may not be able to withstand additional transformations. We constrain the suite of transformations considered in Scenario #1 to only those transformations that maintained greater than 99% performance on natural images. This allows us to limit the amount of false positives in our adversarial detector to less than 1%.</p>

<h2 id="results-1">Results</h2>
<p>In the tables below we report the prediction rates for the best parameter combination per transformation type (e.g. rotation, brightness).</p>

<h3 id="pgd-1">PGD</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.049505</td>
      <td style="text-align: center">0.950495</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.283019</td>
      <td style="text-align: center">0.716981</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.441441</td>
      <td style="text-align: center">0.553864</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.454545</td>
      <td style="text-align: center">0.545455</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.555556</td>
      <td style="text-align: center">0.444444</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-l0">CW-L0</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.114286</td>
      <td style="text-align: center">0.885714</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.6</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.558442</td>
      <td style="text-align: center">0.441558</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">0.5</td>
      <td style="text-align: center">0.995305</td>
      <td style="text-align: center">0.561905</td>
      <td style="text-align: center">0.4334</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.741935</td>
      <td style="text-align: center">0.258065</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">0.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.815126</td>
      <td style="text-align: center">0.184874</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h3 id="cw-linf">CW-LInf</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transformation</th>
      <th style="text-align: center">Parameter</th>
      <th style="text-align: center">Performance on natural images</th>
      <th style="text-align: center">Performance on adversarial images</th>
      <th style="text-align: center">Differential</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">rotate</td>
      <td style="text-align: center">7</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.819444</td>
      <td style="text-align: center">0.180556</td>
    </tr>
    <tr>
      <td style="text-align: center">brightness</td>
      <td style="text-align: center">1.75</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.910256</td>
      <td style="text-align: center">0.089744</td>
    </tr>
    <tr>
      <td style="text-align: center">gamma</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.939394</td>
      <td style="text-align: center">0.060606</td>
    </tr>
    <tr>
      <td style="text-align: center">contrast</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.943182</td>
      <td style="text-align: center">0.056818</td>
    </tr>
    <tr>
      <td style="text-align: center">saturation</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.944444</td>
      <td style="text-align: center">0.055556</td>
    </tr>
    <tr>
      <td style="text-align: center">blur</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">hue</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h2 id="discussion-1">Discussion</h2>

<p>The highest average difference between performance on natural images and adversarial images is achieved by rotating images by 7 degrees. This transformation is ideal from a false-positive point of view as all natural images are unphased by it. NOTE: this does not mean that the predicted class is correct, only that the predicted class is not altered by rotating the image 7 degrees. Our ability to detect adversarial examples depends on the attack:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">TP rate</th>
      <th style="text-align: center">FP rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">PGD</td>
      <td style="text-align: center">0.95</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-L0</td>
      <td style="text-align: center">0.885</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">CW-LInf</td>
      <td style="text-align: center">0.18</td>
      <td style="text-align: center">0.0</td>
    </tr>
  </tbody>
</table>

<p>The results are quite promising for PGD and CW-L0, but the CW-LInf attack proved to be more resilient (to all tested transformations). The results appear to confirm our hypothesis about the fragility of the evasion attacks - more subtle attacks are more easily detected precisely because they are more subtle. In other words, if the attack has a low signal-to-noise ratio then rotating the image has a better chance to render it ineffective.</p>

<h1 id="scenario-4-defending-a-model-with-adversarial-methods">Scenario 4: Defending a model with adversarial methods</h1>

<p>Adversarial training is a technique used to enhance the robustness of machine learning models against adversarial examples. The simplest way to perform adversarial training is to include adversarial examples in the training set.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Train a model with the original training data
2. Use the model to generate adversarial examples
3. Add the adversarial examples (with correct labels) to the training data
4. Retrain the model
</code></pre></div></div>
<p>A more sophisticated approach called Robust training <a href="https://arxiv.org/abs/1706.06083">(Madry et. al, 2017)</a> alters the optimization process itself in order to make models more resilient to adversarial perturbations. These perturbations are carefully crafted to deceive the model into making incorrect predictions.</p>

<h2 id="comparing-detection-and-adversarial-training">Comparing detection and adversarial training</h2>

<p>Adversarial training strengthens models intrinsically, while detection approaches focus on identifying adversarial examples during inference. Both play crucial roles in addressing the challenges posed by adversarial attacks.</p>

<ul>
  <li>advantages of adversarial detection
    <ul>
      <li>can defend against white-box and black-box threat models</li>
      <li>can be used to protect existing/pretrained models</li>
      <li>may work against UNKNOWN attacks</li>
    </ul>
  </li>
  <li>advantages of adversarial training
    <ul>
      <li>moderate success against all KNOWN attacks</li>
    </ul>
  </li>
</ul>

<p>NOTE: If you are training your own model then you can do BOTH!</p>

<h1 id="conclusion">Conclusion</h1>
<p>This was a long post, so I will summarize the results here:</p>
<ol>
  <li>It is good practice to test your model‚Äôs performance against non-adversarial image transformations.</li>
  <li>Evasion attacks (e.g. PGD and CW) can be used to alter the predictions of a model.</li>
  <li>Some evasion attacks (e.g. PGD and CW-L0) can be detected at inference time due to their lack of robustness against non-adversarial image transformations.</li>
  <li>Other evasion attacks (e.g. CW-LInf) may be rendered inneffective using adversarial training.</li>
</ol>

<p>As ML adoption grows, understanding and addressing these vulnerabilities become crucial for maintaining system integrity and security. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="adversarial" /><category term="evasion" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post describes simple approaches to attack and defend machine learning models.]]></summary></entry><entry><title type="html">Experiment 17</title><link href="https://dlfelps.github.io/2024/06/04/explainable.html" rel="alternate" type="text/html" title="Experiment 17" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/04/explainable</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/04/explainable.html"><![CDATA[<p>This post introduces Concept Bottleneck Models - an interpretable approach to machine learning.</p>

<h2 id="introduction-to-explainable-ai">Introduction to Explainable AI</h2>

<p>Explainable machine learning models describe how and why a prediction was made. Experts in explainable AI (XAI) make a distinction between interpretabiliy and explainability: <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253521002281">source</a></p>

<ul>
  <li>
    <p>Interpretability is the extraction of relevant sub-symbolic information from a machine-learning model concerning relationships either contained in data or learned by the model</p>
  </li>
  <li>
    <p>Explainability is the ability to translate this sub-symbolic information in a comprehensible manner through human-understandable language expressions</p>
  </li>
</ul>

<p>This blog post answers the following questions:</p>
<ol>
  <li>How do you make a machine learning model more interpretable and explainable?</li>
  <li>What are the tradeoffs and when should you do it?</li>
</ol>

<h2 id="experimental-study">Experimental study</h2>

<p>In order to demonstrate the benefits of XAI we will use a standard image classification task on well-known benchmark - <a href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a>, which contains 11,788 images of birds. This dataset is typically used for fine-grained classification since it contains over 200 species of birds. Additionally, for each image, it captures meta-data about each bird (i.e. 28 features including bill shape, wing shape, head color, etc.). This information will be critical to creating an interpretable model.
<img src="https://i0.wp.com/www.beyourownbirder.com/wp-content/uploads/2019/09/american-goldfinch-1.jpg" alt="American goldfinch" /></p>

<p><a href="https://i0.wp.com/www.beyourownbirder.com/wp-content/uploads/2019/09/american-goldfinch-1.jpg">Image source</a></p>

<h2 id="establishing-the-baseline">Establishing the baseline</h2>

<p>Before we build an interpretable model, we will establish the performance of a non-interpretable model. For this dataset we will use a ResNet-18 pretrained model from the <a href="https://github.com/osmr/imgclsmob">pytorchcv</a> package.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pytorchcv.model_provider import get_model 

cub_model = get_model('resnet18_cub', pretrained=True)
</code></pre></div></div>

<p>This model has 74.4% accuracy on the test set, which is impressive given that there are 200 classes and the classes are all similar because they are all birds. We will next describe how to adapt this model to be interpretable.</p>

<h2 id="concept-bottleneck-models">Concept Bottleneck Models</h2>

<p>Deep learning models are powerful due to their ability to learn features that are discriminative to the prediction task. However, these features are not interpretable by humans, leading to the so-called <a href="https://news.mit.edu/2022/machine-learning-explainability-0505">‚Äúblack box problem‚Äù</a>. In an effort to make deep learning models more transparent, Concept Bottleneck Models (CBMs) make use of a human-interpretable feature layer called the ‚Äúconcept layer‚Äù. This layer contains features that are meaningful to a human and representative of the features necessary for a human to perform the same kind of prediction. The final prediction is made based on the values of the concept layer. There are two primary advantages of this approach:</p>
<ol>
  <li>Users can understand the feature values immediately before the prediction layer</li>
  <li>Users can update the feature values to improve accuracy (sometimes called intervention)</li>
</ol>

<p><img src="/assets/images/cbm_cub.jpeg" alt="concept bottleneck model" title="concept bottleneck model" /></p>

<p><a href="https://arxiv.org/pdf/2007.04612">image reference</a></p>

<h3 id="training-a-concept-bottleneck-model">Training a Concept Bottleneck Model</h3>
<p>In the original <a href="https://arxiv.org/pdf/2007.04612">paper</a> the authors test three ways to train a Concept Bottleneck Model:</p>

<ul>
  <li>
    <p>Independent: learn a concept predictor (L<sub>C</sub>) and learn a task predictor (L<sub>Y</sub>)</p>
  </li>
  <li>
    <p>Sequential: learn a concept predictor (L<sub>C</sub>) then learn a task predictor (L<sub>Y</sub> <span>|</span> L<sub>C</sub>)</p>
  </li>
  <li>
    <p>Joint: simultaneously learn concept predictor (L<sub>C</sub>) and task predictor (L<sub>Y</sub> <span>|</span> L<sub>C</sub>)</p>
  </li>
</ul>

<p>The joint approach offered the best performance on the CUB dataset. However, because the concept and task predictors are trained simultaneously using a dual loss function is it prone to side-channel leaks, in which the task predictor drives the concept predictor to leak additional information about features that are not strictly related to the concepts. The extent to which this occurs affects the overall interpretability of the model.</p>

<p>Independent and sequential approaches offered similar performance on CUB and do not suffer from the side-channel leak problem. The independent approach is recommended for systems that anticipate high intervention rates because its task predictor is trained using ground truth concept values. The sequential approach is recommended when intervention is not likely and the concepts are difficult to predict. This allows the task predictor to learn to ignore concepts that may be difficult to predict.</p>

<h4 id="learning-the-concepts">Learning the concepts</h4>
<p>There are 28 concepts in the CUB-200 dataset. These are expert-selected concepts relevant to the problem of bird species identification.</p>

<p><img src="/assets/images/cub_attributes.jpeg" alt="bird attributes" title="bird attributes" /></p>

<p><a href="https://authors.library.caltech.edu/records/cvm3y-5hh21/files/CUB_200_2011.pdf?download=1">image reference</a></p>

<p>We will use the training data to learn to predict these features by creating one classifier per concept. The embedding features from the baseline ResNet-18 model serve as the input to each concept classifier. As you can see in the figure below, the ability to predict each concept varies greatly. This is partially due to the fact that the embedding space is optimized to differentiate at the class level (global prediction) rather than the individual concepts (local predictions).</p>

<p><img src="/assets/images/concept_plot.png" alt="concept predictions" title="concept predictions" /></p>

<p>Let‚Äôs take a closer look at a few examples:</p>

<ul>
  <li>Head Pattern (33% accuracy) - this concept is difficult to predict from the embedded features; the classifier only performs marginally better than the base rate (+7% better)</li>
  <li>Eye Color (80% accuracy) - this concept is the most accurately predicted concept; but it does not perform better than the base rate, suggesting that this feature is not informative and predicts that birds have black eyes regardless of input</li>
  <li>Crown color (55% accuracy) - although this concept is not among the most accurately predicted, it does perform significantly higher than the base rate (+30% better) suggesting that the embedded features at least partially capture this information</li>
</ul>

<h4 id="learning-the-classes">Learning the classes</h4>
<p>Because the performance of the concept predictors was relatively low, we will use a sequential training approach. After learning one concept predictor per concept L<sub>C<sub>i</sub></sub>, we use the predicted concepts (rather than the ground truth concepts) to learn the task predictor.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Interpretable features</th>
      <th style="text-align: center">Concept labels required</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">0.594</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.744</td>
    </tr>
  </tbody>
</table>

<p>We added interpretability to our model at the cost of about 25 points of accuracy. This would be an unacceptable tradeoff for most real-world applications. Even though the overall accuracy is low, if either the accuracy of the concept predictor or task predictor is independently high then they can still be useful in certain contexts.</p>
<ul>
  <li><strong>Expert assistant</strong>:   This scenario assumes the system is being used by a highly trained user (e.g. ornithologist for CUB). The concept predictor (with high accuracy) can save the expert time by pre-populating values for the concept features. The expert then performs a final review of the concept features and assigns the final class label (e.g. species of bird for CUB).</li>
  <li><strong>Psuedo-expert</strong>: This scenario assumes the system is being used by non-skilled users (e.g. Mechanical Turk workers). The worker is able to make simple observations about the image (e.g. what is the color of the bird‚Äôs wing, which of the following shapes most closely matches the bird‚Äôs beak, etc.). The task predictor (with high accuracy) then assigns the species of bird given the observations.</li>
</ul>

<h2 id="discussion">Discussion</h2>

<h3 id="limitations-of-cbms">Limitations of CBMs</h3>

<p>We have already observed the first limitation of Concept Bottleneck Models - they don‚Äôt tend to be as accurate as non-interpretable models for real-world applications. This gap can be reduced using a residual modeling technique such as the one proposed in <a href="https://arxiv.org/abs/2205.15480">Yuksekgonul, Wang, &amp; Zou, (2022)</a>, which splits the model‚Äôs predictions into an interpretable part and a non-interpretable part.</p>

<p>Another limitation is the assumption of the concept labels themselves. The data labeling cost for such a dataset is significantly higher than one that only collects the task labels. Furthermore, the concepts themselves are difficult to get right without expert input. One solution, proposed in <a href="https://arxiv.org/abs/2304.06129">Oikarinen et. al (2023)</a>, uses a combination of a large language model (GPT-3) and a multimodal foundation model (CLIP) to extract concepts for any task. See <a href="https://arxiv.org/abs/2304.06129">paper</a> for details. The reported accuracy of their approach is included in the table below.</p>

<h3 id="limitations-of-resnet-18-backbone">Limitations of ResNet-18 backbone</h3>

<p>I have made it tradition to incorporate DINOv2 into my ml-portfolio series posts. In this post, I replaced the ResNet-18 backbone with DINOv2 and the accuracy improves 15 points. I can‚Äôt think of many applications that would prefer an interpretable model with 60% accuracy over a non-interpretable model with 90% accuracy. NOTE: Although the DINOv2 features were better for overall task accuracy, they did not significantly improve concept accuracy.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">¬†</th>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Interpretable features</th>
      <th style="text-align: center">Concept labels required</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">Y</td>
      <td style="text-align: center">0.594</td>
    </tr>
    <tr>
      <td style="text-align: center">Label-free CBM</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">*Y</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">0.743</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">ResNet-18</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.744</td>
    </tr>
    <tr>
      <td style="text-align: center">Baseline</td>
      <td style="text-align: center">DINOv2</td>
      <td style="text-align: center">N</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">0.905</td>
    </tr>
  </tbody>
</table>

<p>*Features in this space take on continuous values correlated with human relatable concepts.</p>

<h3 id="alternative-approaches-to-interpretability">Alternative approaches to interpretability</h3>

<p>Saliency/attention maps are a common post-hoc interpretability technique that can be applied to any deep learning model. The purpose of these maps is to show the user which input features the model used for to perform most of their computations (i.e. paid the most attention to). They have one advantage over Concept Bottleneck Models in that they do not affect accuracy.</p>

<p><img src="/assets/images/dino_attention_map.png" alt="attention map" title="attention map" /></p>

<p><a href="https://arxiv.org/pdf/2309.16588">image reference</a></p>

<p>But for the purposes of interpretability, I find that the approach falls short. At best it confirms for the user that the expected parts of the input were used to make a decision and not a spurious object (i.e. a common example is that the class ‚Äúfrisbee‚Äù is assigned to images containing large amounts of green grass). But attention maps fail to explain anything about the internal features of the model and its decision process. DINOv2 attention maps tend to highlight a single, main subject in an image. But it doesn‚Äôt do that because it knows the difference between a cow and a blue heron; it was trained in a totally unsupervised way! Furthermore, attention maps do not provide a mechanism for intervention.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post I demonstrated how to make a machine learning model more interpretable and explainable. I also discussed the tradeoffs and alternative uses for such models when their accuracies do not perform at practical levels. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="explainable" /><category term="interpretable" /><category term="computer-vision" /><category term="ml-portfolio" /><summary type="html"><![CDATA[This post introduces Concept Bottleneck Models - an interpretable approach to machine learning.]]></summary></entry><entry><title type="html">Experiment 16</title><link href="https://dlfelps.github.io/2024/06/03/few-shot.html" rel="alternate" type="text/html" title="Experiment 16" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://dlfelps.github.io/2024/06/03/few-shot</id><content type="html" xml:base="https://dlfelps.github.io/2024/06/03/few-shot.html"><![CDATA[<p>This post explores unique concepts in few-shot learning.</p>

<h2 id="introduction">Introduction</h2>

<p>Few-shot learning describes the situation where a classifier must generalize to new classes using only a few examples of each new class. It represents scenarios where data collection (or annotation) is costly for the classes you care about, but you may have access to related data. There are some unique terms associated with few-shot learning, which we will introduce next.</p>

<p>In standard supervised learning, the training and testing sets contain the SAME classes. The classifier is tested for its ability to discriminate between KNOWN classes.</p>

<p><img src="/assets/images/supervised_learning.PNG" alt="supervised learning" title="supervised learning" /></p>

<p><a href="https://ieeexplore.ieee.org/document/10287966">image reference</a></p>

<p>In few-shot learning, the training and testing sets contain DIFFERENT classes. A few-shot classifier is tested for its ability to discriminate UNKNOWN classes given only a few examples. These samples are further divided into the support set and query set. The support set contains labeled examples (for tuning the few-shot classifier) and the query set contains unlabeled samples for evaluation. The number of classes in the support set is denoted by N and the number of examples per class is denoted by K. A 5-way 1-shot method describes a method that has (N=5) classes with (K=1) examples per class.</p>

<p><img src="/assets/images/few_shot_learning.PNG" alt="few_shot_learning" title="few_shot_learning" /></p>

<p><a href="https://ieeexplore.ieee.org/document/10287966">image reference</a></p>

<h2 id="experiment">Experiment</h2>

<p>This experiment will test a model in a 5-way 5-shot task (i.e. 5 new classes, 5 examples each). We will use a standard benchmark in few-shot learning -  <a href="https://www.vision.caltech.edu/datasets/cub_200_2011/">CUB-200-2011</a> dataset. It is a fine-grained image classification task with over 200 different species of birds. The large number of classes make it suitable for few-shot learning as several can be withheld for testing while maintaining a difficult baseline. We next explain our basic approach to few-shot learning (prototype models) and then compare the performance of three ways to train the model.</p>

<h3 id="few-shot-framework-prototype-model">Few-shot framework: Prototype model</h3>

<p>We adopt a single framework for all variations of our experiment - <a href="https://arxiv.org/abs/1703.05175">Prototypical networks</a>. The concept is based on the idea that there is a single prototype representation for each class. A prototype for each class is calculated from the mean of its support set in the embedding space of a neural network. Our three approaches explore three different ways to train this neural network. Classification in a prototypical model is simply a nearest neighbors using only the prototypes<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. The code uses models and task loaders from <a href="https://github.com/sicara/easy-few-shot-learning">Easy Few-shot learning</a>.</p>

<p><img src="/assets/images/prototypical_network.jpeg" alt="prototypical network" title="prototypical network" /></p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221001818">image reference</a></p>

<h3 id="approach-1-pretrained-model">Approach #1: Pretrained model</h3>

<p>The first approach adapts a pretrained model using transfer learning. The transfer learning process starts with a model that is trained on a large and general dataset (e.g. Imagenet). The purpose of this model is to rely on the learned feature maps - which should be robust - and adapt the later layers to classify your new classes (see image below).</p>

<p><img src="/assets/images/transfer_learning.png" alt="transfer model" title="transfer model" /></p>

<p><a href="https://www.nature.com/articles/s41598-024-54923-y">image reference</a></p>

<p>This experiment uses a ResNet-18 model with pretrained weights from an Imagenet dataset. For each task in the evaluation phase, the support set is used to calculate the prototypes and the the query sets are classified from the prototypes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18(weights=ResNet18_Weights.DEFAULT)
model.fc = nn.Flatten()
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="approach-2-classical-model">Approach #2: Classical model</h3>

<p>This approach uses a subset of the training data to generate a pretrained model. NOTE: the test data contains classes that were not included in the training data. This approach is not always viable since it requires prior knowledge of the few-shot task data distribution. However, it can be powerful if you work in a specific domain and have labeled data from that domain readily available. To simulate it here, we split the 200 species in CUB into 140 for training and 60 for testing. During test time, a 5-way 5-shot task is created by randomly sampling 5 classes from the 60 test classes. This process is repeated 500 times to get a statistically significant measure of generalization performance.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18()
model.fc = nn.Linear(512, 140)
model.load_state_dict(torch.load('/content/easy-few-shot-learning/classical_model_18_acc_744.pt')) # pretrained just for you!
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="approach-3-episodic-model">Approach #3: Episodic model</h3>

<p>Episodic training (also called meta-learning) mirrors the few-shot tasks that will be used to test the final model during the training phase. Each ‚Äúepisode‚Äù is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization. This strategy typically assumes prior knowledge of N and K that will be used at test time. <a href="https://arxiv.org/pdf/2204.11181">ref1</a> <a href="https://arxiv.org/pdf/1703.05175">ref2</a></p>

<p>Episodic training can be performed following a classic training regime if additional data is available or using pretrained-weights if it is not. In this case we reserve the same 140 classes as in Approach #2 for training. But instead of performing classic training, which reduces cross entropy across all classes, we adjust weights based on many training episodes (5-way 5-shot). This process is slower as there is additional overhead required for each episode.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = resnet18()
model.fc = nn.Flatten()
model.load_state_dict(torch.load('/content/easy-few-shot-learning/episodic_model_18_acc_779.pt')) # pretrained just for you!
few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)
</code></pre></div></div>

<h3 id="bonus-approach-pretrained-model-dinov2-backbone">BONUS Approach: Pretrained model (DINOv2 backbone)</h3>

<p>The three primary variations above can be compared directly because they are all variations of Resnet-18 and use Prototypical models for evaluation. In this bonus approach, we replace the Resnet-18 backbone of the pretrained model (approach #1) with DINOv2. The rest of the setup is the same.</p>

<h2 id="results">Results</h2>

<p>For this purposes of this discussion we will focus on Approaches #1-#3. The accuracy improves with each successive approach, but the assumptions also increase. Namely, in moving from transfer learning to classic learning we assume knowledge of the domain of the test classes and also assume access to labeled examples from that domain. In moving from classic learning to episodic learning we further assume knowledge of the task itself (5-way 5-shot) and the distribution of the data within it (i.e. uniformly sampled). This does not always reflect the real word - a <a href="https://arxiv.org/abs/2204.11181">study</a> showed that if these assumptions are incorrect they can impact performance significantly.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Backbone</th>
      <th style="text-align: center">Training approach</th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">transfer learning</td>
      <td style="text-align: center">0.684</td>
    </tr>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">classic</td>
      <td style="text-align: center">0.773</td>
    </tr>
    <tr>
      <td style="text-align: center">Resnet-18</td>
      <td style="text-align: center">episodic</td>
      <td style="text-align: center">0.779</td>
    </tr>
    <tr>
      <td style="text-align: center">DINOv2</td>
      <td style="text-align: center">transfer learning</td>
      <td style="text-align: center">0.964</td>
    </tr>
  </tbody>
</table>

<h2 id="conclusion">Conclusion</h2>

<p>We compared three approaches to few-shot learning and discussed the conditions and assumptions necessary to use them. The code for this experiment can be found at my <a href="https://github.com/dlfelps/ml_portfolio">ML portfolio website</a>.</p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>There is some similarity between Prototypical models and Image retrieval tasks. Image retrieval can be viewed as a prototypical model where each image is its own prototype. And instead of returing a single class, the top-N closest prototypes are returned.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Felps</name><email>dlfelps@gmail.com</email></author><category term="few-shot" /><category term="computer-vision" /><category term="ml-portfolio" /><category term="meta-learning" /><summary type="html"><![CDATA[This post explores unique concepts in few-shot learning.]]></summary></entry></feed>